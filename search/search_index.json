{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Amazon Web Service Studies","text":"<p>This site is about deeper studies and keep all my notes on AWS services, while I learn all this world. It includes summary, and labs + references to other repos I built to do proof of concepts.</p> <p>Version 0.24</p> <p>Created 09/20/2022 - Updated 07/01/2024</p>"},{"location":"#aws","title":"AWS","text":"<ul> <li>Created in 2002, and launched as AWS in 2004 with S3, and SQS as first service offerings.</li> </ul>"},{"location":"#why-cloud","title":"Why cloud","text":"<p>There are 5 main advantages to cloud infrastructure as AWS:</p> <ul> <li>Cost Savings: Only pay for what is used, leveraging economy of scale: AWS EC2 instances have different pricing model depending of the resource capacity and usage. Enterprise are moving from capex to variable expense (OPEX Operational Expenses).</li> <li>Agility: Teams can experiment and innovate quickly and frequently at minimum cost. Define infrastructure in minutes, as code, not weeks or even months to get hardware.</li> <li>Elasticity: Scale up and down so no need to guess resource capacity.</li> <li>Innovation: Focus on business apps, not IT infrastructure and data centers.</li> <li>Global Footprint: Extensible, reliable, and secure on global cloud infrastructure. Reach in a minute.</li> </ul> Notes <ul> <li>The cloud transition has happened much faster because it yields great value and has fewer blockers, and bigger customers gain, drive higher volume of re-investments into the platform.</li> <li>Scalability is the ability of an application to accommodate growth without changing design. Scalability ensures that systems remain highly available into the future as the business expands. </li> <li>Elasticity is the power to instantly scale computing resources up or down easily. Elastic Load Balancing and Auto Scaling can automatically scale the AWS cloud-based resources up to meet unexpected demand.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li>Enable to build scalable apps, adaptable to business demand</li> <li>Global reach</li> <li>Extend Enterprise IT with more elastic resources</li> <li>Support Data lake at scale with high availability</li> <li>Support flexible big data analytics</li> <li>No server management adoption</li> </ul>"},{"location":"#deep-dive","title":"Deep dive","text":"<ul> <li>Six strategies to move to the cloud</li> </ul>"},{"location":"#cloud-value-frameworks","title":"Cloud Value Frameworks","text":"<p>The Frameworks includes four business value pillars:</p> <ul> <li>Cost savings: Total Cost Ownership. -50% is classical </li> <li>Staff productivity:  62% improvement </li> <li>Operational resilience: -32% downtime</li> <li>Business Agility: 47% improvement </li> </ul> <p></p> <p>(IDC numbers)</p>"},{"location":"#cost-saving","title":"Cost saving","text":"<ul> <li>Understand the true cost of existing IT capabilities</li> <li> <p>ROI = Cost saving / (sunk cost + migration cost)</p> <ul> <li>For sunk cost: assess the hardware depreciation and the potential recovery value by reselling data center or hardware.</li> <li>Migration costs: more difficult to assess, but we can use the break even migration cost per server by defining a target ROI. Now only one unknown in the previous equation: migration cost = Cost Savings / ROI - sunk cost. </li> </ul> </li> </ul> <p>For actual cost, consider:</p> <ul> <li>Server cost with HW and SW license</li> <li>Storage cost with HW and SW license</li> <li>Network cost with HW and SW license</li> <li>Facilities cost for each of those machines: power, cooling, space</li> <li>Site Reliability Engineer cost</li> <li>Extras: like project management, training, legal, advisors, contractors, cost of capital</li> <li>Think about standard depreciation of 3 or 5 years. Match to 3 year AWS EC2 reserved instances</li> <li>Use EC2 Reserved Instance volume to assess discount</li> <li>Use realistic metrics and ratios like VM density,  servers, racks...</li> <li>Explore current CPU and memory usage</li> <li>Apply cost saving by using automation and configuration as code.</li> <li>Cost assessment can take from 3 weeks to multi months.</li> <li>Migration Evaluator to do on-premise server analysis to optimize cloud migration planning.</li> </ul>"},{"location":"#cloud-readiness","title":"Cloud readiness","text":"<ul> <li>Human skills and experience are required to transition to the cloud.</li> <li>Application readiness to migrate: assess dependencies, integration needs, and application translation.</li> <li>Each stakeholders (devOps, operations, CFO, procurement) have their point of view.</li> </ul>"},{"location":"#additional-impacts","title":"Additional impacts","text":"<ul> <li>Cost of delays - risk premium</li> <li>Competition - competitive ability</li> <li>Governance and compliance</li> </ul>"},{"location":"#operational-resilience","title":"Operational resilience","text":"<p>It really means security and up time.</p> <p>Impact for downtime is a direct cost on business revenue, but also cost to get back up: which includes 3nd party fee, equipment replacement, recovery activities, investigation cost.... Customer churns, company's reputation...</p>"},{"location":"#business-agility","title":"Business agility","text":"<p>Look at responding faster, experimenting more, and delivering results in the same or less amount of time. Business agility is about delivering more, respond faster to customer\u2019s requests or problems, develop new product, add features more quickly, expend to new market. </p> <p>Business agility allows customers to innovate by increasing \"fail fast\" while reducing risks and costs. Being able to easily shut down failed initiatives without the pain and wasted resources associated with an inflexible on-premises environment.</p> <p>The KPIs to consider includes at least:</p> <ul> <li>New app launched per year</li> <li>Time-to-market for new app. (Observed 20% gain)</li> <li>Time to provision new environments (days)</li> <li>Deployment frequency</li> <li>Time to deploy to production, to test...</li> <li>Features per release (observed 26% more)</li> <li>Total # of defects</li> <li>% defects found in test</li> <li>MTTR: mean time to resolution metric</li> <li>Response time to defect</li> <li>Customer retention in %</li> <li>New feature adoption in %</li> <li>Value per release in $ (+34% more revenue per user)</li> </ul> <p>Moving to the cloud does not have to be a binary proposition. Enterprises may move as much or as little of their infrastructure to the cloud as suits to their business.</p>"},{"location":"#cloud-financial-management","title":"Cloud financial management","text":"<p>Includes four key areas:</p> <ol> <li>Measurement and accountability: establishing cost transparency to ensure visibility</li> <li> <p>Cost Optimization: identify waste, scale based on demand, improve cost efficiency</p> <ul> <li>Right sizing: select the lowest cost instance that meets performance requirements. Look at CPU, RAM, storage and network usage to identify downsizing opportunity. See AWS CloudWatch</li> <li>Increase elasticity: shutdown test and dev instances. Automatic scaling. </li> <li>Choose the right pricing model: on-demand, reserved instances (predictable workload), convertible RIs, spot instance....</li> <li>Use the right storage: automate aging from different S3 services</li> </ul> </li> <li> <p>Planning and forecasting: based on actual and future costs and needs</p> <ul> <li>AWS pricing calculator to estimate the cost of the architecture solution.</li> <li>AWS price list API</li> <li>AWS Cost Explorer</li> </ul> </li> <li> <p>Cloud financial operations: invest in tools, people, and automation</p> </li> </ol>"},{"location":"#global-infrastructure","title":"Global Infrastructure","text":"<p>AWS is a global infrastructure with 32 regions and 2 to 6 separated availability zones per region (102 AZs). Ex of AZ: us-west-1-2a.</p> <p></p> <p>AZ is one or more Data Center with redundant power, networking and connectivity. Isolated from disasters using different facilities. Interconnected with low latency network (AWS maps the AZ letter identifiers to different physical AZs between different AWS accounts).</p> <p>Data centers are independent facilities typically hosting 50k servers up to 80k servers. Larger DCs are not desirable because the economy of scale is not that great but the blast radius is becoming too big. Inbound traffic is 110 Tbps within a single DC.</p> <p>James Hamilton's presentation about AWS infrastructure</p> <p>AWS services are local with very few are global:</p> <ul> <li>EC2 is a regional service. Region-scoped services come with availability and resiliency. </li> <li>IAM is a global service.</li> </ul> <p>AWS Local Zone location is an extension of an AWS Region where users can run latency sensitive applications in geography, in large population density, close to the end-users (Los Angeles local zone). Enterprises can extend any VPC from the parent AWS Region into Local Zones. Local Zones have their own connections to the internet and support AWS Direct Connect.</p> <p>AWS Wavelength enables developers to build applications that deliver single-digit millisecond latencies to mobile devices and end-users. AWS compute and storage services are deployed within the telecommunications providers\u2019 data centers at the edge of the 5G networks, and seamlessly access the breadth of AWS services in the region.</p> <p>Choose an AWS region, depending of the requirements:</p> <ul> <li>Compliance with data governance and legal requirements.</li> <li>Close to users to reduce latency.</li> <li>Service availability within a region .</li> <li>Pricing.</li> </ul>"},{"location":"#availability-and-reliability","title":"Availability and reliability","text":"<p>Be sure to get clear agreement on the following definitions:</p> <ul> <li>Fault Tolerant: characteristic for a system to stay operational even if some of its component fails.</li> <li>High Availability: Ensures that systems are always functioning and accessible and that downtime is minimized as much as possible without the need for human intervention.</li> <li>Durability: A byproduct of storage redundancy, durability ensures that a customer\u2019s data will be saved regardless of what happens.</li> <li>Reliability: The ability of a system to recover from infrastructure or service failures and the ability to dynamically acquire computing resources to meet demand and mitigate disruptions.</li> <li>Disaster Recovery: preparing for and recovering from a disaster.</li> </ul> <p>&gt;&gt;&gt; Next Networking</p>"},{"location":"#interact-with-aws","title":"Interact with AWS","text":"<ul> <li>Management console: services are placed in categories: compute, serverless, database, analytics...</li> <li>AWS CLI</li> <li>SDK for C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby</li> <li>Defining infrastructure as code with Terraform, AWS CDK, AWS CloudFormation, or AWS SAM for serverless function deployment.</li> </ul>"},{"location":"#some-application-patterns","title":"Some application patterns","text":"<p>For any solution architecture, developers need to assess cost, performance, reliability, security and operational excellence.</p>"},{"location":"#stateless-app","title":"Stateless App","text":"<p>The step to grow a stateless app: add vertical scaling by changing the EC2 profile. (While changing, user has out of service). Second step is to scale horizontally, each EC2 instance has static IP address and DNS is configured with 'A record' to get each EC2 end point. But if one instance is gone, the client App will see it down until TTL expires.</p> <p>The reference architecture includes DNS record set with alias record (to point to Application Load Balancer). Using alias as ALB address may change over time with TTL of 1 hour. Use load balancers in 3 AZs (to survive disaster) to hide the horizontal scaling of EC2 instances (managed with auto scaling group) where the app runs. Health checks are added to keep system auto adaptable and hide system down, and restricted security group rules to control EC2 instance accesses. ALB and EC instances are in multi different AZs. The EC instances can be set up with reserved capacity to control cost.</p> <p></p>"},{"location":"#stateful-app","title":"Stateful app","text":"<p>For stateful app, often architects use the concept of shopping card, where at each interaction of the user, it is possible the traffic would be sent to another EC2 instance that the one which started to process the shopping cart. Using ELB with stickiness will help to keep the traffic to the same EC2, but in case of EC2 failure we still loose the cart data. An alternate is to use, user's cookies to keep the whole cart at each interaction. This approach looks like a stateless app as state is managed by client and the cookie. For security reason the app needs to validate the cookie content. Cookie has a limit of 4K data.</p> <p>Another solution is to keep session data into an elastic cache, like Redis, and use the sessionId as key, persisted in a user's cookie. So EC2 managing the interaction can get the cart data from the cache using the sessionID. It can be enhanced with a RDS to keep user data. Which can also support the CQRS pattern with read replicas. Cache can be updated with data from RDS so if the user is requesting data within an active session, it hits the cache.</p> <p></p> <p>Cache and database are set on multi AZ, as well as the EC2 instances and load balancers, all to support higher availability. Security groups need to be defined to get all traffic to the ELB and limited traffic between ELB and EC2 and between EC2 and the cache or between EC2 and the DataBase.</p> <p>Another example of stateful app is the one using image stored on disk. In this case EC2 EBS volume will work only for one app instance, but for multi app scaling out, we need to have a Elastic FS which shpuld be Multi AZ too.</p>"},{"location":"#deploying-app","title":"Deploying app","text":"<p>The easiest solution is to create AMI containing OS, dependencies and app binary. This is completed with User Data to get dynamic configuration. Database data can be restored from Snapshot, and the same for EFS data.</p> <p>Elastic Beanstalk is a developer centric view of the app, hiding the complexity of the IaaS. From one git repository it can automatically handle the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring. </p> <p>Cloud Formation and CDK definitions, can do the deployment of an application as well as the infrastructure provisioning. </p> <p>&gt;&gt;&gt; Next: Infrastructure</p>"},{"location":"ai-ml/","title":"AI and ML services introduction","text":"<p>Info</p> <p>Created 12/2022 - Updated 1/30/24</p> <p>Amazon started to use AI in product/book recommendations in the 90s. Now it is used everywhere: fulfillment automation, inventory management, voice driven interactions, drones... </p> <p>The following diagram illustrates the Amazon ML stack:</p> <p></p>"},{"location":"ai-ml/#ml-amazon-services","title":"ML Amazon Services","text":""},{"location":"ai-ml/#rekognition","title":"Rekognition","text":"<p>For detection inside of image or video. Used for face detection, labeling, celebrity recognition...</p> <p>AWS CDK code samples</p>"},{"location":"ai-ml/#transcribe","title":"Transcribe","text":"<p>Speech to Text service, always self trained. It supports both synch and asynch requests. Words can be customized to meet specific business needs, and special vocabulary. Can be enhanced with custom training sets. </p>"},{"location":"ai-ml/#polly","title":"Polly","text":"<p>Text to speech. It uses speech synthesis markup language (SSML) to help us put emphasize on words, includes breathing sounds, whispering.... Also it is possible to customize the pronunciation of words and do substitution using Pronunciations lexicons. </p> <p>Support 31 languages, 20+ NTTS.</p> <p>It also supports asynchronous synthesis to process up to 200k characters. Texts are sent over TLS and Poly does not retain texts processed. </p> <p>It supports SSML, Speech Markup Language to control speech delivery: pause, paragraphs, rate of speech, phonetic pronunciations, acronyms and abbreviations.</p>"},{"location":"ai-ml/#translate","title":"Translate","text":"<p>Text translation service that uses advanced machine learning (neural network) technologies to provide high-quality translation on demand. Low latency &lt;150ms and &lt; 80ms for conversation.</p> <p>5000 characters in synch requests, and support batch processing.</p> <p>It is trained on 11 domains. Pay per use.</p>"},{"location":"ai-ml/#lex","title":"Lex","text":"<p>Build conversational bots or chatbots and virtual assistant. Lex includes speech to text, and NLU.</p> <p>It also includes ML to do automated designer of the conversation by analysing existing conversation transcripts.</p>"},{"location":"ai-ml/#amazon-textract","title":"Amazon Textract","text":"<p>Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents. It uses Optical Character Recognition.</p>"},{"location":"ai-ml/#amazon-connect","title":"Amazon Connect","text":"<p>Cloud contact center.</p>"},{"location":"ai-ml/#comprehend","title":"Comprehend","text":"<p>Uses natural language processing (NLP) to extract insights about the content of documents. Entity extraction, detect language, sentiment analysis...</p> <p>No need to provide training data, it is always trained.</p>"},{"location":"ai-ml/#sagemaker","title":"SageMaker","text":"<p>See dedicated note.</p>"},{"location":"ai-ml/#forecast","title":"Forecast","text":"<p>Fully managed service that uses statistical and machine learning algorithms to deliver highly accurate time-series forecasts.</p>"},{"location":"ai-ml/#kendra","title":"Kendra","text":"<p>Highly accurate and intelligent search service that enables users to search unstructured and structured data using natural language processing and advanced search algorithms. </p> <p>User can define ingestion process for documents and repositories.</p>"},{"location":"ai-ml/#personalize","title":"Personalize","text":"<p>Fully managed machine learning service that uses our data to generate item recommendations for the end users. Recommenders are services to be integrated in application to get real-time recommendation. Optimized for Retail, media and entertainment. It is usind on amazon.com.</p> <p>Includes recipes to define recommenders.</p> <p>Amazon Personalize samples Git Repo.</p>"},{"location":"ai-ml/#ml-domain-of-knowledge","title":"ML domain of knowledge","text":""},{"location":"ai-ml/#data-engineering","title":"Data Engineering","text":""},{"location":"ai-ml/#data-repositories","title":"Data repositories","text":""},{"location":"ai-ml/#data-ingestion-solution","title":"Data ingestion solution","text":""},{"location":"ai-ml/#data-transformation-solution","title":"Data transformation solution","text":""},{"location":"ai-ml/#data-analysis","title":"Data analysis","text":""},{"location":"ai-ml/#sanitize-and-prepare-data-for-modeling","title":"Sanitize and prepare data for modeling","text":""},{"location":"ai-ml/#feature-engineering","title":"Feature engineering","text":""},{"location":"ai-ml/#data-analysis-for-ml","title":"Data analysis for ML","text":""},{"location":"ai-ml/#modeling","title":"Modeling","text":""},{"location":"ai-ml/#frame-business-problems-as-ml-problems","title":"Frame business problems as ML problems","text":""},{"location":"ai-ml/#model-selection","title":"Model selection","text":""},{"location":"ai-ml/#train-model","title":"Train model","text":""},{"location":"ai-ml/#perform-hyperparameter-optimization","title":"Perform hyperparameter optimization","text":""},{"location":"ai-ml/#evaluate-model","title":"Evaluate model","text":""},{"location":"ai-ml/#ml-implementation-and-operations","title":"ML implementation and operations","text":""},{"location":"ai-ml/#build-model-for-enterprise-needs","title":"Build model for enterprise needs","text":""},{"location":"ai-ml/#technology-fit-for-purpose","title":"Technology fit for purpose","text":""},{"location":"ai-ml/#security-specific-for-ml","title":"Security specific for ML","text":""},{"location":"ai-ml/#deploy-and-operationalize","title":"Deploy and operationalize","text":"<ul> <li>Expose endpoints and interact with them.</li> <li>Understand ML models.</li> <li>Perform A/B testing.</li> <li>Retrain pipelines.</li> <li>Debug and troubleshoot ML models.</li> </ul>"},{"location":"ai-ml/bedrock/","title":"Amazon Bedrock","text":"<p>Amazon Bedrock is a managed platform for hosting and running different Foundation Models. Currently it has support for Amazon Titan, AI21 Labs, Cohere, Anthropic and Stability AI models.</p> <p>Amazon Bedrock addresses the fact that a single solution or model is unlikely to solve every business problem. Custom data sets help companies to differentiate their generative AI applications.</p> <ul> <li>Amazon Titan is a generative LLM for summarization, text generation, classication and open-ended Q&amp;A, developed internally to Amazon. Titan FMs are built to detect and remove harmful content.</li> <li>AI 21 labs is a startup who wants to improve NLP, with generation of natural text. Their goal is to have AI to work alongside humans and empower them to be better versions of their writing and reading. Builds Jurassic-2 Multilingual LLMs for text generation in Spanish, French, German, Portuguese, Italian, and Dutch</li> <li>Anthropic is a a startup with tailored AI assistant, and API exposed foundational model. The Claude 2 is a LLM for thoughtful dialogue, content creation, complex reasoning, creativity, and coding, based on Constitutional AI and harmlessness training.</li> <li>Stability AI offers open AI models for Image, Language, Audio, Video, 3D, biology...</li> <li>Cohere Text generation model for business applications and embeddings model for search, clustering, or classification in 100+ languages.</li> </ul> <p>Generative AI models like Titan and Claude use probability distributions to generate responses to questions.</p>"},{"location":"ai-ml/bedrock/#value-propositions","title":"Value propositions","text":"<ul> <li>Serverless, fully managed service.</li> <li>Find the right model for what we're trying to do, with single API access. It supports text-to-text, text-to-image, image-to-image generation.</li> <li>Customize the existing FM with our own documents and labelled data.</li> <li>Integrate and deploy model into the business applications. Integrate agents to improve workflow and tasks execution.</li> <li>Run workload on optimized price/performance ships like Trainium and Inferentia.</li> <li>Foundation model providers will not have access to the proprietary data / requests.</li> <li>Comprehensive data protection and privacy.</li> <li>Support for governance and auditability, with logging and monitoring.</li> <li>Users are able to fine-tune foundational models privately, keep a private copy of the base FM.</li> <li>For large language models, it can take noticeable time to generate long output sequences. Rather than waiting for the entire response to be available, latency-sensitive applications may like to stream the response to users.</li> </ul>"},{"location":"ai-ml/bedrock/#pricing","title":"Pricing","text":"<ul> <li>On demand: For text generation the pricing is based on input and output token count for LLMs. For embeddings, only input token processed. There are request per minute (RPM) and token per minute (TPM) limits.</li> <li>Provisioned throughput: provision sufficient throughput to meet large consistent inference workloads.</li> </ul> <p>Model Units provide throughput for a specific model, measured in terms of Max Input TPM and Max Output TPM.</p> <p>SageMaker Jumpstart differs by the cost structure (), the service operations, the regional availability, the security level and the open source or proprietary LM models available. Used to tune open source models, or deploy models to production quickly, may be in region where Bedrock is not available. Using JumpStart, we can perform inference on a pre-trained model, even without fine-tuning it first on a custom dataset.</p>"},{"location":"ai-ml/bedrock/#integration","title":"Integration","text":"<ul> <li>Deploy in a single or multi-tenant options.</li> <li>Integrate with vector store databases like Amazon OpenSearch Serverless, Pinecone and Redis.</li> <li>Private connection between Amazon Bedrock service and our VPCs. Encryption at rest and in transit using KMS keys.</li> <li>Deliver customized semantic search capabilities with Titan Embeddings or Cohere Embed to create vectors of company's data.</li> </ul>"},{"location":"ai-ml/bedrock/#security","title":"Security","text":"<ul> <li> <p>Use AWS IAM for access control. To access the Amazon Bedrock console, we need permissions to list and view details about the Amazon Bedrock resources in our AWS account.</p> <pre><code>\"Effect\": \"Allow\",\n\"Action\": [\n    \"bedrock:ListFoundationModels\",\n    \"bedrock:GetFoundationModelAvailability\",\n    \"bedrock:ListProvisionedModelThroughputs\",\n    \"bedrock:InvokeModel\",\n    \"bedrock:InvokeModelWithResponseStream\"\n],\n\"Resource\": \"*\"\n</code></pre> </li> <li> <p>AWS Cloudtrail for auditing</p> </li> <li>data is encrypted at rest and in transit (HTTPS) </li> <li>AWS CloudWatch for monitoring</li> </ul>"},{"location":"ai-ml/bedrock/#workshops-and-demos","title":"Workshops and demos","text":"<ul> <li>Text playground</li> <li>Amazon Bedrock workshop.</li> <li>Amazon Bedrock Workshop git repo.</li> <li>Test connection to Bedrock via boto3</li> <li>Test a prompt to Claude instant v1 to demonstrate a blog post creation for business decision leader.</li> <li>Test creating image with StabilityDiffusion</li> <li>Simple test to call Bedrock with Langchain using on zero_shot generation.</li> </ul>"},{"location":"ai-ml/bedrock/#a-simple-playground-demonstrations","title":"A Simple Playground demonstrations","text":"<p>Be sure to have enabled Anthropic Claude V2 model. The following are examples of Bedrock playground capabilities. </p> Demo of text generation for a blog post <pre><code>write a blog post about computers\n</code></pre> <ul> <li>Explain the prompt (Human:  \\n Assitant: ) <li>Explain the parameters, Temperature, To P, Top=K, Length, Stop Sequence</li> <li>Demonstrate with API request button that the same could be done with AWS CLI</li> Another example with query and prompt with unknown to Claude knowledge <pre><code>I am a IT consultant and I need deep information on design.\nI want to understand how to integrate Amazon Bedrock in my solution.\nIf you do not know the answer, say you do not know.\n</code></pre> <p>The response is showing, no hallucination was done, and we need to completement Claude knowledge to be able to answer.</p> Demonstrate chatbot <ul> <li>Explain the concept of streaming to generate output content in real time, as it comes. <pre><code>Tell me about the best car to buy. Give me a numbered list with the top 5 options. Include information on pricing, fuel efficiency, consumer ratings, and comfort of design. List the sources used for each item in the list. hybrid or electric technology used. Order the list so that the most desirable option is listed first\n</code></pre></li> </ul> <p>Here is an example of output</p> <p></p> Demonstrate image generation <pre><code>create an image of hound basset drinking water from a river\n</code></pre> <p></p> <p>Yes it can hallucinate with image too.</p>"},{"location":"ai-ml/bedrock/#access-to-bedrock-via-api","title":"Access to Bedrock via API","text":"<p>Interaction with the Bedrock API is done via the AWS SDK for Python: boto3.</p> <p>The boto3 provides different clients for Amazon Bedrock to perform different actions. The actions for <code>InvokeModel</code> and <code>InvokeModelWithResponseStream</code> are supported by Amazon Bedrock Runtime where as other operations, such as ListFoundationModels, are handled via Amazon Bedrock client.</p> Use utils tool to connect to Bedrock <pre><code>os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\" \nboto3_bedrock = bedrock.get_bedrock_client(\n                    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n                    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n                    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n                )\n</code></pre>"},{"location":"ai-ml/bedrock/#summarization","title":"Summarization","text":"<p>Text summarization is a Natural Language Processing (NLP) technique that involves extracting the most relevant information from a text document and presenting it in a concise and coherent format. In order to get the model running a summarization task, we use a prompt engineering, which sends to the model instructions (on plain text) about what is expected when it processes our data and about the response.</p> <p>See a first example on a small text in llm-langchain/summarization/SmallTextSummarization.py.</p> Prompt engineering <p>Prompt engineering is about designing optimal prompts to instruct the model to perform a task. Example of prompt for Q&amp;A. Below is an example of Zero-shot prompting: <pre><code>Q: &lt;Question&gt;?\nA:\n</code></pre></p> <p>While few-shot prompting gives more question/answer pairs.</p> <p>Prompt includes Instruction, Context, Input Data, and Output Indicator. </p> Prompt for small summary using Claude <pre><code>prompt = \"\"\"\nHuman: Please provide a summary of the following text.\n&lt;text&gt;\n&lt;/text&gt;\nAssistant:\"\"\"\n</code></pre> <p>The Bedrock product documentation includes a lot of prompt templates for the different models.</p> <p>When we work with large documents, we can face some challenges as the input text might not fit into the model context length, or the model hallucinates with large documents, or, out of memory errors. <code>LangChain</code> can be used to chunk the big document, to send smaller pieces to the model, and then use a summary chain with a map-reduce algorithm. See an example of code in summarization/long-text-summarization-mr.py.</p>"},{"location":"ai-ml/bedrock/#qa","title":"Q&amp;A","text":"<p>Question and answer applications, quite often use private documents to be more specific. Retrieval Augmented Generation, or RAG is used to augment the prompt to LLM. Researches found that LLM knowledge could be augmented on the fly by providing the additional knowledge base as part of the prompt. The idea is to send a context with relevant new content and ask with a prompt like:</p> <pre><code>question = \"How can I fix a flat tire on my Audi A8?\"\nprompt_data = f\"\"\"Answer the question based only on the information provided between ## and give step by step guide.\n#\n{context}\n#\n\nQuestion: {question}\nAnswer:\"\"\"\n</code></pre> <p>We can add more documents to the corpus, and use vectors and vector database to augment the prompts by adding relevant data in the context. The approach is to process the documents, split them into smaller chunks, create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model, create an index using the chunks and the corresponding embeddings and a stored in a document store index:</p> <p>A potential architecture looks like:</p> <p></p> <p>When executing the Q&amp;A, the question embedding is compared with the embeddings in the search index, the (top N) relevant document chunks are retreived and added to the context in the prompt. The prompt is sent to the model and it returns the contextual answer based on the documents retrieved.</p> <p>LangChain has API to split documents, and upload embeddings into in-memory vectorDB (FAISS)  or OpenSearch on AWS.</p> <p>See some of my RAG code in this folder. </p> <p>Using the Index Wrapper we can abstract away most of the heavy lifting such as creating the prompt, getting embeddings of the query, sampling the relevant documents and calling the LLM. (See code in labs and LangChain section.)</p> <p>See the Guide to choose the right components for your RAG solution on AWS.</p>"},{"location":"ai-ml/bedrock/#chatbots","title":"Chatbots","text":"<p>Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about.</p> <p>LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. It also provides easy ways to incorporate these utilities into chains. It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.</p> <p>As before, the first process in a building a contextual-aware chatbot is to generate embeddings for the context. </p> <p></p>"},{"location":"ai-ml/bedrock/#agent","title":"Agent","text":"<p>Agents for Amazon Bedrock are fully managed capabilities that make it easier for developers to create generative AI-based applications that can complete complex tasks for a wide range of use cases and deliver up-to-date answers based on proprietary knowledge sources. It automatically break down tasks and create an orchestration plan.</p>"},{"location":"ai-ml/bedrock/#labs-notes","title":"Labs notes","text":"<p>This section covers some notes from Bedrock workshop and the data science on AWS workshops.</p> <ul> <li> <p>The sagemaker execution role needs access to bedrock resource, so define an inline policy like</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Sid\": \"BedrockFullAccess\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\"bedrock:*\"],\n        \"Resource\": \"*\"\n    }\n]\n}\n</code></pre> </li> <li> <p>Each model has its own parameters and output. It is important to test a prompt with different model. Example with Titan</p> <pre><code>body = json.dumps({\"inputText\": prompt_data})\nmodelId = \"amazon.titan-e1t-medium\"  \naccept = \"application/json\"\ncontentType = \"application/json\"\n\nresponse = boto3_bedrock.invoke_model(\n    body=body, modelId=modelId, accept=accept, contentType=contentType\n)\nresponse_body = json.loads(response.get(\"body\").read())\n\nembedding = response_body.get(\"embedding\")\n</code></pre> <p>Titan supports generating text embeddings, which is a vector representation of the input or output text. It can be used for text similarity or search.</p> </li> </ul> Existing models <p>Available text generation models under Amazon Bedrock have the following modelIDs. Use <code>aws bedrock list-foundation-models | grep modelId</code>. (the list will change over time)</p> <ul> <li>amazon.titan-tg1-large</li> <li>amazon.titan-embed-g1-text-0</li> <li>ai21.j2-grande-instruct</li> <li>ai21.j2-jumbo-instruct</li> <li>anthropic.claude-instant-v1</li> <li>anthropic.claude-v1</li> <li>anthropic.claude-v2</li> </ul> <ul> <li>Using LangChain allow us to augment the Large Language Models by chaining together various components, to be used for RAG, ChatBot,... My LandChain experimentations with Bedrock are in this repo..</li> </ul>"},{"location":"ai-ml/bedrock/#more-readings","title":"More Readings","text":"<ul> <li>LangChain</li> <li>Bedrock Workshop  with companion github</li> <li>Guidelines for Bedrock LLM.</li> <li>Generative AI application builder on AWS.</li> <li>Rock party to interact with LLM</li> </ul>"},{"location":"ai-ml/q/","title":"Amazon Q","text":"Info <p>Created: Dec 2023 - Updated: 02/26/24. State: Draft</p>"},{"location":"ai-ml/q/#introduction","title":"Introduction","text":"<p>A Generative AI assistant for business with special access to company data and systems with built-in connectors. Amazon Q is built on Amazon Bedrock and uses multiple FMs to complete its tasks. It is built to be secure and private</p> <p>One specific deployment is Amazon Q in the AWS management console, or integrated in developer IDE with CodeWhisperer.</p> <p>It uses the context of the ongoing session to enhance its responses, it recalls past questions and answers. Context is linked to the session and the AWS console page.</p>"},{"location":"ai-ml/q/#pricing","title":"Pricing","text":"<ul> <li>See Pricing documentation</li> </ul>"},{"location":"ai-ml/q/#for-developers","title":"For developers","text":"<p>Amazon Q aims to provide an AI assistant to support developers and builders at each stage of the development process. Generative AI can help to automate repetitive coding tasks, generate test cases, fix bugs, optimize code, summarize and explain code, and accelerate many other parts of the SDLC.</p> <p>CodeWhisperer's response to a code snippet is influenced by the existing code. It will not include suggestion from licensed open source code.</p> <p>Amazon Q network troubleshooting, you can query reachability between two resources in your AWS account and troubleshoot any network connectivity issues.</p>"},{"location":"ai-ml/q/#important-links","title":"Important links","text":"<ul> <li> Next Generation Developer Experience workshop.</li> </ul>"},{"location":"ai-ml/sagemaker/","title":"Amazon SageMaker","text":""},{"location":"ai-ml/sagemaker/#introduction","title":"Introduction","text":"<p>Fully managed machine learning service, for developers and data scientists, to develop machine learning models then directly deploy them into a production-ready hosted environment. It includes a Feature store. It also provides an integrated Jupyter authoring notebook instance.</p> <p>The figure below explains how SageMaker works for model training, by using S3 bucket as source of data (Ground Truth), ECR Container registry to get predefined image, models are persisted in S3 bucket output folder:</p> <p></p> <p>Amazon SageMaker always uses Docker containers when running scripts, training algorithms, and deploying models. We can create a training job with the SageMaker console, AWS CLI, Python notebook,  or using the SageMaker Python SDK.</p> <p>After we trained our machine learning model, we can deploy it using Amazon SageMaker deployment, using one of the following scenarios:</p> <ul> <li> <p>One prediction at a time, use real-time inference hosting service: limit to 6MB and t&lt;60s </p> <p></p> </li> <li> <p>Workloads that tolerate cold start can use serverless inference. Pay for only when used.</p> </li> <li>For large payload &gt; 1GB, long processing, use batch processing. </li> <li>When the request is queued, use Asynchronous inference.</li> </ul> <p>See inference deployment options for details.</p>"},{"location":"ai-ml/sagemaker/#benefits","title":"Benefits","text":"<ul> <li>Labelling raw data and active learning</li> <li>Fully managed notebook</li> <li>Models are in OCI images</li> <li>Amazon SageMaker Clarify helps improve the machine learning (ML) models by detecting potential bias and by explaining the predictions that models make.</li> <li>SageMaker JumpStart  provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. For LLMs, it includes LLaMa-2-7b, BLOOM 176B, FLAN-T5 XL, or GPT-J 6B...</li> <li>Isolate team in a security domain which includes EBS, collaborative editing capabilities, computes, sharing spaces.</li> <li>Canvas is used to integrate with data source connectors, includes ready to use models and do ETL.</li> <li>Model endpoint can be called asynchronously with data from S3.</li> <li>Can use Sagemaker docker image for deep learning with different framework like TensorFlow, PyTorch, ...</li> </ul>"},{"location":"ai-ml/sagemaker/#key-term-definitions","title":"Key Term Definitions","text":"Term Definition Inferentia AWS custom chip which is designed for ultra-fast low cost ML inference. New library releases in Q2 2023 have made LLM compilation easier SageMaker Distributed Training SageMaker provides distributed training libraries and supports various distributed training options for deep learning tasks such as computer vision (CV) and natural language processing (NLP). With SageMaker\u2019s distributed training libraries, you can run highly scalable and cost-effective custom data parallel and model parallel deep learning training jobs. SageMaker Jumpstart Foundation Models SageMaker JumpStart provides pretrained, open-source models for a wide range of problem types to help you get started with machine learning. You can incrementally train and tune these models before deployment. JumpStart also provides solution templates that set up infrastructure for common use cases, and executable example notebooks for machine learning with SageMaker. Titan A textual foundation model developed internally to Amazon. First renditions is text to text and text to vector models. These models are zero-shot instructor models and can solve tasks like summarization, question answer, language generation, and advanced search. Trainium AWS custom chip which is designed for ultra-fast low cost ML training and inference."},{"location":"ai-ml/sagemaker/#components","title":"Components","text":"<ul> <li>Canvas: generate prediction with a no-code approach. With Canvas, we can chat with popular large language models (LLMs), access Ready-to-use models, or build a custom model trained on new data. See getting started.</li> <li>Autopilot</li> <li>Governance</li> <li>Hpyerpod Clusters</li> <li>Ground Truth</li> <li>Notebook</li> <li>Training</li> <li>Inference</li> <li>Augmented AI</li> </ul>"},{"location":"ai-ml/sagemaker/#pricing","title":"Pricing","text":"<p>Pricing information.</p>"},{"location":"ai-ml/sagemaker/#sagemaker-studio","title":"SageMaker Studio","text":"<p>A single, web-based IDE to do all the ML development tasks. It supports the classical ML development steps of:</p> <ul> <li>Prepare Data.</li> <li>Build Model using Notebook.</li> <li>Train and tune model.</li> <li>Deploy and manage.</li> </ul> <p>To use SageMaker Studio, an administrator needs to get a Domain sets up (see steps in SM immersion day).</p>"},{"location":"ai-ml/sagemaker/#domain","title":"Domain","text":"<p>A  Domain consists of an associated Amazon Elastic File System (Amazon EFS) volume; a list of authorized users; and a variety of security, application, security policies, S3 bucket, and Amazon Virtual Private Cloud configurations.  Each user in a domain receives a personal and private home directory within the EFS for their own notebooks, Git repositories, and data files. Within a domain data scientists and developers can co-work on the same data and models.</p> <p></p> <p>After creating a Domain, we got a User Portal to access the environment. Members given access to Studio have a unique sign-in URL that directly opens Studio, and they sign in with their IAM Identity Center credentials (SSO).</p> <p>Once a user from Identity Service is assigned to a Domain, she/he can start Studio:</p> <p></p> <ul> <li> <p>A user needs to create an executable environment, for example a Data Science one, with all the needed libraries to use Jupyter notebook with AWS SDK boto3. The environment is a EC2 machine (t3.medium).</p> </li> <li> <p>We can use CDK to create a Studio with scripts to perform automated tasks at setup time and code repo.</p> </li> </ul> A Simple tutorial <p>The classical model development and deployment steps are:</p> <ol> <li>Be sure to have an IAM Role created so SageMaker Studio running in a EC2 instance can access remote AWS services, like S3...</li> <li>Inside Studio, create a SageMaker notebook instance - Use Jupyter notebook with Conda and Python3</li> <li>Prepare the data: create S3 bucket, load csv source as training data set, build train and test data sets by splitting the source data.</li> <li> <p>Train the model to learn from the data, using session to cluster hosts:</p> <pre><code>import sagemaker\n# Use SageMaker estimator \nsess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(xgboost_container, role, instance_count=1, \n                instance_type='ml.m4.xlarge',\n                output_path='s3://{}/{}/output'.format(bucket_name, prefix),\n                sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, silent=0,\n            objective='binary:logistic', num_round=100)\n# fit on the training set\nxgb.fit({'train': s3_input_train})\n</code></pre> </li> <li> <p>Deploy the model:</p> <pre><code>xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')\n</code></pre> </li> <li> <p>Evaluate a ML model's performance</p> <pre><code>from sagemaker.serializers import CSVSerializer\n\ntest_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.serializer = CSVSerializer() # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\nprint(predictions_array.shape)\n</code></pre> <p>It generates a confusion matrix like:</p> <pre><code>Overall Classification Rate: 89.5%\n\nPredicted      No Purchase    Purchase\nObserved\nNo Purchase    90% (10769)    37% (167)\nPurchase        10% (1133)     63% (288) \n</code></pre> </li> </ol>"},{"location":"ai-ml/sagemaker/#the-ml-process","title":"The ML process","text":"<p>The following figure illustrates the classical Machine Learning process:</p> <p></p>"},{"location":"ai-ml/sagemaker/#data-wrangler","title":"Data Wrangler","text":"<p>Data Wrangler (File &gt; New &gt; Data Wrangler Flow) is a tool to help do feature engineering, and build data workflow.</p> <ul> <li> <p>From defining connection to data source, and import dataset:</p> <p></p> </li> <li> <p>Add a Data Analysis to the data flow, using Data Quality and Insights Report:  </p> <p></p> </li> <li> <p>Or add one to assess the correlation between one variable and the output, using a data analysis based on Histogram:</p> <p></p> </li> <li> <p>Adding table summary to the flow to get means, min, max... per column.</p> </li> <li> <p>We can also add Bias for the marital status to y:</p> <p></p> <p>We observe a class imbalance where the married class is 21% more represented than other classes. We also observe that the married class is 2.8% less likely to subscribe to a bank term deposit</p> </li> <li> <p>We can find out Target Leakage which occurs when there is data in a machine learning training dataset that is strongly correlated with the target label, but is not available in real-world data.</p> <p></p> </li> <li> <p>We can add more analysis like histograms, or add a predifined transformation, like Synthetic Minority Over-sampling Technique (SMOTE):</p> <p></p> </li> <li> <p>Remove any outliers using Standard deviation at 3 sigma.</p> <p></p> </li> <li> <p>Scale the numerical features, using min-max scaler from 0 to 1.</p> </li> <li> <p>Replace feature characters to replace '.' to '_' for categorical features.</p> <p></p> </li> <li> <p>Finally we can add custom transform in Python (user-defined function), Python Pandas, PySpark, like compute variance_inflation_factor for numerical features and drop columns with vif &gt; 1.2</p> </li> <li> <p>Do one_hot_encoding transformation for categorical data, choose all string type columns as Input columns and select Columns as Output style.</p> <p></p> </li> </ul>"},{"location":"ai-ml/sagemaker/#persist-to-feature-store","title":"Persist to Feature Store","text":"<p>We can also use the SageMaker Feature Store as a centralized storage for all the features potentially created by multiple teams and that can then also be retrieved consumed by multiple teams. The access is not obvious (Use + on last tranformation and menu &gt; Add destination &gt; Feature Store), start by defining a feature group:</p> <p></p> <p>The feature store can be online (used for real time inference) or offline (for batch training). Offline is a S3 bucket. The JSON schema helps to validate the data set.</p>"},{"location":"ai-ml/sagemaker/#processing-job","title":"Processing job","text":"<p>To do the data wrangler, SageMaker can start batch processing job using Apache Sparks.</p> <p></p> <p>The trained parameters have been fitted on the available data in Data Wrangler with certain transforms including One-hot encode, Scale values and Standard deviation numeric outliers. These transforms depend on the available data and must be re-fitted when new data is available.</p> <p>We can schedule the job to run at certain time of the day, from different input file.</p> <p>The outcome of the batch processing job will be a set of parquet files with the processed data saved in the 'off-line store', a flow definition, and spark configuration files.</p>"},{"location":"ai-ml/sagemaker/#develop-with-sklearn-library-and-deploy-with-sagemaker","title":"Develop with sklearn library and deploy with SageMaker","text":"<p>A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later</p> <ul> <li>See the tutorial: Using SKLearn with SageMaker and a code to compute a RandomForest for a SaaS company assessing the risk of their customers to churn big-data-tenant-analytics &gt; CompanyRisk.</li> <li>See also: Using the SageMaker Python SDK: SageMaker Python SDK provides several high-level abstractions for working with Amazon SageMaker.</li> </ul> <p>Scripts, training and test data sets are in s3. The SDK helps to access SageMaker constructs and deploy the model as a service with reachable endpoint. The endpoint runs a SageMaker-provided Scikit-learn model server and hosts the model produced by our training script.</p> <p>As SageMaker Pythin SDK uses a Docker image to create sklearn environment we can define our own image with our custom libraries via a <code>requirements.txt</code>.</p> <p>We run Scikit-learn training scripts on SageMaker by creating SKLearn Estimators.</p>"},{"location":"ai-ml/sagemaker/#jumpstart","title":"Jumpstart","text":"<p>Amazon SageMaker JumpStart provides a set of foundation models, build-in algorithms and prebuilt ML solutions for common use cases that can be deployed readily in just a few steps. JumpStart is a ML hub with 400+ built-in algorithms with pre-trained models including foundation models. The price is based on the resources used.</p> <p>Consider Jumpstart when:</p> <ul> <li>The best LLM for the task is not in Bedrock</li> <li>The size of requests and/or request frequency of the use case makes Jumpstart more cost effective \u2014 Paying by the hour instead of the token.</li> <li>The terms and conditions of Bedrock models don\u2019t meet the use case.</li> <li>Using JumpStart, we can perform inference on a pre-trained model, even without fine-tuning it first on a custom dataset. We can also fine tune a model on specific dataset.</li> <li>Deploying a model, differs by model and instance availability of the required instance type.</li> </ul> Generate text with Prompt only approach <p>The following code can be integrated into a Jupyter Notebook: it uses Google Flan T5 LLM, with dialog data sets from Hugging Face, transformers tokenizer, and a prompt to expect a summarization of the conversation between two humans:</p> <pre><code>from transformers import AutoTokenizer\nfrom transformers import AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\n# Load a dialog data sets from Hugging Face\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\ndataset = load_dataset(huggingface_dataset_name)\n\nmodel_checkpoint='google/flan-t5-base'\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n# Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.\n# Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n\nstart_prompt = 'Summarize the following conversation.\\n'\nend_prompt = '\\n\\nSummary: '\ndialogue = dataset['test'][example_indices[0]]['dialogue']\nsummary = dataset['test'][example_indices[0]]['summary']\nprompt = f'{start_prompt}{dialogue}{end_prompt}'\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\nprint(f'INPUT PROMPT:\\n{prompt}\\n')\nprint(f'MODEL GENERATION:\\n{output}\\n')\nprint(f'BASELINE SUMMARY:\\n{summary}')\n</code></pre>"},{"location":"ai-ml/sagemaker/#deeper-dive","title":"Deeper dive","text":"<ul> <li> Onboard to Amazon SageMaker Domain Using IAM Identity Center to define user in IAM-IC and then use Domain in SageMaker to authorize users to login via SSO.</li> <li> Labs: Creating a scikit-learn Random Forest Classifier in AWS SageMaker. Applied to company risk to churn demo in this folder.</li> <li> Using the SageMaker Python SDK.</li> <li> Examples of using SageMaker Python SDK.</li> <li> Amazon SageMaker Workshops</li> <li> SageMaker Immersion Day</li> <li>gitHub Amazon SageMaker Examples including Inference Recommender.</li> <li>scikit_learn_data_processing_and_model_evaluation</li> <li> Add permissions to a Amazon SageMaker Studio account when we need to enable access from Studio to SageMakerAPI using the IAM policies <code>AmazonSageMakerFullAccess</code> and <code>AWSCloudFormationFullAccess</code>. The lab is little bit old, so now in SageMaker we need to access users via Domain.</li> <li>MLOps deployment best practices for real-time inference model serving endpoints with Amazon SageMaker.</li> <li> Generative AI and Data Science on AWS.</li> <li> Generative AI on Amazon SageMaker - Workshop: addresses SageMaker Quickstart solutions for text-text and text-image models, prompt engineering, fine-tuning Llama2, genAI use cases, coding Llama2.</li> <li>Personalized Movie Tag-Line Recommendations using Amazon Bedrock and Amazon Personalize.</li> </ul> <p>Please ensure that the role \"arn:aws:iam::4...:role/service-role/AmazonSageMaker-ExecutionRole-20221207T113525\" exists and that its trust relationship policy allows the action \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\". Also ensure that the role has \"s3:GetObject\" permissions and that the object is located in us-west-2.</p>"},{"location":"analytics/","title":"Analytics Services","text":"<p>Analytics helps companies decide where and when to launch new products, when to offer discounts, and when to market in new areas.</p> <p>Data analysis is the process of interpreting data that leads to meaningful decisions. Data analysis is a small part of data analytics.  Data analytics uses raw data captured from many sources to process, analyze, and interpret what may happen and how an organization can use this knowledge for its benefit. </p> <p>There are four types of analytics:</p> <ul> <li>Descriptive analytics helps answering \"What happened?\". It provides insights into trends and patterns. It uses data visualization.</li> <li>Diagnostic analytics is characterized by techniques, such as drill-down, data discovery, data mining, and correlations.</li> <li>Predictive analytics answers the question of \"What might happen?\". It uses historical data and statistical algorithms to make forecasts about future outcomes. It includes machine learning, forecasting, pattern matching, and predictive modeling.</li> <li>Prescriptive analytics is based on predictive analytics, to actually recommend actions or responses to the predicted outcome. It includes machine learning, graph analysis, simulation, complex event processing, neural networks, and recommendation engines.</li> </ul> <p>The AWS portfolio of fully managed analytics services includes a cloud-native big data platform, a log analytics and search engine for operational data and a distributed streaming platform for real-time data movement.</p> Document history <p>Created Nov 23 - Updated 01/30/2024</p>"},{"location":"analytics/#domain-knowledge","title":"Domain Knowledge","text":"<p>Each enterprise has data management platform challenges to be able to increase volumes, velocity and variety of data.</p> <p>Data an analytics domain of knowledge addresses the following subjects.</p>"},{"location":"analytics/#data-collection","title":"Data Collection","text":"<p>Data collection is gathering data from multiple sources to a single store.</p> <ul> <li>Evaluate costs associated with data acquisition, transfer, and provisioning from various sources into the collection system.</li> <li>Determine data persistence at various points of data capture.</li> <li>Identify the latency characteristics of the collection system.</li> <li>Select the data collection solution</li> </ul>"},{"location":"analytics/#storage-and-data-management","title":"Storage and data management","text":"<p>Structured and semi-structured data are stored in database, while unstructured data are in data lakes. Transactional databases are called OLTP (online transaction processing) databases. In an OLTP system, the most common queries are called lookup queries. OLAP (online analytical processing) is associated more with aggregate queries that only need a few columns.</p> <ul> <li>Determine data access and retrieval patterns.</li> <li>Select appropriate data layout, schema, structure, and format.</li> <li>Define data life cycles based on usage patterns and business requirements.</li> </ul>"},{"location":"analytics/#data-processing","title":"Data processing","text":"<p>Data transformation is the formatting, organizing, and controlling of data.</p> <ul> <li>Design a solution to transform and prepare data for analysis, applying appropriate ETL or ELT techniques. With ELT, all data cleansing, transformation, and enrichment occur within the data warehouse.</li> <li>Implement failover, scaling, and replication mechanisms.</li> </ul>"},{"location":"analytics/#analysis-and-visualization","title":"Analysis and Visualization","text":""},{"location":"analytics/#data-security","title":"Data security","text":"<ul> <li>Apply data governance and compliance controls.</li> </ul>"},{"location":"analytics/#aws-athena","title":"AWS Athena","text":"<p>AWS Athena runs analytics directly on S3 files or federated storage, using SQL language to query the files (CSV, JSON, Avro, Parquet...). <code>S3 Access Logs</code> log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files. </p> <ul> <li>No need to load the data to Athena, the query is executed on top of S3.</li> <li>Based on Presto for DML queries and Apache Hive for datawarehouse and SQL queries.</li> <li>Queries are done on high availability capability so will succeed, and scale based on the data size.</li> <li>No need for complex ETL jobs to prepare our data for analytics.</li> <li>Athena integrates with Amazon QuickSight for easy data visualization.</li> <li>Integrated with AWS Glue Data Catalog, allowing us to create a unified metadata repository across various services, crawl data sources to discover schemas and populate our Catalog with new and modified table and partition definitions, and maintain schema versioning.</li> <li>Pricing pet TB of data scanned.</li> <li> <p>It also includes <code>Federated Query</code> to run SQL queries across data stored in relational and non-relational , object, and custom data sources. It uses the Data Source Connectors which executes a Lambda to run the Federated Query.</p> </li> <li> <p>Prefer using Apache Parquet data format for better performance and optimized cost. It is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON</p> </li> <li>Partition our data in S3 folder.</li> </ul> CSV to Parquet <p>For Python, Pandas support it by reading the csv file into dataframe using read_csv and writing that dataframe to parquet file using <code>to_parquet</code>. Apache Drill has also such tool. In Spark the data frame has write.parquet API. Finaly AWS Glue can also do such transformation.</p>"},{"location":"analytics/#simple-demo-script","title":"Simple demo script","text":"<ul> <li>Create a S3 bucket to keep results of Athena queries.</li> <li>Create a second S3 bucket to keep source data, and upload a csv file as data source.</li> <li> <p>Create a database in Athena:</p> <pre><code>CREATE DATABASE mydatabase\n</code></pre> </li> <li> <p>Define SQL query to create table to match the source (external table) and run it in the Editor.</p> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS tablename-datasource (\n    `Date` DATE,\n    Time STRING,\n    Location STRING, ... \n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ','\nLINES TERMINATED BY '\\n'\nLOCATION 's3://url-to-bucket'\n</code></pre> </li> <li> <p>Use the SQL query Editor to select columns, do any type of query.</p> </li> </ul>"},{"location":"analytics/#lambda-example","title":"Lambda example","text":"<p>It is possible to run Athena query from Lambda function and get the result pushed in an output S3 bucket or in another place.</p> <p>See the lambda code in the labs/lambdas/s3-aurora folder.</p> <p>Also see the boto3 Athena API.</p>"},{"location":"analytics/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Product documentation</li> <li>Getting started with Athena</li> <li>How do I analyze my Amazon S3 server access logs using Athena?</li> <li>See also code sample.</li> <li>Calling SageMaker function from an Athena Query to do ML.</li> </ul>"},{"location":"analytics/#aws-batch","title":"AWS Batch","text":"<p>As a fully managed service, AWS Batch helps to run batch computing workloads of any scale across multiple AZs within a region. </p> <p>Job definition specifies how jobs are to be run, and it can use container image. </p> <p>A compute environment is a set of managed or unmanaged compute resources (Fargate or EC2) that are used to run jobs. For EC2 it uses a dynamic provisioning with option to use Spot instances.</p> <p>A job queue stores the submitted jobs until the AWS Batch Scheduler runs the job on a resource in the compute environment.</p> <p>It used EBS for temporary storage.</p> <p>Can be scheduled with EventBridge schedule, can also being orchestrated with Step function.</p>"},{"location":"analytics/#quicksight","title":"QuickSight","text":"<p>Dashboard tool, serverless, machine learning powered BI service.</p> <p>Two type of persona: dashboard developers and end-users (read-only on the dashboard).</p> <p>Integrated with RDS, Aurora, Athena, S3, RedShift, OpenSearch, Timestream, with Saleforce, Jira... It can integrate to any JDBC compliant database. It can import CSV, XLSX, JSON, TSV files and log files.</p> <p>If data is imported inside QuickSight, it uses in memory computation using SPICE (Super-fast, Parallel, In-memory Calculation Engine) engine.</p> <p>The development process is described in the figure below (link to AWS doc)</p> <p></p> <ul> <li>Defining a dataset and then working in removing column, applying filters, changing field names or data types, adding calculated fields, or use SQL to joins between tables.</li> </ul> <p>Here is an example of integration with a table in Aurora</p> <p></p> <ul> <li>Add Visual. A Visual can include multiple dataset and then multiple sheets. An Interactive Sheet is a collection of data expressed in visuals that users can interact with when the sheet is published to a dashboard. A Paginated Report is a collection of tables, charts, and visuals that are used to convey business critical information. </li> </ul> <p></p> <ul> <li>Create Dashboard from an Analysis.</li> <li>Share dashboard so it can be seen by end users.</li> </ul> <p>With enterprise edition we can define groups of users, and share dataset, visual and dashboard with group or individual user.</p> <p>The dataset can be shared between developers so they can develop their own analysis. Visualizations can also be shared during development, then the readonly dashboard is shared to end users.</p> <p>To get input data for a dashboard we can define parameters. Parameters can also be used for exchanging context between sheets.</p> <p>QuickSight can generate smart queries that uses only required tables rather than joining in all tables defined in the dataset.</p>"},{"location":"analytics/#some-how-to","title":"Some how to","text":"<ul> <li>Invite user using the right top menu (human icon), and Manage QuickSight, then invite people by email. Create Group, and then add users. The 3 letters to search really need to be the first 3 letters.</li> <li>Once data are in SPICE, they need to be refreshed from the data source to get the last records updates. A refresh can be scheduled.</li> <li>Add trend and add X axis variable coming from the dataset and Value for Y.</li> <li>To add a widget to filter the data, use filter, select the column and add it to the current sheet (contextual menu)</li> <li>To add a transition from a sheet to another by passing the value of the selected elements.</li> </ul>"},{"location":"analytics/#deeper-dive_1","title":"Deeper Dive","text":"<ul> <li>youtube channel for QuickSight.</li> <li>Demo Central - learning</li> <li>Very good workshop. 5 stars</li> <li>Build your first quicksight dashboard - video</li> <li>Embedding Amazon QuickSight dashboards in your applications. </li> <li>Embedding Analytics dashboard in application.</li> </ul>"},{"location":"analytics/#example-of-a-big-data-ingestion-pipeline","title":"Example of a big data ingestion pipeline","text":"<p>The requirements are:</p> <ul> <li>Use serverless ingestion pipeline.</li> <li>Collect and transform data in real-time.</li> <li>Support SQL query on transformed data.</li> <li>Persist SQL results into S3.</li> <li>Keep into warehouse and create dashboard.</li> <li>Data could come from IoT devices</li> </ul> <p>A potential architecture will be:</p> <p></p> <ul> <li>Using AWS Glue Data Catalog as the Metastore for Hive</li> <li>Using an External MySQL Database or Amazon Aurora as Hive metastore</li> </ul>"},{"location":"analytics/emr/","title":"Elastic MapReduce - EMR","text":"<p>This is the managed service for big data solution implementation using petabyte-scale datasets.  It is used for data processing, interactive analytics, and ML using open-source frameworks such as Apache Spark, Apache Hive, Apache Flink, or Presto.</p> <p>It offers four different ways to deploy applications:</p> <ul> <li>Serverless</li> <li>On EC2: closest deployment environment to a YARN-based Hadoop platform</li> <li>On EKS: when EKS is the deployment standard</li> <li>On Outposts</li> </ul> <p>EMR supports virtually unlimited storage capacity through the EMRFS backed by Amazon S3, which can be shared across multiple EMR clusters or Amazon EMR Serverless. It also supports HDFS backed by EBS volumes and instance stores.</p>"},{"location":"analytics/emr/#value-propositions","title":"Value propositions","text":"<ul> <li>No cluster to operate (with the Serverless deployment)</li> <li>Avoid over or under-provisioning resources</li> <li>Supports Apache Iceberg to create tables and query, merge, and insert data  </li> <li>Auto determination of resources, compute and memory, needed to run analytics jobs</li> <li>Automatically scales workers up or down based on the workload and parallelism required at every stage of the job graph</li> <li>Track usage cost by applications</li> <li>Easy integration with AWS Redshift, Athena, S3 and data ingestion platform like Kinesis, MSK</li> <li>Integrate with AWS Service Catalog for managing EMR applications, and AWS Lake Formation for fine grained access control to the data.</li> </ul>"},{"location":"analytics/emr/#emr-ec2-cluster","title":"EMR EC2 cluster","text":"<p>EMR is a cluster of EC2 instances which are nodes in Hadoop (HDFS). We can use Reserved Instances and Spot instances to reduce costs. There are three node types:</p> <ul> <li>Master nodes: coordinate cluster, and distribution of data and tasks among other nodes. </li> <li>Core nodes: run tasks and store data in the Hadoop Distributed File System (HDFS) </li> <li>Task nodes: (optional)  run tasks and do not store data in HDFS</li> </ul> <p>EMR comes bundled with Spark, HBase, Presto, Hive, Flink...  When launching a cluster, it performs bootstrap actions to install custom software and applications. When the cluster is in running state, we can submit work to it. Work includes a set of steps. The cluster can auto terminate at the end of the last step.</p> <p>We can submit one or more ordered steps to an Amazon EMR cluster. Each step is a unit of work that contains instructions to manipulate data for processing by software installed on the cluster.</p> <p>For auto scaling of the task nodes, it uses Spot instances. Master nodes should be on Reserved Instances.</p>"},{"location":"analytics/emr/#emr-cluster-use-cases","title":"EMR cluster Use cases","text":"<ul> <li>Big data analytics: what-if analysis using statistical algorithms and predictive models to uncover hidden patterns, correlations, market trends...</li> <li>Scalable data pipelines: process it at petabyte scale.</li> <li>Real-time data streams: to create long-running, highly available, and fault-tolerant streaming data pipelines.</li> <li>Analyze data and ML adoption.</li> </ul> <p>AWS Lake Formation integrates with Amazon EMR to set up, secure, and manage data lakes. You can use Lake Formation permissions together with the AWS Glue Data Catalog to provide fine-grain, column-level access to the data lake. Jobs that we submit with Amazon EMR steps can use job-centric runtime roles to access AWS resources, such as objects in Amazon S3.</p>"},{"location":"analytics/emr/#getting-started-emrec2","title":"Getting Started EMR/EC2","text":"<p>See the getting started tutorial with Spark, PySpark scripts stored in S3 (details below). </p> Tutorial <p>The Python code and data sets are in the folder: labs/analytics/emr-starting. The goal is to process food establishment inspection data.</p> <ul> <li>Create a cluster using the script <code>create-cluster.sh</code> (it uses <code>aws emr create-cluster</code> command).</li> <li>Unzip data sources in a S3 bucket (e.g. <code>s3://jb-data-set/restaurants/</code>)</li> <li> <p>In the console, once the cluster is in waiting mode, add a Step with Spark Application, in cluster deployment mode, </p> <p></p> <p>Or run <code>deploy-app.sh</code> (it uses <code>aws emr add-steps</code> command).</p> </li> <li> <p>The results looks like</p> <pre><code>name,total_red_violations\nSUBWAY,322\nT-MOBILE PARK,315\nWHOLE FOODS MARKET,299\n...\n</code></pre> </li> </ul> <p>For other EMR examples see the playground and Spark Studies.</p> <p>See Pricing based on EC2 type and region.</p>"},{"location":"analytics/emr/#emr-serverless","title":"EMR Serverless","text":"<p>The newest and easiest way for data analysts and engineers to run open-source big data analytics frameworks without configuring, managing, and scaling clusters and servers.</p> <p>End users work from EMR Studio using notebooks, but we can also submit jobs via APIs and CLI. </p> <p>It uses the concept of applications to keep configurations and metadata. A job is a request submitted to an Amazon EMR Serverless application that runs asynchronously and is tracked through completion. Jobs are run in a single Availability Zone to avoid cross-AZ network communication. In the figure below the green layer is owned by the user, with data in S3, while the orange layer is managed by AWS. Spark or Hive engines run jobs on the data coming from S3 (in fact data can come from a lot of different data sources). </p> <p></p> <p>No need to right-size clusters for varying jobs and data sizes. It automatically adds and removes workers at different stages of the job. EMR is charged for aggregated vCPU, memory, and storage resources used from the time a worker starts running until it stops.</p> <ul> <li>We can submit jobs using workflow orchestration services like AWS Step Functions, Apache Airflow, or AWS Managed Workflows for Apache Airflow.</li> <li>Logging: By default, EMR Serverless stores application logs securely in Amazon EMR managed storage for a maximum of 30 days. Before our jobs can send log data to Amazon S3, we must allow <code>s3:PutObject</code> on the <code>arn:aws:s3:::.../*</code> s3 bucket, in the permissions policy for the job runtime role. </li> <li> <p>Monitoring with CloudWatch custom dashboard: See the CloudFormation definition under lab/analytics/emr-serverless folder and using the command <code>./defineCWdashboard.sh</code>, we can get a dashboard for the Serverless EMR application:</p> <p></p> <p>So we need to define one dashboard per application.</p> <p>Every minute EMR Serverless emits (CPUAllocated, IdleWorkerCount,MaxCPUAllowed) metrics at the application level and at the worker-type and capacity-allocation-type levels.</p> </li> </ul>"},{"location":"analytics/emr/#emr-serverless-use-cases","title":"EMR Serverless Use cases","text":"<ul> <li>Spark ETL.</li> <li>Large scale SQL queries using Hive.</li> <li>Interactive analysis using Jupyter notebooks with EMR Studio.</li> <li>Analysis using Presto</li> <li>Real-time streaming data pipelines: perform fault-tolerant stream processing of live data streams using Apache Spark or Apache Flink data frameworks.</li> <li>AI/ML: pre-process data and train models and perform prediction and validation to build accurate ML models. It may use Spark MLlib, TensorFlow, and Apache MXNet. </li> </ul>"},{"location":"analytics/emr/#emr-serverless-getting-started","title":"EMR Serverless - Getting Started","text":"<p>Source is the tutorial - getting started. </p> Details <ul> <li>We need an IAM role with a custom trust policy to enable others to perform actions in this account (see role <code>EMRServerlessS3RuntimeRole</code> and security policy <code>EMRServerlessS3AndGlueAccessPolicy</code>). </li> <li> <p>Use EMR Studio and create an application. We can now use Graviton as CPU:</p> <p></p> </li> <li> <p>Define the PySpark script to be used and put it in a S3 bucket. For example WordCount.py</p> <pre><code>aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://jb-data-set/scripts/\n</code></pre> </li> <li> <p>Define a job using the script, using the Spark properties of: <code>--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1</code></p> <p></p> </li> <li> <p>Once the job runs the status shows as <code>Success</code>, we  can view the output of the job in the S3 bucket.</p> </li> <li>Log should be in logs folder.</li> <li> <p>Delete output from s3 bucket: <code>aws s3 rm s3://jb-data-set/emr-serverless-spark/ --recursive</code></p> </li> <li> <p>WordCount.py app with CLI: Scripts are under labs/analytics/emr-serverless</p> </li> <li> <p>If the application was not created before like in manual step above, use the following command: (which is in the script <code>createApplication.sh</code>) </p> <pre><code>aws emr-serverless create-application --release-label emr-6.8.0 --type \"SPARK\"  --name My_First_Application\n</code></pre> </li> <li> <p>Get the application ID: <code>./getApplicationId.sh My_First_Application</code></p> </li> <li> <p>Be sure to have the wordcount.py in the scripts folder in s3 bucket</p> <pre><code>aws s3 cp s3://us-east-1.elasticmapreduce/emr-containers/samples/wordcount/scripts/wordcount.py s3://DOC-EXAMPLE-BUCKET/scripts/\n</code></pre> </li> <li> <p>Get the role ARN </p> <pre><code>aws iam list-roles | jq -r '.Roles[] | select(.RoleName==\"EMRServerlessS3RuntimeRole\") | .Arn '\n</code></pre> </li> <li> <p>Submit the job:  <code>./submitJob.sh</code>. The submission output looks like:</p> <pre><code>{\n\"applicationId\": \"00f6b0eou5biqd0l\",\n\"jobRunId\": \"00f6b25bek7v3f0l\",\n\"arn\": \"arn:aws:emr-serverless:us-west-2:....:/applications/00f6b0eou5biqd0l/jobruns/00f6b25bek7v3f0l\"\n}\n</code></pre> </li> </ul>"},{"location":"analytics/emr/#emr-on-eks","title":"EMR on EKS","text":"<p>Advantages:</p> <ul> <li>Run with other workload deployed on EKS. Fully managed lifecycle of the EMR jobs.</li> <li>3x faster performance.</li> <li>Improves resource utilization and simplifies infrastructure management across multiple Availability Zones.</li> <li>Deploy in seconds instead of minutes.</li> <li>Centrally manage a common computing platform to consolidate EMR workloads with other apps. Access to built-in monitoring and logging functionality.</li> <li>Reduce operational overhead with automated Kubernetes cluster management and OS patching</li> </ul> <p></p> <ul> <li>Amazon EMR uses virtual clusters to run jobs and host endpoints. A virtual cluster is a Kubernetes namespace that Amazon EMR is registered with. </li> </ul>"},{"location":"analytics/emr/#deployment-and-setup","title":"Deployment and setup","text":"<ul> <li>Prepare a EKS cluster</li> <li>Amazon EMR on EKS needs CoreDNS for running jobs on EKS cluster. So update CoreDNS if needed.</li> <li>Enable cluster access for Amazon EMR on EKS to a specific namespace by creating a k8s role, role binding to a k8s user, and map this user to the service linked role <code>AWSServiceRoleForAmazonEMRContainers</code>.</li> <li>Enable IAM Roles for Service Accounts (IRSA) on the EKS cluster by creating an OIDC identity provider</li> <li>Create a job execution role</li> </ul>"},{"location":"analytics/emr/#deeper-dive","title":"Deeper dive","text":"<ul> <li>EKS workshop with EMR</li> <li>Blog Run Big Data Applications without Managing Servers </li> </ul>"},{"location":"coding/","title":"Coding practices","text":"<p>Update</p> <p>Created 09/20/2022 - Updated 05/03/2024</p>"},{"location":"coding/#how-to-reduce-the-cost-as-much-as-possible","title":"How to reduce the cost as much as possible","text":"<ul> <li>Using aws CLI can be done from own computer, using access key. But keys are saved on computer vias the <code>aws configure</code>. The alternate is to use CloudShell. </li> <li>CloudShell is free </li> <li>Try to do cloud formation stack, CDK or SAM and then destroy resources once proof of concept is done.</li> <li>EC2 try to use the configuration best fitted for the need</li> <li>When using Cloud9, we pay for the EC2 we used to edit code.</li> <li>Sagemaker is very expensive</li> </ul>"},{"location":"coding/#aws-cli-common-commands","title":"AWS CLI common commands","text":"<p>We can access AWS using the CLI or the SDK which both user access keys generated from the console (&gt; Users &gt; jerome &gt; Security credentials &gt; Access Keys).</p> <p>The keys are saved in <code>~/.aws/credentials</code> in different profile:</p> <ul> <li>A named profile is a collection of settings and credentials that we can apply to a AWS CLI command. When you specify a profile to run a command, the settings and credentials are used to run that command.</li> <li>Installation: aws cli</li> <li>The cli needs to be configured: <code>aws configure</code> with the access key ID, and secret, and region to access. Use IAM user to get a new credentials key. The credentials and API key are in <code>~/.aws/credentials</code> in default profile. </li> <li>We can use the aws profile to change user</li> </ul> <p>Test with some commands:</p> <pre><code>aws --version\n# get the users in iam. The currently logged user needs to have the permissions to list iam resources.\naws iam list-users\n# For a given profile\naws iam list-users --profile hackaton\n</code></pre> <ul> <li>VPC scenario with CLI - Tutorial</li> <li>Use CloudShell in west-2 region for using aws cli</li> <li>aws-shell is also available to facilitate the user experience using a laptop terminal console.</li> </ul> <p>When using CLI in a EC2 instance, always use an IAM role to control security credentials. This role may come with a policy authorizing exactly what the EC2 instance should be able to do. </p> <p>Also within a EC2 instance, it is possible to use the URL http://169.254.169.254/latest/meta-data to get information about the EC2. We can retrieve the IAM Role name from that metadata.</p>"},{"location":"coding/#sdk","title":"SDK","text":"<p>Supports different programming languages to integrate with a lot of AWS managed services from any business application. </p> <p>The Python AWS SDK is boto3, with product doc.</p>"},{"location":"coding/#installation","title":"Installation","text":"<pre><code>pip install boto3[crt]\n</code></pre> <p>Set up authentication credentials for your AWS account using either the IAM Console or the AWS CLI.</p> <pre><code>aws configure\n# Verify access\naws iam list-users\n</code></pre>"},{"location":"coding/#localstack","title":"LocalStack","text":"<p>LocalStack is a container to get started with developing and testing AWS cloud &amp; serverless applications locally. It uses:</p> <ul> <li>A docker container to support the AWS services API (it supports even CloudFormation)</li> <li>an <code>awslocal</code> CLI to do the same actions as <code>aws</code> CLI but to the localstack.</li> </ul> <p>There are a lot of tutorials available from this web site see also my example for secret access from Quarkus app and this docker compose file to start the local stack.</p> <p>Quarkus dev mode is using LocalStack as soon as the AWS SDK library is added to the <code>pom.xml</code>.</p>"},{"location":"coding/#devops","title":"DevOps","text":""},{"location":"coding/#cloudformation","title":"CloudFormation","text":"<p>See separate note.</p>"},{"location":"coding/#app-2-container","title":"App 2 container","text":"<p>AWS App2Container is a tool that helps you break down the work of moving your applications into containers, and configuring them to be hosted in AWS using the Amazon ECS, Amazon EKS, or App Runner container management services.</p>"},{"location":"coding/#codecommit","title":"CodeCommit","text":"<p>Version control fully managed service to manage Git repositories. HA, secured, encryption at rest and in transit. </p> <p>Be sure to get the Git Credentials for the IAM user we will use to do the Git repository actions. </p> <ul> <li> <p>Setup SSH connection to CodeCommit</p> </li> <li> <p>Create a repository (for the sam-app created below)</p> <pre><code>aws codecommit create-repository --repository-name sam-app\n</code></pre> </li> <li> <p>If not installed yet, get git-remote-codecommit</p> <pre><code>pip install git-remote-codecommit\n</code></pre> </li> <li> <p>Add origin and git in current folder</p> <pre><code>git init -n main\ngit remote add origin codecommit://sam-app\n# Example of build content not to push\necho -e \"\\n\\n.aws-sam\" &gt;&gt; .gitignore\ngit add .\ngit commit -m \"first drop\"\ngit push -u origin main\n</code></pre> </li> </ul>"},{"location":"coding/#aws-elastic-beanstalk","title":"AWS Elastic Beanstalk","text":"<p>With Elastic Beanstalk, developers upload their application. Then, Elastic Beanstalk automatically handles the deployment details of EC2 capacity provisioning, load balancing, auto-scaling, RDS and application health monitoring.</p> <ul> <li>It is a managed service.</li> <li>Use the concept of application, which is a collection of Beanstalk components (environments, versions, configurations).</li> <li>Instance configuration and OS is managed by Beanstalk</li> <li>Deployment strategy is configurable. It uses CloudFormation to deploy the application and the environment.</li> <li> <p>It defines two pre-configured environments:</p> <ul> <li>Web Server Tier: classical ELB, Auto scaling group and EC2s.</li> <li>Worker environment with the use of SQS queue.</li> </ul> </li> <li> <p>Three architecture models:</p> <ul> <li>Single instance</li> <li>LB + ASG</li> <li>ASG only, for worker type of application.</li> </ul> </li> <li> <p>Support blue/green deployment</p> </li> <li>A custom AMI can improve provisioning times when instances are launched in our environment if we need to install a lot of software that isn't included in the standard AMIs.</li> <li>No charge.</li> </ul>"},{"location":"coding/#elastic-container-registry","title":"Elastic Container Registry","text":"<p>AWS managed container image registry service that is secure, scalable, and reliable. </p> <p>An Amazon ECR repository contains your Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts. One repository per app.</p> <p></p> <p>Client must authenticate to Amazon ECR registries as an AWS user before it can push and pull images.</p> <p>You can control access to your repositories and the images within them with repository policies.</p> <p>As a developer you need AWS CLI and Docker.</p> <p>Pricing: pay for the amount of data you store in your repositories and for the data transfer from your image pushes and pulls.  50 GB per month of always-free storage for their public repositories. For private 500MB first year. Data transfer to services within the same region is free of charge.</p>"},{"location":"coding/#demonstration","title":"Demonstration","text":"<ul> <li>Create one ECR repository per app or microservice. Use AWS cli or AWS console to create the repository:</li> </ul> <pre><code>aws ecr help\n# Get the authentication token and authenticate the docker client\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin &lt;ecr-endpoint&gt;.amazonaws.com\n# ecr end point is the form accountID.dkr.ecr.regionID.amazonaws.com\n# ~/bin/loginECR is the script to use in my env\naws ecr create-repository --repository-name acr-car-ride-mgr\n</code></pre> <ul> <li>From my Laptop use <code>docker build</code> with the ECR URL + repo:tag.</li> </ul> <pre><code># Can also use the docker cli, see The View push commands for your repository\ndocker tag acr-car-ride-mgr:latest &lt;...&gt;us-west-2.amazonaws.com/acr-car-ride-mgr:latest\ndocker push  &lt;...&gt;.amazonaws.com/acr-car-ride-mgr:latest\n</code></pre> <p>If you want to run your application using docker engine inside of EC2, create a simple EC2 and then ssh to it and add docker, and do a docker run. Here are the installation you need:</p> <pre><code>sudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nsudo apt-get install docker-ce docker-ce-cli containerd.io\napt-cache madison docker-ce\nsudo apt-get install docker-ce docker-ce-cli containerd.io\nsudo apt install docker.io\n</code></pre> <p>Then SSH to the EC2 instance and pull the image from ECR, then start the image</p> <pre><code>docker pull \ndocker run -p 8080:8080 ....\n</code></pre>"},{"location":"coding/#app-runner","title":"App Runner","text":"<p>To be done</p>"},{"location":"coding/#chalice","title":"Chalice","text":"<p>A python framework to build serverless applications. We can have a REST API deployed to Amazon API Gateway and AWS Lambda in minutes.</p> <p>See tutorials for REST APIs, events apps...</p> <p>Code sample for car ride in aws-cdk-project-templates repo</p>"},{"location":"coding/#aws-sam-serverless-application-model","title":"AWS SAM - Serverless Application Model","text":"<p>See more detailed article.</p>"},{"location":"coding/#codepipeline","title":"CodePipeline","text":"<p>AWS CodePipeline is a continuous delivery service.</p> <ul> <li>Getting started</li> <li>Pricing 1$ / month per pipeline. All pipelines are free for the first 30 days.</li> </ul>"},{"location":"coding/#sam-build-codepipeline","title":"SAM build CodePipeline","text":"<ul> <li> <p>Create a CodePipeline with SAM CLI</p> <pre><code>sam pipeline init --bootstrap\n# Select the different account, region and default values\n</code></pre> <p>The CloudFormation templates create all the required resources for dev and prod environments</p> </li> </ul> <p>Now that AWS SAM has created supporting resources, we'll continue to create a CloudFormation template that will define our entire CI/CD pipeline.</p> <ul> <li> <p>Define the pipeline as a new cloudformation template (after git push):</p> <pre><code>sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM\n</code></pre> <p></p> <p>During the Deploy steps, cloudFormation stacks are built for each environments:</p> <p></p> <p>Once the deployment steps are done the two new cloudformation stacks are executed</p> <p></p> </li> <li> <p>Get URL endpoint of the two APIs deployed:</p> <pre><code>export DEV_ENDPOINT=$(aws cloudformation describe-stacks --stack-name sam-app-dev | jq -r '.Stacks[].Outputs[].OutputValue | select(startswith(\"https://\"))')\ncurl -s $DEV_ENDPOINT\nexport PROD_ENDPOINT=$(aws cloudformation describe-stacks --stack-name sam-app-prod | jq -r '.Stacks[].Outputs[].OutputValue | select(startswith(\"https://\"))')\ncurl -s $PROD_ENDPOINT\n</code></pre> </li> </ul>"},{"location":"coding/#codebuild","title":"CodeBuild","text":"<p>AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy.</p>"},{"location":"coding/#codedeploy","title":"CodeDeploy","text":"<p>The key concepts to keep in mind are:</p> <ul> <li>Application: tells AWS CodeDeploy what to deploy and how to deploy it. For AWS Lambda application specifies a new version of a Lambda function and how to shift traffic to it.</li> <li>Deployments: lists an application's current and past deployments.</li> <li>Deployment groups: includes details about a target environment, how traffic shifts during a deployment, and monitoring settings.</li> <li>Revisions: contains a version of the source files to deploy to your instances or specifies the version of a Lambda function to shift traffic to.</li> </ul>"},{"location":"coding/#codestar","title":"CodeStar","text":"<p>Update</p> <p>Deprecated mid 2024</p> <p>AWS CodeStar provides a unified user interface, enabling us to easily manage our software development activities in one place.</p> <p>To start a project, we can choose from a variety of AWS CodeStar templates for Amazon EC2, AWS Lambda, and AWS Elastic Beanstalk. We have the option to choose AWS CodeCommit or GitHub to use as the project\u2019s source control.</p> <p>There is no additional charge for AWS CodeStar.</p> <ul> <li>Getting started</li> <li>Product documentation</li> </ul>"},{"location":"coding/#application-composer","title":"Application Composer","text":"<p>Visual designer to develop serverless app, with infrastructure as code CloudFormation templates, following AWS best practices. Product doc.</p> <ul> <li>Can load an exiting CF templates.</li> <li>Can be synch with local work on VSCode IDE: AWS SAM templates are automatically saved to your local machine, making deployment a faster and more seamless experience.</li> <li>Deploy with SAM</li> </ul> <p>Benefits: designing what we want to accomplish and let Application Composer build how it is accomplished.</p>"},{"location":"coding/#aws-proton","title":"AWS Proton","text":"<p>Automated infrastructure as code provisioning and deployment of serverless and container-based applications. It uses the concepts of environment templates (defines shared infrastructure used by multiple applications) and service templates (one application in the env).</p>"},{"location":"coding/#aws-amplify","title":"AWS Amplify","text":"<p>AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS.</p> <ul> <li>installation and configuration</li> </ul> <pre><code>npm i -g @aws-amplify/cli\n# do a configure like aws configure\namplify configure\n</code></pre> <p>See also git doc.</p>"},{"location":"coding/#amazon-codewhisperer","title":"Amazon CodeWhisperer","text":"<p>Amazon CodeWhisperer offers developers a new and seamless way to build applications responsibly on AWS, with strong focus on automated enterprise security, through vulnerability scans and remediation recommendations and code compliance, to built-in code reference tracker and license attribution recommendations.</p> <ul> <li>Part of the Code AI tool like CodeGuru and DevOps Guru.</li> <li>Personas: data analyst (SQL, ETL), software engineer, cloud engineer (IaC, CDK), data scientist (inside SageMaker and Jupyter notebook, get rid of finding plumbing code), SecOps</li> <li>Trained on billions of open source projects and Amazon app lines of code, samples. </li> <li>Reduce rewriting same type of code. Stop the infernal loop, search google or stack overflow to get the relevant answer to our question.</li> <li>It can flag or filter code suggestions that resemble open-source training data.</li> <li>Support AWS services APIs.</li> <li>Run in Lambda editor, and Cloud9</li> <li>IDE Context automatically sent to CodeWhisperer service to get code suggestions in real-time. </li> <li>It is doing reference tracking.</li> <li>57% productivity improvement. </li> <li>Improve application security with scanning capability, detecting and fixing hard to find code vulnerabilities.</li> <li>Enterprise or free packaging: integrated with IAM Identity Center.</li> <li>Support cross-file context for Java. </li> </ul>"},{"location":"coding/#demo-scripts","title":"Demo scripts","text":"Installation <p>In the IDE like VScode, install the AWS Toolkit. Then choose the authentication method, and set up our Builder ID, IAM Identity Center, or IAM credential.</p> Python app with CDK and lambda demonstration. <p>The code is in labs/code-whisperer-demo with a readme to demonstrate the code generation capability.</p>"},{"location":"coding/#codewhisperer-resources","title":"CodeWhisperer resources:","text":"<ul> <li>Product documentation.</li> <li>Amazon CodeWhisperer Overview - Video.</li> <li>Build a Python Event-driven Serverless App with Amazon CodeWhisperer - Video. the labs/code-whisperer-demo includes the code for this demo.</li> <li>Optimize software development with Amazon CodeWhisperer.</li> <li>Short cuts for use actions</li> <li>FAQs.</li> <li>10 ways to build applications faster with Amazon CodeWhisperer.</li> </ul>"},{"location":"coding/#other-resources","title":"Other resources","text":"<ul> <li>Open Web App security project, secure coding practices</li> </ul>"},{"location":"coding/cdk/","title":"Cloud Development Kit - CDK","text":"<p>The AWS CDK lets us build reliable, scalable, cost-effective applications in AWS cloud with the different programming languages such as Go, Java, Python, C#, JavaScript, Typescript. </p> <p>Build with high-level constructs to define any AWS resources with less code. The output of an AWS CDK program is an AWS CloudFormation template which can be deployed with the cdk CLI.</p>"},{"location":"coding/cdk/#advantages","title":"Advantages","text":"<ul> <li>Developers write IaC code in their programming language, with the power of the language to do more complex things than in templates. </li> <li>More flexible than SAM and less complex than CloudFormation templates.</li> <li>Custom CDK constructs let developers encode and share security best practices and compliance measures across the enterprise.</li> <li>Easier way to create lots of similar resources.</li> <li>Ability to abstract truly boilerplate stuff like a VPC.</li> </ul>"},{"location":"coding/cdk/#some-caveats","title":"Some caveats","text":"<ul> <li>Multiple programming language means different developer groups will use different CDK implementation, leading to governance issues. In most enterprise it is not that relevant, and teams are still quite isolated and applying agile manifesto, use the tools they are more confortable to be efficient.</li> <li>CDK abstraction level is may be too high and we have to learn by increment to clearly understand what each construct is really building as AWS resources. </li> <li>It's easy to create a mess of resources without understanding the underlying purpose or intent.</li> <li>With groups with clear separation between Ops and Dev teams, then SREs may not be able to support CDK apps.</li> <li>A framework like SAM dictates how to do configuration, CDK libraries gives us contructs to work with.</li> <li>Code and infrastructure code orgranization is a challenge and need strong practices to avoid not understanding what is what and what the deployment does. Often CloudFormation or SAM are better to understand.</li> <li>At some point, we need to learn CloudFormation fundamentals, as when the If deployment fails, we have to go at the CloudFormation level.</li> </ul>"},{"location":"coding/cdk/#install","title":"Install","text":"<p>The CDK cli is packaged as nodejs app. To install it:</p> <pre><code>npm install -g aws-cdk \n</code></pre> <p>A docker file exists under the <code>labs</code> folder to get an environment with nodes, python 3.9, AWS CLI, CDK CLI...</p> <pre><code># build the docker\ndocker build -t jbcodeforce/aws-python .\n# Start the env\n./startPythonDocker.sh\n# verify installation\naws s3 ls\ncdk --version\n# In app folder and where the cdk.json file is, do\ncdk synth\n</code></pre>"},{"location":"coding/cdk/#concepts","title":"Concepts","text":"<p>A CDK app defines one or more Stacks (= CloudFormation stack). A Stack includes a set of Constructs. Each Construct defines one or more concrete AWS resources.</p> <p>Constructs, Stacks and apps, are represented as classes (types) in the selected programming language. We instantiate constructs within a Stack to declare them to AWS, and connect them to each other using well-defined interfaces. CDK includes a collection of constructs called the AWS Construct Python Library - V2 and a Construct hub to share them.</p> <p>Here is an example of constructs defined in a Python class constructor for a lambda function and an API Gateway:</p> <pre><code>from aws_cdk import (Stack,aws_lambda,aws_apigw)\n\nclass MyLambdaStack(Stack):\n    def __init__(self, scope: Construct, construct_id: str, **kwargs) -&gt; None:\n        super().__init__(scope, construct_id, **kwargs)\n        my_lambda = aws_lambda.Function(self, 'HelloHandler',\n            runtime=aws_lambda.Runtime.PYTHON_3_9,\n            code=aws_lambda.Code.from_asset('lambda'),\n            handler='hello.handler',\n        )\n\n        aws_apigw.LambdaRestApi(\n            self, 'Endpoint',\n            handler=my_lambda,\n        )\n</code></pre> <p>There are 3 level of Constructs (L1 matches to cloud formation resources, L2 is higher level abstration of AWS resources, while L3 are patterns like APIGateway, or ALB for Fargate). </p> <p>The AWS CDK Toolkit is a command line tool for interacting with CDK apps. Need to be in the folder of the <code>cdk.json</code> file to run it.</p> <pre><code>cdk --version\n# Get the CloudFormation template\ncdk ls\n</code></pre> <p>The first time we deploy an AWS CDK app into an environment (account/region), we\u2019ll need to install a \u201cbootstrap stack\u201d. </p> <pre><code>cdk bootstrap aws://$ACCOUNT/$REGION -c account=$ACCOUNT -c environmentType=qa --profile $PROFILE\n</code></pre> <p>This stack includes resources that are needed for the toolkit\u2019s operation. It requires dedicated Amazon S3 buckets to store template and assets.</p> <p></p> <p>CDK applications should be organized into logical units, such as API, database, and monitoring resources, and optionally have a pipeline for automated deployments.</p> <p>For an application with multiple components/microservices, it is possible to organize different stacks, one per component for example, and then link them with a single app. See example in CDK project template or Autonomous Car Ride Solution</p> <p>When packages begin to be used in multiple applications, move them to their own repository. This way, the packages can be referenced by application build systems that use them, and they can also be updated on cadences independent of the application life cycles.</p> <ul> <li>See CDK workshops.</li> <li>The CDK for Python API.</li> </ul>"},{"location":"coding/cdk/#useful-cdk-commands","title":"Useful CDK commands","text":"<ul> <li><code>cdk ls</code>          list all stacks in the app</li> <li><code>cdk synth</code>       emits the synthesized CloudFormation template</li> <li><code>cdk deploy</code>      deploy this stack to the default AWS account/region</li> <li><code>cdk diff</code>        compare deployed stack with current state</li> <li><code>cdk docs</code>        open CDK documentation</li> <li><code>cdk watch</code>       monitors the code and assets for changes and attempts to perform a deployment automatically when a change is detected</li> <li><code>cdk destroy</code>    remove all the resources/stacks. Most resources will get deleted upon stack deletion. CloudWatch logs that are permanently retained</li> </ul>"},{"location":"coding/cdk/#samples","title":"Samples","text":""},{"location":"coding/cdk/#cdk-python-for-an-ec2","title":"CDK Python for an EC2","text":"<ul> <li>Summary of the actions to jumpstart a CDK sample app in python</li> </ul> <pre><code># Create a python CDK project under a new created folder. The name of the folder defines the name of the app.\n# It uses the sample-app template. If template is not specified the stack will have only constructor\ncdk init sample-app --language python\n# cdk init --language python \n# create virtual env\npython3 -m venv .venv\n# Active the virtual env\nsource .venv/bin/activate\n# Install dependencies\npip install -r requirements.txt\n</code></pre> <ul> <li>Then develop the CDK class to define the configuration needed, like in the following code: a simple EC2 instance with code define in user data script to prepare a web server based on nginx:</li> </ul> <pre><code>from aws_cdk import (\n    Stack,\n    aws_ec2 as ec2)\n\namzn_linux = ec2.MachineImage.latest_amazon_linux(\n    generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2,\n    edition=ec2.AmazonLinuxEdition.STANDARD,\n    virtualization=ec2.AmazonLinuxVirt.HVM,\n    storage=ec2.AmazonLinuxStorage.GENERAL_PURPOSE\n)\nwith open(\"./user_data/user_data.sh\") as f:\n    user_data = f.read()\n\nclass Ec2Stack(Stack):\n    self.instance = ec2.Instance(self, \"myHttpdEC2\",\n                                instance_type=ec2.InstanceType(\"t2.micro\"),\n                                instance_name=\"mySimpleHTTPserver\",\n                                machine_image=amzn_linux,\n                                user_data=ec2.UserData.custom(user_data),\n                                )\n</code></pre> <p>See more information on the EC2 Instance API.</p> <ul> <li>Run the cloud formation template generation and then deploy it to the account / region</li> </ul> <pre><code># Synthesize the Cloud Formation template \ncdk synth\n# The first time bootstrap the stack - which will create a CF CDKToolkit\ncdk bootstrap\n# or using account and region\ncdk bootstrap aws://4....../us-west-2\n# Deploy the stack\ncdk deploy\n# Update the code and do a partial (hotswappable) deployment\ncdk deploy --hotswap\n# Set name for the context\ndk deploy  --context s3bucketname=&lt;the name of the s3 bucket&gt; --all\n</code></pre> <ul> <li>Then go to the CloudFormation console and look at the deployed stack, and resources.</li> </ul>"},{"location":"coding/cdk/#some-personal-examples","title":"Some personal examples","text":"Folder Description labs/cdk/ec2-basic EC2 with http server, and security group for inbound traffic deployed on default VPC labs/cdk/ec2-vpc EC2 with VPC and public &amp; private subnets, NAT, IGW, Bastion Host labs/cloud9 for a cloud9 environment for a specific VPC labs/cdk/cdk_workhop Lambda functions in python with an API gateway and TableViewer. lab ECS fargate Flask App VPC with ECS fargate for a Flask APP where container is created during deployment lab EKS VPC with EKS cluster deployment cdk with lambda and API gateway Python lambda + API gateway cdk python for lambda, dynamo, api gtw, powertools (autonomous car manager) Python lambda + API gateway, dynamoDB, iam policy, Alias and version Lambda java cdk AWS CDK my project template A template project to organize java deployment on ECR with CDK infrastructure. AWS resiliency studies With VPC, ELB, ASG, EC2 ec2-vpce-s3 to map the connection debug use case EC2 to VPC gateway endpoint to S3 bucket"},{"location":"coding/cdk/#cdk-blueprint-for-eks","title":"CDK Blueprint for EKS","text":"<ul> <li>Blog introduction EKS blueprint</li> <li>AWS CDK EKS blueprint git repo</li> </ul>"},{"location":"coding/cdk/#other-tools-samples","title":"Other tools - samples","text":"<ul> <li>CDK API v2 for Python</li> <li>Pypi.org search needed when using alpha or beta libraries.</li> <li>CDK Patterns</li> <li>cdk-dynamo-table-viewer An AWS CDK construct which exposes a public HTTP endpoint which displays an HTML page with the contents of a DynamoDB table defined in the stack.</li> <li>AWS CDK samples in Python</li> <li>Constructs HUB</li> <li>A Flask app for orders management with DynamoDB as persistence - ECR - CDK</li> <li>Big Data SaaS demo</li> <li>AWS CDK labs</li> </ul>"},{"location":"coding/cdk/#some-how-tos","title":"Some how-tos","text":"Create a role for an EC2 instance to access S3 <pre><code>role = iam.Role(self, \"Role\",\n            assumed_by=iam.ServicePrincipal(\"ec2.amazonaws.com\"))\n    role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name(\"AmazonS3FullAccess\"))\n</code></pre> Create EC2 in public subnet <p>With an instance profile role, defined from a previous declaration: <pre><code>self.instance = ec2.Instance(self, \"myHttpdEC2\",\n                            instance_type= ec2.InstanceType(\"t2.micro\"),\n                            instance_name=\"mySimpleHTTPserver\",\n                            machine_image=amzn_linux,\n                            vpc=self.vpc,\n                            role=role,\n                            key_name=key_name,\n                            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),\n                            security_group=self.ec2_security_group,\n                            user_data=ec2.UserData.custom(user_data),\n                            )\n</code></pre></p> Declare RDS postgresql with secrets <p>The following python CDK code declares a Postgresql DB in a private subnet within a VPC created before. The access is public. CDK will create a secret in AWS Secret manager.</p> <pre><code> self.postgres = aws_rds.DatabaseInstance(self, \"PostgresqlInstance\",\n                            database_name=\"tenantdb\",\n                            engine=aws_rds.DatabaseInstanceEngine.postgres(version=aws_rds.PostgresEngineVersion.VER_14_5),\n                            vpc_subnets=aws_ec2.SubnetSelection(subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS),\n                            vpc=self.vpc,\n                            port=5432,\n                            removal_policy=RemovalPolicy.DESTROY,\n                            deletion_protection=False,\n                            max_allocated_storage=200,\n                            publicly_accessible=True\n                    )\n</code></pre> Create a user with a policy <p>Python user class. Use password so user can access console and use API. </p> <pre><code>    user = iam.User(self,\n                    \"ecruser\",\n                    user_name=\"ecruser\",\n                    password=SecretValue.unsafe_plain_text(\"T0ch@ngefordemo\")\n                    )\n    user.attach_inline_policy(policy)\n    # or \n    user.add_managed_policy(policy)\n</code></pre> <p>See this cdk for the abac tutorials</p> Create IAM policy <p>See Policy class. Force to true to get the policy being created as managed policy.</p> <pre><code>policy = iam.Policy(self,\"ecr-auth-token-policy\",\n            statements= [\n                iam.PolicyStatement(\n                    actions= [\"ecr:GetAuthorizationToken\"],\n                    resources=[\"*\"],\n                    conditions=[{}],\n                )],\n            policy_name=\"ecr-auth-token-policy\",\n            force= True   \n    )\n</code></pre> Get existing VPC reference <pre><code>    # Retrieve default VPC information\n    self.vpc = aws_ec2.Vpc.from_lookup(self, \"VPC\",\n        is_default=True\n    )\n</code></pre> <p>For other VPC change is_default to False and provide the vpcName. <pre><code># get the vpc-id from parameter store\nvpc_id = ssm.StringParameter.value_from_lookup(self, \"/vpc-id\")\n\n# get the Vpc from the id\nvpc = ec2.Vpc.from_lookup(self, \"vpc\", vpc_id=vpc_id)\n\n# get the subnets in AZ a from the vpc\nsubnets_in_az_a = vpc.select_subnets(availability_zones=[\"ap-southeast-2a\"])\n\n# create AZ lookup\naz_lookup = {}\n\n# iterate over the AZ a subnets \nfor subnet in subnets_in_az_a.subnets:\n    az_lookup[subnet.subnet_id] = subnet.availability_zone\n\n# iterate over the public subnets \nfor subnet in vpc.public_subnets: \n    az_lookup[subnet.subnet_id] = subnet.availability_zone\n\n# fetch the isolated subnets list from parameter store\niso_subnet_ids = ssm.StringParameter.value_from_lookup(self, \"/iso-subnets\").split(\",\")\n\n#create an list to store the subnets in\niso_subnets = []\n\n# iterate over the subnet ids and create the full Subnet object includeing AZ\niso_count = 1\nfor subnet_id in iso_subnet_ids:\n    iso_subnets.append(\n        ec2.Subnet.from_subnet_attributes(\n            self,\n            \"IsoSub\" + str(iso_count),\n            subnet_id=subnet_id,\n            availability_zone=az_lookup[subnet_id],\n        )\n    )\n    iso_count += 1\n</code></pre></p> <p>We can also use boto3 and then get VPC by name: <pre><code>client = boto3.client('ec2',region_name='us-east-1')\n    response = client.describe_vpcs(\n                    Filters=[{\n                        'Name': 'tag:Name',\n                        'Values': [\n                            vpc_name\n                        ]\n                    }]\n                )\n    if len(response['Vpcs']) &gt; 0:\n        vpc=response['Vpcs'][0]\n    else:\n        vpc= None\n    return vpc\n</code></pre></p> Inject Database URL and admin password as env variable <p>First there is those excellent PyPI examples for RDS instance creation. Then when using TaskImageOptions from the ecs_patterns library there is an <code>environment</code> element. <pre><code>task_image_options=aws_ecs_patterns.ApplicationLoadBalancedTaskImageOptions(\n            image=ecs.ContainerImage.from_registry(str(self.account) \n                                                + \".dkr.ecr.\"\n                                                + str(self.region)\n                                                + \".amazonaws.com/acr-car-ride-mgr\"),\n            environment = {\n                \"QUARKUS_DATASOURCES_JDBC_URL\" : \"jdbc:postgresql://\" + str(dbCarRides.db_instance_endpoint_address),\n\n            },\n            task_role = taskRole\n        )\n</code></pre> The database <code>Credentials.from_generated_secret</code> function creates a <code>Secret</code> in <code>SecretManager</code>, with the name <code>rds-secret</code>: <pre><code>     credentials=rds.Credentials.from_generated_secret(\"postgres\",secret_name=\"rds-secret\"),                                     \n</code></pre></p> Unit testing a stack <p>Uses the AWS CDK's assertions module, to validate \"this resource has this property with this value.\" <pre><code>python -m pip install -r requirements-dev.txt\npython -m pytest\n</code></pre></p> Another example of passing a shell script as EC2 user data <pre><code>data = open(\"./VPCstack/user_data.sh\", \"rb\").read()\nuserData=ec2.UserData.for_linux()\nuserData.add_commands(str(data,'utf-8'))\n</code></pre> <p>See VPC stack and bastion host.</p> Create a lambda (env variable) with an API gateway <p>Create a folder, <code>api/runtime</code>, to include the python code for the lambda: <code>get_image.py</code>, function <code>lambda_handler</code>:  <pre><code># import\n    aws_lambda as lambda_,\n    aws_apigateway as apigw,\n\nmyLambda = lambda_.Function(self,\n                \"GetImageFunction\",\n                runtime= lambda_.Runtime.PYTHON_3_10,\n                code=lambda_.Code.from_asset('api/runtime'),\n                handler='get_image.lambda_handler',\n                environment={\n                    'BUCKET_NAME': s3Bucket.bucket_name,\n                })\n</code></pre></p> <p>Add API Gateway with a POST:</p> <p><pre><code>apiGtw = apigw.RestApi(self, \"CWdemoApi\",\n                        rest_api_name=\"CWDemoApi\",\n                       )\nimages = apiGtw.root.add_resource('images')\nimages.add_method('POST',  apigw.LambdaIntegration(getImageFct))\n</code></pre> See code whisperer demo</p> Create a bastion from a custom AMI (Java 17, maven, docker) <p>See the VPCstack definition in aws-cdk-project template. Attention bastion does not have role</p> Access env account from stack <pre><code>self.account\nself.region\n</code></pre> Add VPCFlow to a VPC <pre><code>from aws_cdk import aws_logs as logs\n\ncwLogs = logs.LogGroup(self, '/aws/vpc/flowlogs')\nself.vpc.add_flow_log(\"flowlogs\",destination=ec2.FlowLogDestination.to_cloud_watch_logs(cwLogs),\n                          traftraffic_type=ec2.FlowLogTrafficType.ALL)\n</code></pre> Passing context variables <p>The call to deploy pass the information to the context:</p> <pre><code>dk deploy  --context s3bucketname=&lt;the name of the s3 bucket&gt; --all\n</code></pre> <p>And in the stack definition:</p> <pre><code>s3_bucket_name=self.node.try_get_context(\"s3bucketname\")\n</code></pre> Install python dependencies while deploying Lambda <p>To install Python packages when deploying a Lambda function using the CDK, you can use the Code.fromAsset() option and specify a bundling command that runs pip install.</p> <pre><code>acm_lambda = aws_lambda.Function(self, 'CarMgrService',\n        runtime=aws_lambda.Runtime.PYTHON_3_11,\n        code= aws_lambda.Code.from_asset(path=\"../src/carmgr\",\n                    bundling=BundlingOptions(\n                        image= aws_lambda.Runtime.PYTHON_3_11.bundling_image,\n                        command= [\n                            'bash','-c','pip install -r requirements.txt -t /asset-output &amp;&amp; cp -au . /asset-output'\n                        ],\n                )),\n</code></pre>"},{"location":"coding/cloudFormation/","title":"AWS CloudFormation","text":"<p>AWS CloudFormation helps developers to create and manage a collection of related AWS resources as code. The Yaml or JSON template, called a stack, defines AWS resources. Template may be uploaded from a S3 bucket or from our local computer. </p> <p>The goal is to repeat infrastructure setup between regions or accounts. Template defines a set of resources that work together to create an application or solution.</p> <p>Stacks are defined in region, but StackSets help to share stacks between accounts and regions.</p> <p>Stack can be created with other stacks (nested) or common resources can be managed using a separate stack. Nested stacks are not recommended as best practice, as they could be a large area of impact if something goes wrong (all templates will rollback). </p> <p>Other stacks can simply refer to the existing resources using cross-stack references. This allows independent teams to be responsible for their resources. When creating a template, developer may indicate what resources are available for cross stack references by exporting those values (Export output field). Other stacks can use <code>Fn::ImportValue</code> function to import the value.</p> <p>To create a stack from AWS templates we can use CLI, API, the Console or start from one of the samples.</p> <p>The classical steps are:</p> <ol> <li>Select a template</li> <li>Prepare any required items for the stack, considering input parameters for example.</li> <li> <p>Create the stack, using CloudFormation console, or AWS CloudFormation CLI like</p> <pre><code>aws cloudformation create-stack --stack-name myteststack --template-body file://sampletemplate.yaml --parameters ParameterKey=KeyPairName,ParameterValue=TestKey\n</code></pre> </li> <li> <p>Monitor the stack creation progress</p> </li> <li>Use the stack resources</li> <li>Clean up.</li> </ol> <p>Once stack is created, <code>Change Sets</code> may be applied to update the running resources. It is like a summary of the proposed changes. There is also the <code>Drift</code> detection feature to identify configuration changes between live resources and template. It is possible to use a CloudFormation public registry, with 3nd party resources published in APN.</p> <p>Pay for what the resources use.</p>"},{"location":"coding/cloudFormation/#get-started","title":"Get started","text":"<p>The infrastructure is defined in Stack. The below example is for an EC2 instance with a user data declaration to install and start Apache webserver, by referencing an existing SSH key pair and security group. See in the folder labs/CF.</p> <pre><code>Resources:\n  WebServer:\n    Type: AWS::EC2::Instance\n    Properties:\n      ImageId: !FindInMap [AWSRegionArch2AMI, !Ref 'AWS::Region', !FindInMap [AWSInstanceType2Arch, !Ref InstanceType, Arch]]      \n      InstanceType:\n        Ref: t2-micro\n      KeyName:\n        Ref: my-key-pair\n      SecurityGroups:\n      - Ref: WebServerSecurityGroup\n      UserData:\n        Fn::Base64: !Sub |\n           #!/bin/bash -xe\n           yum update -y\n           yum install -y httpd\n           systemctl start httpd\n           systemctl enable httpd\n           EC2-AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\n           echo \"&lt;h3&gt;Hello World from $(hostname -f) in AZ= $EC2_AZ &lt;/h3&gt;\" &gt; /var/www/html/index.html\n</code></pre> <p>The KeyName property is a literal for an existing key name in the region where the stack is being created.</p> <p>Use the <code>Parameters</code> section to declare values that can be passed to the template when we create the stack.</p> <pre><code>Parameters:      \n  KeyName:\n    ConstraintDescription: must be the name of an existing EC2 KeyPair.\n    Description: Name of an existing EC2 KeyPair to enable SSH access to the instances\n    Type: AWS::EC2::KeyPair::KeyName\n</code></pre> <p>See the getting started guide.</p> <ul> <li>The <code>Ref</code> function returns the value of the object it refers to.</li> <li>Use <code>Mappings</code> to declare conditional values that are evaluated in a similar manner as a look up table statement</li> <li>The Fn::GetAtt function helps to get attribute of a resource.</li> <li> <p>Mappings enable us to use an input value as a condition that determines another value. Similar to a switch statement, a mapping associates one set of values with another. Below the ImageId property of the resource Ec2Instance uses the <code>Fn::FindInMap</code> function to determine its value by specifying <code>RegionMap</code> as the map to use, <code>AWS::Region</code> as the input value to map from, and AMI as the label to identify the value to map to.</p> <pre><code>Mappings:\n    RegionMap:\n        us-east-1:\n        AMI: ami-76f0061f\n        us-west-1:\n        AMI: ami-655a0a20\nResources:\n    Ec2Instance:\n        Type: 'AWS::EC2::Instance'\n        Properties:\n            ImageId: !FindInMap \n                - RegionMap\n                - !Ref 'AWS::Region'\n                - AMI\n</code></pre> </li> <li> <p>See template details.</p> </li> <li>We can associate the <code>CreationPolicy</code> attribute with a resource to prevent its status from reaching create complete until AWS CloudFormation receives a specified number of success signals or the timeout period is exceeded.</li> </ul> <p>Example for S3 bucket and website:</p> <pre><code>Resources:\n  HelloBucket:\n    Type: 'AWS::S3::Bucket'\n    Properties:\n      AccessControl: PublicRead\n      WebsiteConfiguration:\n        IndexDocument: index.html\n        ErrorDocument: error.html\n</code></pre>"},{"location":"coding/cloudFormation/#stacksets","title":"StackSets","text":"<p>It is used to create, update, delete stacks across multiple accounts and region in a single operation. An administrator creates the StackSets.</p> <p>Trusted accounts create, update, delete stack instances from the StackSets. An update to the StackSets, makes all associated stack instances updated.</p>"},{"location":"coding/cloudFormation/#drift","title":"Drift","text":"<p>Evaluate all resources that may have changed by admin console, and that will be reversed back to cloud formation template settings.</p>"},{"location":"coding/cloudFormation/#quotas-and-limits","title":"Quotas and Limits","text":"<p>There are quotas to consider per account, when authoring templates and creating stacks.  The relevant product documentation are AWS CloudFormation endpoints and quotas and .</p> <ul> <li>The number of stack per account, per region is 2000 as a soft limit.</li> <li>StackSet is limited to 1000 per account.</li> </ul>"},{"location":"coding/cloudFormation/#more-advanced-topics","title":"More advanced topics","text":""},{"location":"coding/cloudFormation/#export-resource-from-one-template-to-the-other","title":"Export resource from one template to the other","text":"<p>Use cross-stack reference, for that define the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.</p> <p>Export in Outputs:</p> <pre><code>  \"Outputs\" : {\n    \"PublicSubnet\" : {\n      \"Description\" : \"The subnet ID to use for public web servers\",\n      \"Value\" :  { \"Ref\" : \"PublicSubnet\" },\n      \"Export\" : { \"Name\" : {\"Fn::Sub\": \"${AWS::StackName}-SubnetID\" }}\n    },\n</code></pre> <p>Reference in another CF template</p> <pre><code>  \"Parameters\": {\n    \"NetworkStackName\": {\n      \"Type\": \"String\",\n      \"Default\" : \"SampleNetworkCrossStack\"\n    }\n  }\n.... \n\"NetworkInterfaces\" : [{\n          \"GroupSet\"                 : [{ \"Fn::ImportValue\" :  {\"Fn::Sub\": \"${NetworkStackName}-SecurityGroupID\" } }],\n          \"DeleteOnTermination\"      : \"true\",\n          \"SubnetId\"                 : { \"Fn::ImportValue\" : {\"Fn::Sub\": \"${NetworkStackName}-SubnetID\" } }\n</code></pre>"},{"location":"coding/cloudFormation/#custom-resources","title":"Custom resources","text":"<p>Custom resources enable to write custom logic executed on CF creating, updating or deleting a stack. It can run a lambda function to do a lot of thing on an environment. See the lap-template where a lambda function is called to update a EC2 role or the lambda to update the EC2 for Cloud9.</p> <pre><code>Cloud9SSMRole:\n    Type: Custom::Cloud9SSMRole\n    Properties:\n      ServiceToken: !GetAtt Cloud9SSMRoleFunction.Arn\n\nCloud9SSMRoleFunction:\n    Type: AWS::Lambda::Function\n    ...\n</code></pre>"},{"location":"coding/cloudFormation/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Introduction from Tutorial Dojo</li> <li>AWS CloudFormation Workshop with Git repo aws-samples/cfn101-workshop cloned in Code/Studies folder.</li> <li>Best practices</li> <li>Sample templates for some AWS services</li> </ul>"},{"location":"coding/cloudFormation/#some-personal-examples","title":"Some personal examples","text":"<ul> <li>EDA workshop cloud formation stack refactorized - lab/eda</li> </ul>"},{"location":"coding/cloudFormation/#tools","title":"Tools","text":"<ul> <li>Use CloudFormation linter to validate the yaml declaration</li> <li>Json to Yaml online tool</li> <li>Consider CDK as a higher abstraction layer to generate Cloud Formation stacks.</li> </ul>"},{"location":"coding/cloudFormation/#service-catalog","title":"Service Catalog","text":"<p>Service Catalog is part of a governance practices, within company to define authorized products CF templates. It also support open source Terraform.</p> <p>It includes a Getting Started Library of well-architected product templates.</p> <p>Catalog administrator creates a porfolio and then products as CF templates, the user can instantiate. Each portfolio has a IAM permision to access the portfolio for the given users.</p>"},{"location":"coding/sam/","title":"AWS SAM - Serverless Application Model","text":"<p>AWS SAM is an extension of AWS CloudFormation with a simpler syntax for configuring common serverless application resources such as functions, triggers, databases and APIs.</p> <p>SAM CLI is an open-source. Better to use within Cloud9 environment and install sam CLI.</p> <p>SAM includes two parts:</p> <ol> <li>SAM template specification: It is an extension on top of AWS CloudFormation.</li> <li>A CLI to create new project, build and deploy, perform local debugging and testing, configure pipeline.</li> </ol> <p>Here is an example of definition of a serverless app including: AWS Lambda, API gateway with HTTP api, database in DynamoDB and then IAM permissions for those services to work together.</p> <pre><code>AWSTemplateFormatVersion: 2010-09-09\nTransform: AWS::Serverless-2016-10-31\nResources:\n  getAllItemsFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n        Handler: src/get-all-items.getAllItemsHandler\n        Runtime: nodejs12.x\n        Events:\n            Api:\n            Type: HttpApi\n            Properties:\n                Path: /\n                Method: GET\n        Connectors:\n        MyConn:\n            Properties:\n            Destination:\n            Id: SampleTable\n            Permissions:\n                - Read\n    SampleTable:\n        Type: AWS::Serverless::SimpleTable\n</code></pre> <p>During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax.</p> <p>See SAM templates here</p>"},{"location":"coding/sam/#value-propositions","title":"Value propositions","text":"<ul> <li>Define the application infrastructure code as quickly, using less code.</li> <li>Manage the serverless applications through their entire development lifecycle.</li> <li>Quickly provision permissions between resources with AWS SAM connectors.</li> <li>Continuously sync local changes to the cloud as we develop.</li> <li>On top of CloudFormation or Terraform.</li> </ul>"},{"location":"coding/sam/#getting-started","title":"Getting started","text":"<p>SAM CLI provides a Lambda-like execution environment that lets you locally build, test, and debug applications defined by SAM templates or through the AWS Cloud Development Kit (CDK).</p> <ul> <li>Install, which can be summarized as:</li> </ul> <pre><code>brew install aws-sam-cli\n# or upgrade\nbrew upgrade aws-sam-cli\nsam --version\n</code></pre> <ul> <li>Serverlessland pattern collection.</li> <li>A Complete SAM Workshop.</li> <li>Java samples with SAM CLI.</li> </ul>"},{"location":"coding/sam/#summary-of-application-development-with-sam","title":"Summary of application development with SAM","text":"<p>The diagram below presents the classical app components that can be created with SAM:</p> <p></p> <p>The following steps presents how to get a Lambda in Python exposed with API in API Gateway.</p> <ul> <li> <p>Create a SAM project</p> <pre><code># Create a new function using interactive mode:\nsam init\n# or using parameters: Xray tracing, CloudWatch insights, python\nsam init --name sam-app --runtime python3.9 --architecture arm64 --app-template hello-world --tracing --application-insights \n# or using powertool so we have metrics, logging, tracing\nsam init --app-template hello-world-powertools-python --name carridegenerator --package-type Zip --runtime python3.9 --no-tracing\n</code></pre> <p>See the code templates in the git repo.</p> </li> <li> <p>When using container Image as the packaging mechanism, we need to login to ECR repo via:</p> <pre><code>aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n</code></pre> </li> <li> <p>Run locally - Any code update will be refreshed. First call may take sometime to download docker image. It needs docker engine to run locally. (finch is not supported yet)</p> <pre><code># a python app:\ncd sam-app/hello_world\npip3 install -r requirements.txt\n\ncd ..\nsam local invoke\n# test with input event\nsam local invoke HelloWorldFunction --event events/event.json\n# Emulate API on port 8080\nsam local start-api --port 8080\ncurl http://localhost:8080/hello\n</code></pre> </li> <li> <p>Run unit tests</p> <pre><code># under sam-app\npip3 install pytest pytest-mock\npython3 -m pytest tests/unit\n</code></pre> </li> <li> <p>Build the packaging for AWS</p> <pre><code># under sam-app\nsam build\n# see .aws-sam folder\n# Build with Image on Mac\nDOCKER_HOST=unix://$HOME/.docker/run/docker.sock sam build --use-container\n# Build on linux\nsam build --use-container\n</code></pre> </li> <li> <p>Deploy (be sure to get aws cli 'configured' with access key and secret). It is creating the resources (Lambda function, API Gateway and IAM roles)</p> <pre><code># under sam-app\nsam deploy --guided\n</code></pre> <p>The code is zipped and uploaded to a s3 bucket: <code>aws-sam-cli-managed-default-samclisourcebucket-...</code></p> <pre><code># deploy with parameters\nexport AWS_REGION=$(aws --profile default configure get region)\nsam deploy \\\n    --stack-name wild-rydes-async-msg-1 \\\n    --capabilities CAPABILITY_IAM \\\n    --region $AWS_REGION \\\n    --guided\n</code></pre> Deployment Output <p>A cloud formation template is created which includes Role, and API Gateway so the Lambda is accessible via REST API. <pre><code>CloudFormation stack changeset\n----------------------------------------------------------------------------------------------------------------------------\nOperation                                    LogicalResourceId                            ResourceType                                                                 \n----------------------------------------------------------------------------------------------------------------------------\n+ Add                                        ApplicationInsightsMonitoring                AWS::ApplicationInsights::Application                                               \n+ Add                                        ApplicationResourceGroup                     AWS::ResourceGroups::Group                   \n+ Add                                        HelloWorldFunctionHelloWorldPermissionProd   AWS::Lambda::Permission                                                           \n+ Add                                        HelloWorldFunctionRole                       AWS::IAM::Role                                                                     \n+ Add                                        HelloWorldFunction                           AWS::Lambda::Function                                                               \n+ Add                                        ServerlessRestApiDeployment47fc2d5f9d        AWS::ApiGateway::Deployment                                                        \n+ Add                                        ServerlessRestApiProdStage                   AWS::ApiGateway::Stage                                                            \n+ Add                                        ServerlessRestApi                            AWS::ApiGateway::RestApi                                                           \n-------------------------------------------------------------------------------------------------------------------------\n</code></pre></p> </li> <li> <p>[Optional] We can continuously synchronize local change to the cloud:</p> <pre><code># look at all changes\nsam sync --watch\n# look to code only updates\nsam sync --watch --code\n</code></pre> <p>The <code>sam sync</code> command can also synchronize code changes to the cloud without updating the infrastructure. This code synchronization uses the service APIs and bypasses <code>CloudFormation</code>, allowing AWS SAM to update the code in seconds instead of minutes.</p> </li> <li> <p>To clean up, we have two choices:</p> <pre><code>aws cloudformation delete-stack --stack-name sam-app --region us-west-2\n# with sam\nsam delete\n</code></pre> </li> <li> <p>For building a CodePipeline using <code>sam cli</code> see next section.</p> </li> </ul>"},{"location":"coding/sam/#the-sam-template","title":"The sam template","text":"<p>For the function section of the template, see the lambda reference.</p>"},{"location":"coding/sam/#sam-pipelines","title":"SAM Pipelines","text":"<p>AWS SAM Pipelines makes it easier to create CI/CD pipelines.</p> <ul> <li>Create a CodeCommit repository with something like (if needed see codecommit authentication):</li> </ul> <pre><code>aws codecommit create-reposiory --repository-name sam-app\n# if needed install python library to access code commit\npip3 install git-remote-codecommit\n</code></pre> <ul> <li>Be sure to add .aws-sam and samconfig.yaml in .gitignore</li> </ul> <pre><code>echo -e \"\\n\\n.aws-sam\" &gt;&gt; .gitignore\necho -e \"samconfig.toml\" &gt;&gt; .gitignore\n</code></pre> <ul> <li>To commit change use remote URL like:</li> </ul> <pre><code>git remote add origin codecommit://sam-app\n</code></pre> <ul> <li>Create pipeline resources: </li> </ul> <p><code>sam pipeline init --bootstrap</code> command creates the AWS resources and permissions required to deploy application artifacts from our code repository into our AWS environments.</p> <p>An Amazon S3 bucket is required to store application build artifacts during the deployment process. </p> <p>Pipelines defines two stages, dev and production.</p> <p>A <code>.gitlab-ci.yml</code> pipeline file is generated. The file contains a number of environment variables, which reference the details from AWS SAM pipeline bootstrap command.</p> <p>The Cloud Formation template deploy two stacks and all the needed elements for the CI/CD in dev and prod envs. Going to Code Pipeline console to see the create pipeline.</p> <p>To create a github and git action pipeline see this doc..</p>"},{"location":"coding/sam/#canary-deployments","title":"Canary deployments","text":"<p>Technique to slowly rolling out the changes to a small subset of users before rolling it out to the entire customer base.</p> <p>AWS Lambda allows us to publish multiple versions of the same function. Each version has its own code and associated dependencies, as well as its own function settings.</p>"},{"location":"coding/sam/#useful-information","title":"Useful information","text":"<ul> <li>The template anatomy documentation goes over the template structure. A must read.</li> <li>Yaml template elements</li> <li>Resources and property ref</li> <li>SAM Workshop.</li> <li>Workshop: Serverless Task App with SAM and Amplify with git repo serverless-tasks-webapp.</li> </ul>"},{"location":"coding/sam/#how-to-assessment","title":"How to / assessment","text":"Difference between SAM template and CloudFormation <p>SAM template is an abstraction above CloudFormation (<code>Transform: AWS::Serverless-2016-10-31</code>) but it reuses cloud formation templates for IaC in the \"resources\" section. It includes a \"Global section\" to define properties that are common to all your serverless functions and APIs. Product doc</p> How to create and validate a SAM template? <p>Use CLI: </p> <pre><code>sam init -h\nsam init --runtime python3.9 --dependency-manager pip --app-template hello-world --name sam-app\nsam validate\n</code></pre> Invoke function locally with SAM CLI <pre><code>sam local invoke -e ./event.json\n</code></pre> Declare an API Gateway <pre><code>RestAPI:\nType: AWS::Serverless::Api\nProperties:\n  StageName: Prod\n  TracingEnabled: true\n  Tags:\n    Name: !Sub \"${AWS::StackName}-API\"\n    Stack: !Sub \"${AWS::StackName}\" \n</code></pre> Run API Gateway locally with SAM CLI <pre><code>sam local start-api --port 8080\n</code></pre> Package and deploy an application with SAM CLI <pre><code>sam build\nsam deploy --guided\n</code></pre> Familiar with security best practices and how to leverage SAM policies when working with other services <p>To invoke a Lambda, from external service, we need to define resource policies. For example a Step function calling a Lambda <pre><code> ProcessFormStateMachineExpressSync:\n    Type: AWS::Serverless::StateMachine\n     Properties:\n        Policies:  \n            - LambdaInvokePolicy:\n                FunctionName: !Ref FunctionNameDeclareAbove\n</code></pre></p> <p>For the function to call a AWS service we need to define execution policy:</p> <pre><code>  OrderPublisher:\n    Type: AWS::Serverless::Function\n    Properties:\n    FunctionName: OrderPublisher\n    Handler: com.example.OrderPublisher::handleRequest\n    CodeUri: target/sourceCode.zip\n    Policies:\n        - DynamoDBCrudPolicy:\n            TableName: !Ref OrdersDDBTable\n</code></pre> Build, package a Lambda Layer and/or Lambda Extension <p>Lambda layer helps to reuse dependencies. Layers are uploaded as zip within S3 bucket. Content is in the <code>/opt</code> of the EC2 runner. This is the same process as function. <pre><code>aws lambda publish-layer-version --layer-name my-layer --description \"My layer\" \\\n--license-info \"MIT\" --zip-file fileb://layer.zip \\\n--compatible-runtimes python3.11 --compatible-architectures \"arm64\" \"x86_64\"\n</code></pre></p> <p>A layer can be shared with other AWS account via command like:</p> <pre><code>aws lambda add-layer-version-permission --layer-name my-layer --statement-id xaccount --action lambda:GetLayerVersion  --principal 111122223333 --version-number 1 \n</code></pre> To use a Layer or Extension <p>Add the declaration in the function declaration in the sam template like: <pre><code>   ...\n    Layers:\n    - !Ref libs\nlibs:\nType: AWS::Lambda::LayerVersion\nProperties:\n  LayerName: my-layer\n  Description: Dependencies for the blank sample app.\n  Content:\n    S3Bucket: my-bucket-region-123456789012\n    S3Key: layer.zip\n  CompatibleRuntimes:\n    - python3.11\n</code></pre></p> Implement AWS Step Functions workflow using SAM <p>There are multiple sources of template in the serverless-patterns github repo. New project can be created by using one of the Multiple Step function choices.</p> Able to describe developer workflows <p>Start by creating a sam project, and then run locally to test the function code locally. Add unit test, package, and deploy to AWS account.</p> Familiar with AWS CDK and integration with SAM CLI <p>We can use SAM within a CDK project. In summary we do:</p> <ul> <li>create CDK app: <code>cdk init app --language python</code></li> <li>Add lambda declaration in the CDK stack</li> <li>Implement the lambda function</li> <li>Test locally using SAM CLI and the Stack generated by CDK: <pre><code>cdk synth --no-staging\nsam local invoke MyFunction --no-event -t ./cdk.out/CdkSamExampleStack.template.json\n</code></pre></li> <li>Build using SAM <pre><code>sam build -t ./cdk.out/Stackname.template.json\n</code></pre></li> <li>Deploy via <code>cdk deploy</code></li> </ul> How to use and deploy SAM Pipelines <pre><code>sam pipeline init --bootstrap\n</code></pre> How to perform canary release or other safe deployment with SAM <p>We can use canary deployment for different Lambda function version by using alias traffic shifting. Need to add the following within the function declaration in SAM template: <pre><code>AutoPublishAlias: live\nDeploymentPreference:\n    Type: Canary10Percent5Minutes\n</code></pre></p> <p>Another type is the linear strategy: <code>Linear10PercentEvery3Minutes</code>. When using pipeline and CodeDeploy, it is possible to see the canary release in action and CodeDeploy can be configured to roll-back if there is an issue coming from CloudWatch alarm.</p> Deep understanding of debugging and testing strategies with SAM <p>AWS SAM Accelerate (<code>sam sync --watch</code>) enables developers to test their code quickly against production AWS services in the cloud and avoid <code>sam deploy</code> with CF deployment. </p> Use environment variables <p>Declaring an environment variables for a function, and to get the code from an external python script. The Function is also using a policy to be able to CRUD on the DynamoDB table.</p> <pre><code>UsersFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: src/api/users.lambda_handler\n      Description: Handler for all users related operations\n      Environment:\n        Variables:\n          USERS_TABLE: !Ref UsersTable\n      Policies:\n        - DynamoDBCrudPolicy:\n            TableName: !Ref UsersTable\n      Tags:\n        Stack: !Sub \"${AWS::StackName}\"\n</code></pre> Enabling X-Ray tracing <p>Add <code>TracingEnabled: true</code> properties</p> Add REST API definition for a function <p>Add the following events declaration in the Lambda function (which will be in the triggers view in Lambda console). <pre><code>Events:\n    GetUsersEvent:\n      Type: Api\n      Properties:\n        Path: /users\n        Method: get\n        RestApiId: !Ref RestAPI\n    PutUserEvent:\n      Type: Api\n      Properties:\n        Path: /users\n        Method: put\n        RestApiId: !Ref RestAPI\n    UpdateUserEvent:\n      Type: Api\n      Properties:\n        Path: /users/{userid}\n        Method: put\n        RestApiId: !Ref RestAPI\n    GetUserEvent:\n      Type: Api\n      Properties:\n        Path: /users/{userid}\n        Method: get\n        RestApiId: !Ref RestAPI\n    DeleteUserEvent:\n      Type: Api\n      Properties:\n        Path: /users/{userid}\n        Method: delete\n        RestApiId: !Ref RestAPI\n</code></pre></p> Add Cognito User Pool <pre><code>UserPool:\nType: AWS::Cognito::UserPool\nProperties: \n  UserPoolName: !Sub ${AWS::StackName}-UserPool\n  AdminCreateUserConfig: \n    AllowAdminCreateUserOnly: false\n  AutoVerifiedAttributes: \n    - email\n  Schema: \n    - Name: name\n      AttributeDataType: String\n      Mutable: true\n      Required: true\n    - Name: email\n      AttributeDataType: String\n      Mutable: true\n      Required: true\n  UsernameAttributes: \n    - email\n  UserPoolTags:\n      Key: Name\n      Value: !Sub ${AWS::StackName} User Pool\n</code></pre> Specify a s3_bucket to upload code <p>add s3_bucket in sam</p> <pre><code>\n</code></pre>"},{"location":"coding/sam/#my-examples","title":"My examples","text":"<ul> <li>Repo to illustrate Getting GitHub events to API gtw, Lambda and Slack in Python</li> </ul>"},{"location":"coding/serverless/","title":"Serverless CLI","text":"<p>The serverless Framework is a framework to develop AWS serverless app with external monitoring capabilities. It is a company in Sf, created from a CapitalOne project. Function\u2019s code and configuration are located in the same Git repository. </p>"},{"location":"coding/serverless/#installation","title":"Installation","text":"<p>Need nodejs installed, and then <code>npm install serverless -g</code></p>"},{"location":"coding/serverless/#create-new-service","title":"Create new service","text":"<p>Create using <code>serverless</code> cli (work on powershell, or mac terminal), or by cloning one of the repository and modifying code and <code>serverless.yaml</code></p> <p>See this first tutorial and the python implementation in labs/serverless-fwk/document-mgr</p>"},{"location":"data/","title":"Data services","text":"<p>Info</p> <p>Created 12/8/2022 - Updated 11/26/23</p> <p>AWS has a lot of services to manage data. This is an introduction of some services needed in a lot of solutions. </p> <p>For a product agnostic study on the data processing see these set of notes.</p> <ul> <li>The AWS services solve volume challenges are Amazon S3, AWS Lake Formation, Amazon Redshift. </li> <li>For Variety, Amazon Relational Database Service (Amazon RDS), Amazon DynamoDB, Amazon Redshift, Amazon OpenSearch Service</li> <li>To support Velocity, Amazon EMR, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon Kinesis, AWS Lambda.</li> <li>Veracity is supported by EMR, AWS Glue (prepare and integrate all the data at any scale), AWS Glue DataBrew (clean and normalize data faster and more efficiently), and Amazon DataZone (share data across the organization with built-in governance).</li> <li>Value can be addressed using Amazon QuickSight, Amazon SageMaker, Amazon Athena</li> </ul>"},{"location":"data/#aws-glue","title":"AWS Glue","text":"<p>Serverless data integration and data pipeline to do ETL jobs. We can discover and connect to over 70 diverse data sources, manage our data in a centralized data catalog, and visually create, run, and monitor ETL pipelines to load data into our data lakes. Pay only for resources used while running.</p> <p>The classical AWS Glue architecture looks like:</p> <p></p> <ul> <li>Data catalog is built by using crawlers to different data sources.</li> <li>ETL can be done on data streaming or jobs.</li> <li>To avoid reprocessing data, it use Job Bookmarks.</li> <li>Glue Elastic View is a feature to combine and replicate data across multiple data stores using SQL. It is like virtual table.</li> <li>Glue DataBrew to clean and normalize data using pre-built transformation.</li> <li>Glue Studio, GUI to create, run and monitor ETL jobs</li> <li>Glue Streaming ETL built on Apache Spark Structured Streaming to do data streaming compatible with Kafka, MSK and Kinesis data streaming.</li> <li>Product documentation</li> <li>AWS Glue samples repository</li> </ul>"},{"location":"data/#use-cases","title":"Use cases","text":"<ul> <li>ETL pipeline building. Include real-time data as source.</li> <li>Data preparation and data profiling without coding. AWS Glue crawler to quickly discover the data and create the Data Catalog. </li> <li>Quick job orchestration and visualization with drag and drop.</li> <li>Real-time data processing</li> </ul>"},{"location":"data/#glue-schema-registry","title":"Glue Schema Registry","text":"<p>Centralize schema management. Support Avro 1.10, Json Schema, Protobuf, Java. It allows disparate systems to share a schema for serialization and de-serialization.</p> <p>A registry has an Amazon Resource Name so we can define access permissions to schema and registry. We define iam policies for producer and consumers differently.</p> <p>Versioning is governed by a compatibility rule that is applied on a schema.</p> <p>A schema version that is marked as a checkpoint is used to determine the compatibility of registering new versions of a schema.</p> <p>Try to use BACKWARD compatibility rules, so consumer can read current and previous schema version.</p> <p>AWS provides open-source Serde libraries as a framework for serializing and deserializing data. The serializer decorates each record with the schema version ID.</p> <p>Cached schema version is for 24 hours.</p> <p>We should use AWS PrivateLink to connect any data producer\u2019s VPC to AWS Glue by defining an interface VPC endpoint for AWS Glue.</p> <p>Quota: 10 registries per AWS account per AWS Region.</p>"},{"location":"data/#amazon-elasticache","title":"Amazon ElastiCache","text":"<p>This is the in-memory database service for AWS, at the microseconds access read level.</p> <p>It supports managed Redis (Leader on the key-value store market) or Memcached clusters. Applications queries ElastiCache, if data is not available, gets it from RDS and store it in ElastiCache. </p> <p>It can be used for user session store so user interaction can go to different application instances. </p> <p>ElastiCache for Redis is a multi AZ with Auto-Failover, supports read replicas to scale on the read operations and to have high availability. It has data durability using Read Only File Feature, AOF, persistence, and has backup and restore features. It is supporting cross region replication, with up to 500+ nodes per cluster.</p> <p>Amazon MemoryDB for Redis</p> <p>MemoryDB for Redis is a durable, in-memory database for workloads that require an ultra-fast, primary database. It is used for application using Redis data structures and APIs with a primary, durable database. While ElastiCache for Redis is a service that is commonly used to cache data from other databases and data stores using Redis.</p> <p>Redis supports sharding to scale the data partitioning.</p> <p>Redis authentication tokens enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security. IAM Auth is not supported by ElastiCache.</p> <p>Memcached is a multi-node, used for partitioning of data (sharding), it is not persisted, no backup and restore, no HA via replication. It is based on a multi-threaded architecture. Loosing a node means loosing data.</p>"},{"location":"data/#patterns","title":"Patterns","text":"<p>Some patterns for ElastiCache:</p> <ul> <li>Lazy Loading: all the read data is cached, data can become stale in cache. Cache misses, make a call to the database backend.</li> <li>Write Through: Adds or update data in the cache when written to a DB (no stale data)</li> <li>Session Store: store temporary session data in a cache (using Time To Live features)</li> </ul> <p>Sub millisecond performance, in memory read replicas for sharding.</p>"},{"location":"data/#configuration","title":"Configuration","text":"<p>TBD add screen shots with some explanation</p> <p>For the configuration, it can also being deployed on premises via AWS Outposts.</p> <p>It does not support IAM authentication. But we can set a security token at the Redis cluster creation. For Redis we  can use Redis auth to force user to enter a password to connect. It supports SSL for in-flight encryption.</p>"},{"location":"data/#coding","title":"Coding","text":"<p>There is the Redis Sorted Sets to guarantee both uniqueness and element ordering. Each time a new element is added to the cache, it is ranked and added in the correct order.</p>"},{"location":"data/#documentdb","title":"DocumentDB","text":"<p>Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.</p>"},{"location":"data/#neptune-graphdb","title":"Neptune - GraphDB","text":"<p>Fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. </p> <p>The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.</p> <p>Neptune powers graph use cases such as recommendation engines, fraud detection, knowledge graphs, drug discovery, and network security.</p>"},{"location":"data/#appsync","title":"AppSync","text":"<p>AWS AppSync provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs.</p> <p>AppSync may be combined with DynamoDB to make it easy to build collaborative apps that keep shared data updated in real-time.</p>"},{"location":"data/#amazon-qldb","title":"Amazon QLDB","text":""},{"location":"data/#opensearch","title":"OpenSearch","text":"<p>OpenSearch is a managed services to search data with indexing. It is the new name of open source project, ElasticSearch. Can use up to 3 PB of attached storage.</p> <p>Pricing is pay for each hour of use of an EC2 instance and for the cumulative size of any EBS storage volumes attached to our instances. If a domain uses multiple Availability Zones, OpenSearch Service does not bill for traffic between the Availability Zones. </p>"},{"location":"data/#datazone","title":"DataZone","text":"<ul> <li>Service to catalog, discover, govern, share (cross regions and accounts), and analyze our data.</li> <li>Data producers can catalog and share data they produce</li> <li>Data consumers can discover and use those data</li> </ul> <p>The components:</p> <p></p> <ul> <li>Business data catalog </li> <li>Data projects: link people, teams, analytics tools,... Creating a project, creates IAM role, S3 bucket, Glue data catalog, and Athena database. Project may have profile that defines AWS underlying resources used.</li> <li>Data portal: WebApp to access all the catalog. </li> <li>Governance and access control: publish workflows to get data to the catalog.</li> </ul> <p></p>"},{"location":"data/#sources","title":"Sources","text":"<ul> <li>DataZone Getting started.</li> </ul>"},{"location":"data/#amazon-timestream","title":"Amazon Timestream","text":"<p>Time series database to support trillions of data points per day, keeping recent data in memory and moving historical data to a cost optimized storage tier based upon user defined policies.</p> <p>It includes a query engine with time based analytic functions</p> <p>Timestream is serverless and automatically scales up or down to adjust capacity and performance. The architecture uses different layers to increase decoupling, and a cellular architecture using smaller copies of the time tables, with specific networking end-points. The layers are:</p> <ul> <li>Write or Ingestion layer: Amazon Timestream routes writes for a table, partition, to a fault-tolerant memory store instance that processes high throughput data writes. It uses indexes and partitions data before writing to storage. Partitions are isolated and do not share data. When data is stored in Timestream, data is organized in time order as well as across time based on context attributes written with the data. It is a two dimensions space. Partitions are called Tiles.</li> <li>Storage layer: Responsible to manage the life cycle of the time series data, it offers two data storage: in-memory and on magnetic disk. Time series data close to current time are in-memory and automatically sent to longer storage after a certain time or some retention policy. Once on magnetic store, data is reorganized into a format that is highly optimized for large volume data reads. Other retention policies can be added to get rid of data not useful anymore. </li> <li>Query layer: Queries are done using SQL grammar. Queries are processed by an adaptive, distributed query engine that uses metadata from the tile tracking and indexing service to access and combine data across data stores at the time the query is issued. Queries are run by a dedicated fleet of workers where the number of workers enlisted to run a given query is determined by query complexity and data size.</li> </ul> <p>HA with automatic replications among 3 AZs within same region.</p> <p>With time series data, each data point consists of a timestamp, one or more attributes, and the event that changes over time.</p> <ul> <li>Tutorial to create a table, insert timestream records and query them.</li> <li>Sample app</li> </ul>"},{"location":"data/#data-migration-service","title":"Data Migration Service","text":"<p>As a managed service, AWS DMS is a server (EC2) in the AWS Cloud that runs replication software, to move data from one database to another running in AWS. Schema transformation can be performed using Schema Conversion Tool (SCT). SCT can run on-premises.</p> <p>Sources can be on-premises DBs, and the targets can still be on-premises DBs but most of time are  AWS data store such as RDS, Redshift, DynamoDB, S3, Elasticache, Kinesis data streams, DocumentDB...</p> <p>We can use continuous replication with DMS:</p> <p></p> <p>First 1/ use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, 2/ use the AWS Database Migration Service to migrate data from the source database to the target database, 3/ we can use AWS DMS data validation to ensure that the data has migrated accurately from the source to the target. DMS compares the source and target records and then reports any mismatches.</p> <p>Example of configuration, with EC2 instance type, engine version, allocated storage space for the replication instance, network connectivity...</p> <p></p> <p>Once server is defined, we need to define data provider to specify source and target endpoints and create a migration project to do schema conversion (from Oracle or SQL server to MySQL or PostgreSQL as target). </p> <p>It is possible to leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams as an easier way to do so: See Streaming Data from S3 to Kinesis using DMS. </p> <ul> <li>AWS DMS allows one-shot data migration and applies change data capture (CDC) files to these services.</li> <li>DMS FAQ</li> </ul>"},{"location":"data/#replication-to-aurora","title":"Replication to Aurora","text":"<p>For MySQL engine we have different options:</p> <ul> <li>Use RDS DB snapshot from RDS and restore it in Aurora</li> <li>Create an Autora read replica from the RDS MySQL source, when replication lag is 0, we can then promote Aurora as Write and Read.</li> <li>For external MySQL, use XtraBackup to create a file backup, upload it to S3 and import the file into Aurora from S3. Or use <code>mysqldump</code> utility to move to Aurora MySQL DB.</li> <li>Use DMS if both DBs run in parallel</li> </ul>"},{"location":"data/#aws-backup","title":"AWS Backup","text":"<p>Fully-managed service that makes it easy to centralize and automate data protection across AWS services, in the cloud, and on premises. It supports cross-region backups and in multiple AWS accounts across AWS Organizations.</p> <p>Src: EC2, EBS, EFS, Amazon FSx for windows, Lustre, and Storage Gateway, RDS, DynamoDB..</p> <p>Target is S3 bucket or Vault Lock.</p> <p>A backup plan is a policy expression that defines when and how we want to back up the AWS resources. It uses incremental backup. Backup plans are composed of one or more backup rules. The number of days to store data is called the backup lifecycle.</p> <p>Retention period for snapshots can be infinite, while the retention period for continuous backups can range from 1 day to 35 days.</p> <p>Backup Vault lock is used for Write Once Read Many state. It is to defend on malicious delete as backup cannot be deleted.</p> <p>We can create automated backup schedules and retention policies, lifecycle policies to expire unncecessary backups after a period of time.</p> <p>We can use AWS Backup Audit Manager to audit the compliance of our AWS Backup policies against controls that we define. A control is a procedure designed to audit the compliance of a backup requirement.</p> <p>AWS Backup helps to support the regulatory compliance or business policies for data protection.</p> <p>Combined with AWS Organizations, we can configure AWS Backup to monitor activities in all of the accounts in one place. We can define Organizations backup policies to create the backup plans across multiple accounts. It is not possible to delete or edit these backup plans individually within the account. With this solution, we will enforce the company's backup policy reliably and consistently.</p>"},{"location":"data/#application-discovery-service","title":"Application Discovery Service","text":"<p>Managed service to help plan the migration to the AWS cloud by collecting usage and configuration data about the on-premises servers. It is integrated with AWS Migration Hub, which simplifies the migration tracking as it aggregates the migration status information into a single console.</p> <p>It offers two ways of performing discovery: </p> <ul> <li>Agentless: work on VM inventory, configuration, and performance history</li> <li>Agent-based: by deploying AD Agent on each of the VMs and physical servers, it collects static configuration data, detailed time-series system-performance information, inbound and outbound network connections, and processes that are running.</li> </ul>"},{"location":"data/data-lake/","title":"Data Lake with AWS","text":"<p>Talend's definitions for data lake and data warehouse: \"A data lake is a vast pool of raw data, the purpose for which is not yet defined. A data warehouse is a repository for structured, filtered data that has already been processed for a specific purpose.\"</p> <p>In AWS context, a set of AWS services support your data strategy:</p> <p></p> <ul> <li>Amazon Redshift</li> <li>S3</li> <li>Aurora</li> <li>EMR: Created in 2009, it is a managed service to run Spark, Hadoop, Hive, Presto, HBase... Per-second pricing and save 50%-80% with Amazon EC2 Spot and reserved instances.</li> <li>DynamoDB</li> <li>Athena</li> <li>Glue</li> <li>OpenSearch</li> <li>Lake Formation</li> <li>SageMaker</li> </ul>"},{"location":"data/data-lake/#big-data","title":"Big Data","text":"<p>The 5 V's of big data are:</p> <ul> <li>Volume: terabytes, petabytes and even exabytes level. </li> <li>Variety: includes data from a wide range of sources and formats.</li> <li>Velocity: data needs to be collected, stored, processed and analyzed within a short period of time.</li> <li>Veracity: Trust the data. Ensure data integrity within the entire data chain: security and free from compromise.</li> <li>Value: get the usefulness from the data, by querying the data and generate reports.</li> </ul>"},{"location":"data/data-lake/#data-engineering-capabilities","title":"Data engineering capabilities","text":"<ul> <li>Continuous or scheduled data ingestion: ingest petabytes of data with auto-evolving schemas, read from files or streaming sources.</li> <li>Declarative ETL pipelines: intent-driven development. Automatic data lineage.</li> <li>Data quality validation and monitoring: support defining data quality and integrity controls.</li> <li>Fault tolerant and automatic recovery</li> <li>Data pipeline observability with data lineage, data flows diagrams, job monitoring.</li> <li>Support both batch and streaming processing</li> </ul>"},{"location":"data/data-lake/#lake-formation","title":"Lake Formation","text":"<p>AWS Lake Formation is a service that makes it easy to set up a secure data lake in days instead of month. A data lake is a centralized, curated, and secured repository that stores all the data, structured and unstructured, both in its original form and prepared for analytics.</p> <p>If we already use S3, we typically begin by registering existing S3 buckets that contain our data. Lake Formation creates new buckets for the data lake and import data into them. It adds its own permissions model, with fine grained access control down to the column, row or cell level.</p> <p>Amazon S3 forms the storage layer for Lake Formation. </p> <p>AWS Lake Formation is integrated with AWS Glue which we can use to create a data catalog that describes available datasets and their appropriate business applications. Lake Formation lets us define policies and control data access with simple \u201cgrant and revoke permissions to data\u201d sets at granular levels.</p>"},{"location":"data/data-lake/#features","title":"Features","text":"<ul> <li> <p>Ingestion and cleaning</p> <ul> <li>AWS Glue</li> <li>Serverless Spark</li> <li>Blueprints: predefined template to ingest data in one shot or incremental load.</li> <li>ML transforms: deduplications</li> </ul> </li> <li> <p>Security</p> <ul> <li>Data catalog</li> <li>Centralized permissions</li> <li>Real time monitoring</li> <li>Auditing</li> </ul> </li> <li> <p>Analytics &amp; ML - Integration</p> <ul> <li>Redshift Spectrum</li> <li>EMR</li> <li>Glue</li> <li>Athena</li> <li>Quicksight</li> </ul> </li> </ul>"},{"location":"data/data-lake/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>How data formation works</li> <li>Create a data lake from a JDBC source</li> <li>Building secured data lakes on AWS</li> <li>Cost modeling data lakes - whitepaper</li> </ul>"},{"location":"data/dynamodb/","title":"DynamoDB","text":"<p>AWS proprietary NoSQL database, Serverless, provisioned capacity, auto scaling, on demand capacity. Fully managed,  Highly Available with replication across multiple AZs in one AWS Region by default, Read and Writes are decoupled, and DAX can be used for delivering a read cache. </p> <p>Single digit ms latency, even with increased number of requests. Can support millions of requests per second, trillions of row, 100s of TB storage. Data is stored on solid-state disks (SSDs) and may be encrypted at rest.</p> <p>It is integrated with IAM for authentication and authorization.</p> <p>A <code>table</code> is a collection of <code>items</code>, and each item is a collection of <code>attributes</code>. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.  Useful when the solution does not want to design a data schema upfront and which may change overtime. </p> <p>The maximum size for one item (or record) in a table is 400k.</p> <p>The read operations can be eventually consistent or strongly consistent.</p> <p>There is two capacities modes: </p> <ul> <li>provisioned: where you specify and pay for read capacity units and write capacity units. Need to plan beforehand. Less expensive. Used when we know the traffic pattern.</li> <li>on-demand: read and writes automatically scale up/down with your workloads. Better for unpredictable workloads. More expensive.</li> </ul> <p>Indexing is done using the primary key, but we can define secondary indexes which are a group of attributes to be used in queries. Global secondary index can span all of the data in the base table, across all partitions. It is stored in its own partition space away from the base table and scales separately from the base table.</p>"},{"location":"data/dynamodb/#scoping-considerations","title":"Scoping considerations","text":"<ul> <li>Does the data model fit, key-value approach? what wil lbe the primary key?</li> <li>Do you need to support complex queries or just get by the primary key?</li> <li>Do you need global access patterns or data distribution across regions?</li> <li>Review the performance needs for read and write per second. Can operations be split between regions?</li> <li>How large will individual items be? How many items over time? Should items be deleted?</li> <li>What write/read consistency do you require? </li> </ul>"},{"location":"data/dynamodb/#quotas-limit-pricing","title":"Quotas / Limit / Pricing","text":"<ul> <li>Service, account, and table quotas in Amazon DynamoDB.</li> <li>Pricing</li> </ul>"},{"location":"data/dynamodb/#security","title":"Security","text":"<p>DynamoDB defined a set of AWS managed policies that defined access policies on DynamoDB features, like replications, Read-write or read-only access.</p> <p>DynamoDB supports resource-based policies for tables, indexes, and streams.</p> <p>All user data stored is fully encrypted at rest, using symmetric keys in KMS (AWS owned (by default), AWS managed key, or customer managed key). It uses 256-bit Advanced Encryption Standard (AES-256). This includes primary key, local and global secondary indexes, streams, global tables, backups, and DynamoDB Accelerator (DAX) clusters. Developer cannot encrypt only a subset of items in a table: all table data is encrypted.</p> <p>DynamoDB uses envelope encryption and key hierarchy to encrypt data. Your AWS KMS encryption key is used to encrypt the root key of this key hierarchy. Developer can use AWS CloudTrail and Amazon CloudWatch Logs to track the requests that DynamoDB sends to AWS KMS.</p> <p>When client app access an encrypted table, DynamoDB decrypts the table data transparently. Call to KMS is not done at each table operation. The key is refreshed once every 5 minutes per caller with active traffic. Be sure to reuse connection with the SDK code. </p> <p>Developer can change the KMS key for a table at any time, either in the DynamoDB console or by using the <code>UpdateTable</code> operation.</p> <p>All the data in DynamoDB is encrypted in transit. By default, communications to and from DynamoDB use the HTTPS protocol. Data in the application, needs to be encrypted before sending it to Dynamodb using client side encryption. If you store sensitive or confidential data in DynamoDB, consider including client-side encryption in your security plan. See the What is the AWS Database Encryption SDK  product documentation.</p> <p>Below is an example of approach using a KMS AWS managed key, knowing its arn in KMS:</p> <pre><code>kms = boto3.client('kms')\n\ndef encrypt(text_to_encrypt):\n    return kms.encrypt(\n        KeyId=key_arn, \n        Plaintext=text_to_encrypt\n    )['CiphertextBlob']\n\nitem['ssn']= encrypt(item['ssn']) \ndynamodb.put_item(\n  TableName='MyTable',\n  Item=item\n)\n</code></pre> <p>Attention, with customer managed keys, deletion of the key, will make the tables not accessible. You cannot use a customer managed key with DynamoDB Accelerator (DAX) clusters.</p> <p>See also this important usage note.</p>"},{"location":"data/dynamodb/#coding","title":"Coding","text":"<p>With the aws CLI:</p> <pre><code>aws dynamodb create-table --table-name Orders \\\n                          --attribute-definitions AttributeName=orderID,AttributeType=S \\\n                          --key-schema AttributeName=orderID,KeyType=HASH \\\n                          --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\n</code></pre> <p>See AWS dynamodb cli cheat sheet</p>"},{"location":"data/dynamodb/#sdk","title":"SDK","text":"<p>Create Table with Python SDK:</p> <pre><code>import boto3\n\n# Create DynamoDB client\ndynamodb = boto3.client('dynamodb')\n\n# Table attributes\ntable = 'Products'\nkey_schema = [\n    {'AttributeName': 'Id', 'KeyType': 'HASH'},\n]\n\nattribute_definitions = [\n    {'AttributeName': 'Id', 'AttributeType': 'N'},  \n]\n\n# Create the table\ntry:\n    response = dynamodb.create_table(                \n        TableName=table,\n        KeySchema=key_schema,\n        AttributeDefinitions=attribute_definitions,\n        ProvisionedThroughput={\n            'ReadCapacityUnits': 10,\n            'WriteCapacityUnits': 5\n        }\n    )\nexcept:\n    print(\"Unable to create table:\", table)\n\nelse:\n    print(f\"Created table {table}\")\n</code></pre> <ul> <li>Transactional write with TransactWriteItems API</li> </ul> <pre><code>dynamodb = boto3.client('dynamodb') \n# prepare data\nitems = [\n  {\n    'Put': {\n      'TableName': table,  \n      'Item': {\n        'Id': {'N': '123'}, \n        'Name': {'S': 'Item 1'}\n      }\n    }\n  },...\n\ntry:\n  response = dynamodb.transact_write_items(\n    TransactItems=items,\n    ClientRequestToken=\"unique_idempotency_id\"\n  )\n\nexcept ClientError as e:\n</code></pre>"},{"location":"data/dynamodb/#cdk","title":"CDK","text":"<p>Create a dynamodb instance with CDK:</p> <pre><code>class CdkStack(Stack):\n\n    def __init__(self, scope: Construct, construct_id: str, **kwargs) -&gt; None:\n        super().__init__(scope, construct_id, **kwargs)\n        # create dynamo table\n        order_table = aws_dynamodb.Table(\n            self, \"orders\",\n            partition_key=aws_dynamodb.Attribute(\n                name=\"orderID\",\n                type=aws_dynamodb.AttributeType.STRING,\n            ),\n            table_class= aws_dynamodb.TableClass.STANDARD_INFREQUENT_ACCESS,\n            billing_mode=aws_dynamodb.BillingMode.PAY_PER_REQUEST,\n            # make it global\n            #replicationRegions= ['us-west-1', 'us-east-1', 'us-west-2'],\n        )\n</code></pre> <p>See also the examples for CDK.</p>"},{"location":"data/dynamodb/#partisql","title":"PartiSQL","text":"<p>A SQL-compatible query language, to select, insert, update, and delete data in Amazon DynamoDB.</p>"},{"location":"data/dynamodb/#other-coding-pattern","title":"Other coding pattern","text":"<ul> <li>Using quarkus, docker image for dynamodb, : Add dynamodb extension, and do data transformation between item and the business entity managed by the REST resource.</li> <li>For client application, there is this nice tutorial from Quarkus.</li> </ul>"},{"location":"data/dynamodb/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator - DAX","text":"<p>To address read congestion of the read operations, DAX is a managed service of a distributed cache cluster in front of DynamoDB. It brings microsecond latency. The APIs are the same as DynamoDB's APIs. </p> <p>It caches the most frequently used data, thus offloading the heavy reads on hot keys off the DynamoDB table, hence preventing the \"ProvisionedThroughputExceededException\" exception.</p>"},{"location":"data/dynamodb/#dynamodb-stream-processing","title":"DynamoDB Stream processing","text":"<p>It is possible to get ordered stream of item-level modification such as Create, Update, Delete in a DynamoDB table. It is relevant for:</p> <ul> <li>Reacting on changes in real-time.</li> <li>Real-time usage analytics</li> <li>Inserts into derivative tables</li> <li>Cross-region replication</li> <li>Lambda invocation on item updates </li> </ul> <p></p>"},{"location":"data/dynamodb/#dynamodb-global-table","title":"DynamoDB global table","text":"<p>The goal is to make a table accessible with low latency from different regions. It uses two-way replication, active-active so application can read and write to the table in any region. We need to enable streaming to get the replication running.</p>"},{"location":"data/dynamodb/#dynamodb-time-to-live","title":"DynamoDB Time to live","text":"<p>This feature helps to remove records after a specified timestamp. It is used to clean old records, like older than 2 years, or for session data, to be removed after 2 hours.</p>"},{"location":"data/dynamodb/#backup","title":"Backup","text":"<p>DynamoDB supports continuous backup using point-in-time recovery (PITR). It can go up to the last 35 days. Recovery creates new table.</p>"},{"location":"data/rds/","title":"Data services","text":"<p>Info</p> <p>Created 12/8/2022 - Updated 11/26/23</p>"},{"location":"data/rds/#relational-database-service-rds","title":"Relational Database Service - RDS","text":"<p>Managed service for SQL based database (MariaDB, MySQL, PostgreSQL, SQL server, Oracle, Amazon Aurora), the only things customers do is to define their schema and optimize their queries, AWS manages scaling, HA, backups, software patching, server maintenance...  RDS is used for On Line Transaction Processing (OLTP). As an exception, RDS Custom for Oracle (or MSSQL) allows SRE to access and customize the database server host and operating system.</p> <ul> <li>A DB instance provides a network address called an endpoint.</li> <li>When deployed in own VPC, be sure to have at least two subnets, each in a separate Availability Zone. See VPC and RDS. Prefer installation in private subnet with no public IP address (avoid it at least). </li> <li>It uses general purpose SSD (gp2, gp3), provisioned IOPS (io1) or magnetic storage (magnetic storages do not have consistent performance). See instance storage. Amazon RDS volumes are built using Amazon EBS volumes.</li> <li>HA supported with Primary, in-synch replication to secondary instance. Read replicas can be used to scale reading operations and they are asynchronously replicated. Resources aren't replicated across AWS Regions unless you do so specifically.</li> </ul> <p></p> <ul> <li> <p>Support multi AZs for Reliability Availability with automatic failover to standby, app uses one unique DNS name. Continuous backup and restore to specific point of time restore. </p> </li> <li> <p>Transaction logs are backed-up every 5 minutes. Support user triggered snapshot.</p> </li> <li> <p>Supports Storage Auto Scaling to increase storage dynamically with automatic detections of running out of free storage and scale it. (Free storage &lt; 10%, low storage last at least 5 minutes, 6 hours have passed since last notification)</p> </li> <li> <p>For Oracle and MS SQL it is possible to setup a RDS custom. Where you have access to OS and Database.</p> </li> </ul> <p>From a solution architecture point of view:</p> <ul> <li>Operations:  small downtime when failover happens. For maintenance, scaling with read replicas, updating underlying EC2 instances, or restore EBS, there will be manual interventions.</li> <li>Security: AWS is responsible for OS security, we are responsible for setting up KMS, security groups, IAM policies, authorizing user accesses to the DB, and enforcing SSL.</li> <li>Reliability: Multi AZ feature helps to address reliability, with automatic failover to the standby instance in case of failures.</li> <li>Performance: depends on EC2 instance type, EBS volume type, ability to add Read Replicas. Doesn\u2019t auto-scale, adapt to workload manually. </li> <li>Fit for purpose: for OLAP prefer Redshift.</li> </ul>"},{"location":"data/rds/#multi-az-db-cluster-read-replicas","title":"Multi AZ DB Cluster - Read Replicas","text":"<p>Read replicas helps to scale the read operations. Can create up to 5 read-replicas within a AZ, across AZ and across regions. Replication is asynch (eventually consistent). Use cases include, reporting, analytics on existing DB, or develop a ML model. The application which needs to access a read-replica DB needs to change the connection parameters.</p> <p>AWS charges for network when for example data goes from one AZ to another. Replicas in the same region in RDS managed services are for free, cross regions has a network cost.</p> <p>Read replica is great for read-heavy workloads and takes the load off the primary database (BI jobs). </p>"},{"location":"data/rds/#dr-multi-az-db-instance","title":"DR - Multi AZ DB instance","text":"<p>RDS supports instance standby with synchronous replication from master to standby. The application talks to one DNS name, and there will be automatic failover, if connection to master RDS fails.</p> <p>Read-replica DB can be setup as a multi AZ for DR, but RTO is not 0. Read replicas can be promoted to become a primary database, but this is a manual procedure.</p> <p>It is possible to move from a Single-AZ to a Multi-AZ with zero downtime, by changing the multi-AZ parameter in the RDS.  The internal process is creating a snapshot from master, create a DB via restore operation in target AZ and start synchronous replication from master to standby.</p> <p>With Multi-AZ RDS, a failure of one node generally recovers within a minute by switching to the standby instance with the only availability impact being the time it takes for the DNS pointer to update.</p> <p>For a single-AZ RDS instance, the RTO of an outage where the data isn't impacted, just the EC2 instance, the availability impact would be the length of time it takes for the underlying EC2 instance to recover, which could take several minutes plus the time to replay any transaction logs.</p>"},{"location":"data/rds/#outpost-integration","title":"Outpost integration","text":"<p>AWS Outposts uses the same hardware as in public AWS Regions to bring AWS services, infrastructure, and operation models on-premises. With RDS on Outposts, you can provision managed DB instances close to the business applications that must run on-premises.</p>"},{"location":"data/rds/#security","title":"Security","text":"<ul> <li>Support at rest Encryption (need to specify it at launch time). Master needs to be encrypted to get encrypted replicas. </li> <li>We can create a snapshot from unencrypted DB and then copy it by enabling the encryption for this snapshot. From there, we can create an Encrypted DB</li> <li>For in-flight encryption use AWS TLS root certificates on the client side.</li> <li>To authenticate, traditional user/pwd can be used but also IAM roles to connect to your DB. Enable IAM Database Authentication.</li> <li>Audit Logs are sent to CloudWatch.</li> </ul> <p>Customer's responsibilities:</p> <ul> <li>Check the ports / IP / security group inbound rules in DB\u2019s SG</li> <li>In-database user creation and permissions or manage through IAM</li> <li>Creating a database with or without public access</li> <li>Ensure parameter groups or DB is configured to only allow SSL connections</li> <li>Specify a time window to do maintenance, for version to version migration for example.</li> </ul>"},{"location":"data/rds/#backup","title":"Backup","text":"<p>RDS has automatic bakcup executed daily during the maintenance window. But the transaction logs are backed-up every 5 minutes. </p> <ul> <li>It is possible to restore the DB states from oldest backup to 5mn ago. </li> <li>1 to 35 days of retention for automatic backup (0 to disable). If you use manual backup with snapshot, then retention is defined as long as you want.  </li> </ul> <p>Note</p> <p>The manual snapshot can be used when using the database rarely, and the cost of keeping a snapshot, is far less than letting the DB running. </p>"},{"location":"data/rds/#rds-proxy","title":"RDS Proxy","text":"<p>This is a managed service to keep a pool of database connections between the DB and the clients, so it will improve the performance to access to the database servers. The connections are kept alive. The client connects to the proxy as it will do with the database. It is valuable with Lambda to DB connection.</p> <p></p> <p>This is a serverless, autoscaling, HA over multi-AZ.</p> <p>With this failover on EDS or Aurora is reduced  by up to 66%.</p> <p>Enforce IAM Authentication for DB and securely store credentials in AWS secrets manager.</p> <p>Not expose to public internet. Only visible in the VPC.</p>"},{"location":"data/rds/#code-examples","title":"Code Examples","text":"<p>See Playground RDS</p> <ul> <li>Autonomous Car Ride uses PostgreSQL in RDS and quarkus app.</li> <li>SaaS demo - Tenant manager</li> </ul>"},{"location":"data/rds/#deeper-dive","title":"Deeper dive","text":"<ul> <li> <p>Burst vs Baseline with RDS and GP2:</p> <ul> <li>Dimensions that matter are the size, latency, throughput, and IOPS of the volume.</li> <li>With gp2, IOPS is a function of the size of the volume: 3x GiB with a min of 100 IOPS and a max of 10k IOPS</li> <li>Starting with I/O credit at 3000 IOPS, it can be consumed at burst and replinished at the rate of 3 IOPS per GiB per s.</li> <li>With disk above 1TB, the baseline performance &gt; burst perf.</li> </ul> </li> </ul>"},{"location":"data/rds/#amazon-aurora","title":"Amazon Aurora","text":"<p>Proprietary SQL database storage engine, works using PostgreSQL and mySQL drivers.</p> <ul> <li>Operations:  less operation, auto scaling storage: it can grow up by increment of 10GB from 10GB to 128 TB.</li> <li>Security: AWS is responsible for OS security, users are responsible for setting up KMS, security groups, IAM policies, authorizing users in DB, enforcing SSL.</li> <li>Reliability: Multi-AZ, HA with minimum of 3 AZs, with 2 copies of the data in each AZ. 6 copies overall.</li> <li>Performance: Sub 10ms replica lag, up to 15 replicas (MySQL has only 5 replicas). It costs 20% more than RDS. 5x performance improvement over mySQL on RDS, and 3x for PostgreSQL. The compute resources can scale up to 96 vCPUs and 768 GB of memory.</li> </ul>"},{"location":"data/rds/#ha-and-read-scaling","title":"HA and Read Scaling","text":"<p>Failover in Aurora is instantaneous. It\u2019s HA (High Availability) native. Use 1 master - 5 readers to create 6 copies of the data over 3 AZs. It supports cross-region replications.</p> <ul> <li>It needs 4 copies out of 6 to consider write operation as successful.</li> <li>And 3 copies out of 6 needed for read operations.</li> <li>There is a self-healing capability in case of data corruption with peer-to-peer replication.</li> <li>Storage is done across 100s of volumes.</li> <li>Autoscaling on the read operation from 1 to 15 read-replicas. </li> </ul> <p></p> <p>It is CQRS at DB level, and read can be global.</p> <p>Each connection is handled by a specific DB instance. When app connects to an Aurora cluster, the host name and port specify an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections.</p> <p>Use writer endpoint for write operation and reader endpoint to access read-replicas. Aurora automatically performing load-balancing among all the Aurora Replicas. It is also possible to design replicas to run on different EC2 server type, and then custom endpoints can be defined to access to those servers. This could be interesting for analytic queries.</p> <p>It also supports one write with multiple readers and parallel query, multiple writes.</p>"},{"location":"data/rds/#other-capabilities","title":"Other capabilities","text":"<p>With Aurora Global Database, one primary region is used for write and then up to 5 read only regions with replica lag up to 1 s. Promoting another region (for disaster recovery) has an RTO of &lt; 1 minute.</p> <ul> <li>Serverless: Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. An Aurora Serverless DB cluster is a DB cluster that automatically starts up, shuts down, and scales up or down its compute capacity based on the application\u2019s needs. Aurora Serverless provides a relatively simple, cost-effective option for infrequent, intermittent, sporadic or unpredictable workloads.</li> <li>Multi-master, to protect on write node failure. Every node supports read and write operations. The client has multiple DB connection definitions for failover.</li> <li>Global Aurora: across region replicas, or use Global Database with one primary region for R/W and up to 5 secondary regions (Read-only), with a replica lag &lt; 1s and up to 16 read replicas per secondary region. Promoting a region for DR should lead to a RTO &lt; 1 mn. It takes less than a second to do replicas cross region. </li> <li>Aurora has integration with ML services like SageMaker and Comprehend. </li> </ul>"},{"location":"data/rds/#backup_1","title":"Backup","text":"<p>Could not be disabled, and automatic is up to 35 days retention.</p> <p>It is possible to clone an existing Aurora DB, which is fast and cost-effective. For example to create a 'Staging' DB from production one. </p> <p>We can share snapshot with other AWS accounts.</p>"},{"location":"data/rds/#code-examples_1","title":"Code examples","text":"<ul> <li>Building serverless applications with Amazon Aurora Serverless</li> </ul>"},{"location":"data/rds/#fit-for-purpose","title":"Fit for purpose","text":"Needs Considerations Single digit latency DynamoDB SQL based OLTP RDS SQL based OLAP RedShift"},{"location":"data/redshift/","title":"Amazon RedShift","text":"<p>Redshift is the cloud data warehouse managed services, with no data movement or data transformation. Its goal is to manage up to 16 PB of data (it is like 500 billion pages of standard printed text, or 11,000 4k movies).</p>"},{"location":"data/redshift/#use-cases","title":"Use cases","text":"<ul> <li>Unified data warehouse and data lake.</li> <li>Get insights by running real-time and predictive analytics on all of your data, across operational databases, data lake, data warehouse, and thousands of third-party datasets.</li> <li>Support all analytics workload.</li> <li>Scalable, managed services, serverless, cost effective.</li> </ul>"},{"location":"data/redshift/#architecture","title":"Architecture","text":"<p>Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale.</p> <p>It is based on Postgresql but is not used for OLTP. It is used for analytical processing and data warehousing, scale to Peta Bytes. It is Columnar storage of data. It uses massively parallel query execution.</p> <p>Data can be loaded from S3, DynamoDB, DMS and other DBs. It can scale from 1 to 128 nodes, and each node has 160GB per node. Redshift spectrum performs queries directly on top of S3.</p> <p>The cluster architecture is based on a leader node to support query planning and aggregate results, manages metadata, and compute nodes to perform the queries, distribute the data and send results back.</p> <p></p> <ul> <li>Provision node size in advance, and we can use Reserved Instances for cost saving.</li> <li>Node types include Dense Compute (dc2.large ...) with a number of slices and storage in SSD, and Dense Storage (ds2.xlarge) based on HDD up to 2 PB, or RA3 with more vCPU and total capacity for very large datahouse.</li> <li>Slice is compute and storage and receives query script, executes those scripts and return results.</li> <li>During the cluster creation an tool can help size the cluster according to the data needs:</li> </ul> <p></p> <p>It will then propose on-demand, reserved (1 year), or 3 years pricing model.</p> <ul> <li>RedShift is not multi-AZ, but we can use snapshots to do point-in-time backup and store in S3. Snapshots are incremental. And backup can be restored in a new cluster.</li> <li>For DR purpose snapshot can be copied to another region.</li> <li>RedShift Spectrum helps to query data already in S3 without loading them. The query is submitted to thousands of redshift spectrum nodes.</li> </ul> <p></p> <ul> <li>The configuration includes network, VPC, subnet, security group, enhanced VPC routing to keep traffic through a VPC. Database name and port and if we want encryption using KMS or HSM. Specify maintenance window, monitoring and backup.</li> <li>For monitoring we can use CloudWatch alarm to monitor the disk usage of our cluster and send notification to a SNS topic.</li> <li>Once create the managed service offers a JDBC endpoint, and ODBC endpoint and HTTP API.</li> <li>Redshift Spectrum pushes many compute-intensive tasks, such as predicate filtering and aggregation, down to the Redshift Spectrum layer. Thus, Redshift Spectrum queries use much less of your cluster's processing capacity than other queries. </li> </ul>"},{"location":"data/redshift/#features","title":"Features","text":"<ul> <li>Serverless or create and manage your own cluster</li> <li>Set up SQL Workbench/J to access your data in the Amazon Redshift cluster using the Amazon Redshift JDBC driver.</li> <li>Analyze the data with standard SQL from SQL Workbench/J.</li> </ul>"},{"location":"data/redshift/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>Product documentation</li> <li>Deploy a data warehouse</li> </ul>"},{"location":"infra/","title":"Major infrastructure services","text":"<p>Info</p> <p>Created Sep 23 - Updated 07/01/2024</p>"},{"location":"infra/#amazon-elastic-compute-cloud-ec2-components","title":"Amazon Elastic Compute Cloud - EC2 components","text":"<ul> <li>EC2 is a renting machine, where users size for what the plan to use.</li> <li>Amazon EC2 instances are a combination of virtual processors (vCPUs), memory, network, graphics processing units (GPUs), and, instance storage.</li> <li>EC2 may store data on EBS virtual drives..</li> <li>Distribute network load across machines using Elastic Load Balancer.</li> <li>Auto scale the instances via Auto Scaling Group.</li> </ul> <p>EC2 can have MacOS, Linux or Windows OS.</p> <p>Amazon Machine Image (AMI) is the OS image with preinstalled softwares. Amazon Linux 2 for linux base image. See <code>AMI Catalog</code> within the selected region to get what AMI could be used.</p> <p></p> <p>Figure 1: EC2 instance</p> <p>When creating an instance, user can select the OS, CPU, RAM, the VPC, the subnet, the storage (EBS) for root folder, the network card, and the firewall rules defined as Security Group.</p> <p>The security group helps to isolate the EC2 instance, for example, to authorize traffic for ssh on port 22 (TCP) and HTTP on port 80. Get the public ssh key, and when the instance is started, use a command like: <code>ssh -i EC2key.pem  ec2-user@ec2-52-8-75-8.us-west-1.compute.amazonaws.com</code> to connect to the EC2 via ssh. On the client side, the downloaded <code>.pem</code> file needs to be restricted with <code>chmod 0400</code>.</p> <p>User may also use EC2 Instance Connect to open a shell terminal from the web browser (still needs to get SSH port accessible in the security group).</p> <p>vCPU represents thread running on core CPU. We can optimize vCPU allocation on the EC2 instance, once created, by updating the launch configuration.</p> Warning <p>There is a vCPU-based On-Demand Instance limit per region which may impact the creation of new instance. Just submit the limit increase form to AWS and retry the failed requests once approved.</p> <p>See this EC2 playground for the deployment of a HTTP server demonstration.</p> <p>or install nginx on EC2</p>"},{"location":"infra/#ec2-life-cycle","title":"EC2 life cycle","text":"<ol> <li>When users launch an instance, it enters in the <code>pending</code> state. Billing is started when in <code>running</code> state.</li> <li>During rebooting, instance remains on the same host, and maintains its public and private IP addresses, in addition to any data on its instance store.</li> <li>Only EBS-backed instances can be stopped and restarted. With stopped instance, the instance can be rebooted or terminated.</li> <li>When users <code>terminate</code> an instance, the instance stores are erased, and we lose both the public IP and private IP addresses of the machine. Storage for any Amazon EBS volumes is still available and billed. Note that Reserved Instances that applied to terminated instances are still billed until the end of their term (1 or 3 years) according to their payment option.</li> <li>We will be billed when our On-Demand instance is preparing to hibernate with a stopping state.</li> <li>If the instance is stopped, AWS usually moves the instance to a new host.</li> </ol> <p>When user launches a new EC2 instance, the EC2 service (control plane) attempts to place the instance in such a way that all of the user's instances are spread out across underlying hardware to minimize correlated failures. Developer may use placement groups to influence the placement of a group of interdependent instances to meet the needs of the workload. For example in case of high performance computing, using a cluster placement group makes sense.</p> <p>EC2 has a section to add <code>User data</code>, which could be used to define a bash script to install dependent software and to start some services at boot time (like <code>httpd</code>).</p>"},{"location":"infra/#ec2-hibernate","title":"EC2 Hibernate","text":"<p>When hibernating, in memory state is preserved, persisted to a file in the root EBS volume. It helps to make the instance startup time quicker. The root EBS volume needs to be encrypted.</p> <p>To use hibernation for supported EC2 instance types, user must configure hibernation at the first launch of the EC2 instance. Provide the required root volume encryption by creating an AMI from an encrypted snapshot of the existing volume.</p> <ul> <li>Memory is constrained by 150GB RAM.</li> <li>No more than 60 days.</li> <li>No instance store volume possible.</li> </ul>"},{"location":"infra/#ec2-types","title":"EC2 types","text":"<p>EC2 instance types like t2.micro or c5.2xlarge define CPU, memory capacities (see ec2instances.info or the reference AWS ec2/instance-types). The first letter defines the class as:</p> <ul> <li>R: (memory) applications that needs a lot of RAM \u2013 in-memory caches.</li> <li>C: (Compute Optimized) applications that needs good CPU \u2013 compute / databases, ETL media transcoding, High Perf web servers, scientific modeling.</li> <li>M:  applications that are balanced (think \u201cmedium\u201d) \u2013 general / web app.</li> <li>I: (storage) applications that need good local I/O (instance storage) \u2013 databases, NoSQL, cache like Redis, data warehousing, distributed file systems.</li> <li>G: applications that need a GPU.</li> <li>T2/T3 for burstable instance: When the machine needs to process something unexpected (a spike in load for example), it can burst. Use burst credits to control CPU usage.</li> </ul> <p>Graviton processors are designed by AWS for cloud workloads to optimize cost and energy consumption. (t4g., M6g., C7*). It uses Arm64 architecture (not x86 compatible). Every vCPU is a full physical core, and use a single non-uniform memory access to get every CPU same access time to memory. 40% better price/performance ratio against other processors.</p>"},{"location":"infra/#ec2-nitro-system","title":"EC2 Nitro System","text":"<p>The Nitro System is a rich collection of building blocks to bring flexible design and rapidly deliver EC2 instance types with an ever-broadening selection of compute, storage, memory, and networking options. It uses new virtualization infrastructure and hypervisor. Supports IPv6, better I/O on EBS and better security.</p> <p>Nitro System includes next generations of EC2. Instance type starts with C5, D5,...</p>"},{"location":"infra/#launch-types","title":"Launch types","text":"<ul> <li>On-demand: short workload, predictable pricing, pay per second after first minute. No long term commitment.</li> <li>Reserved for one or 3 years term, used for long workloads like database. Get discounted rate from on-demand. Up to 72% discount. Upfront cost and pay monthly. We can buy and sell reserved instance in the marketplace. When no more needed, <code>Terminate</code> the Reserved Instance as soon as possible to avoid getting billed at the on-demand price when renting term expires.</li> <li>Convertible reserved instance for changing resource capacity over time.</li> <li>Scheduled reserved instance for job based workload, used on a recurring schedule. (Not all instance types are available)</li> <li>Dedicated hosts to book entire physical server and control instance placement. # years. Use cases arounf BYOL (Used to port Microsoft license). Can be on-demand or reserved. Most expensive solution. Use it when users deploy a database technology on an EC2 instance and the vendor license bills based on the physical cores. Bare-metal is part of this option.</li> <li>Capacity reservations: reserve capacity in a specific AZ for any duration.</li> <li> <p>Spot instance for very short execution - 90% discount vs on-demand - used for work resilient to failure, like batch job, data analysis, image processing, stateless, containerized...</p> <ul> <li>Define a max spot price and get the instance while the current spot price &lt; max price user wants to pay. The hourly spot price varies based on offer and capacity.</li> <li>If the current spot price &gt; max, then instance will be stopped within 2 minutes.</li> <li>With spot block, we can define a time frame without interruptions from 1 to 6 hours.</li> <li>The expected state is defined in a 'spot request' which can be cancelled. One time or persistent request types are supported. Cancel a spot request does not terminate instance, but this is the first thing to do to avoid cost.</li> <li>Spot fleets allow to automatically request spot and on-demand instances with the lowest price to meet the target capacity within the price constraints.</li> </ul> </li> </ul> <p>Use EC2 launch templates to automate instance launches, to simplify permission policies, and to enforce best practices across the organization. Look very similar to docker image. It includes AMI, instance type, EBS volume, a subnet, ... an user data. Launch templates are versioned and defined in one region. A template can be created from another one.</p>"},{"location":"infra/#instance-pools","title":"Instance pools","text":"<p>When facing fluctuating demands, it is critical to maintain fault tolerance and application availability in a cost effective manner.</p> <p>For each instance family (C5,..), each instance size (8XL, 4XL...), in each availability zone, in every region, there is a separate pool of machines, with different availability and different pricing.</p> <p>So to be sure to get spot instances, we need to be flexible on the family, size, AZ and region.</p> <p>Workload needs to be flexible to account for interruptions and or location to maximize application uptime.</p>"},{"location":"infra/#metadata","title":"Metadata","text":"<p>Within an EC2 instance shell, we can get access to EC2 metadata by going to the URL: http://169.254.169.254/latest/meta-data/.</p> <p>We can also review scripts used to bootstrap the instances at runtime using http://169.254.169.254/latest/user-data/</p>"},{"location":"infra/#ec2-monitoring","title":"EC2 monitoring","text":"<p>Use the following metrics:</p> <ul> <li>CPU utilization with credit usage and balance</li> <li>Network In/Out</li> <li>Status check, with instance status to check EC2 VM, and system status for underlying hardware. They are used to do EC2 recovery: set a CloudWatch alarm like StatusCheckFailed_System to do the recovery. Recall that a recovery keeps private and public addresses, EIP, metadata and placement group.</li> <li>Disk with read/write for ops/bytes</li> <li>RAM is not included in EC2 metrics, need to add an agent in EC2 to get those metrics back to CloudWatch.</li> </ul>"},{"location":"infra/#ami","title":"AMI","text":"<p>Bring our own image. Shareable on Amazon Marketplace. Can be saved within S3 storage. By default, the AMIs are privates, and locked for the account / region (need copy between regions).</p> <p>AMIs can be copied and shared See AWS doc - copying an AMI.</p>"},{"location":"infra/#basic-fault-tolerance","title":"Basic Fault Tolerance","text":"<p>The following diagram illustrates some fault tolerance principles offered by the AWS services within one AZ:</p> <p></p> <p>Figure 2</p> <ul> <li>AMI defines image for the EC2 with static or dynamic configuration. From one AMI, user can scale by adding new EC2 based on same image.</li> <li>Instance failure can be replaced by starting a new instance from the same AMI.</li> <li>Auto Scaling Group groups EC2 instances to adapt their quantity according to the current instances' state or workload.</li> <li>To minimize down time, user can have one EC2 instance in <code>Standby</code> mode, and uses elastic IP addresses to be re-assigned in case of the primary EC2 failure. </li> <li>Data is saved on EBS and replicated to other EBS inside the same availability zone.</li> <li>Snapshot backup can be done to replicate data between AZs and/or regions, and persisted for long retention in S3. </li> <li>Need to flush data from memory to disk before any snapshot.</li> <li>Elastic Load Balancer balances traffic among servers in multiple AZs and DNS will route traffic to the good server.</li> <li>Elastic IP addresses are static and defined at the AWS account level, per region. New EC2 instance can be reallocated to Elastic IP @. EIPs are mapped by internet gateway to the private address of the EC2. The service may be down until new EC2 instance is restarted.</li> <li>ELB ensures higher fault tolerance for EC2s, containers, lambdas, IP addresses  and physical servers.</li> <li>Application LB load balances at the HTTP, HTTPS level, and within a VPC based on the content of the request.</li> <li>NLB is for TCP, UDP, TLS routing and load balancing.  </li> </ul>"},{"location":"infra/#placement-groups","title":"Placement groups","text":"<p>Define strategy to place EC2 instances:</p> <ul> <li>Cluster: groups instances into a low-latency group in a single Availability Zone.<ul> <li>Highest performance while talking to each other as when performing big data analysis.</li> </ul> </li> <li>Spread: groups across underlying hardware (max 7 instances per group per AZ).<ul> <li>Reduced risk in case of simultaneous failure.</li> <li>EC2 Instances are on different physical hardware.</li> <li>Used for application that needs to maximize high availability.</li> <li>Critical Applications where each instance must be isolated from failure from each other.</li> </ul> </li> <li>Partition: spreads instances across many different partitions (which rely on different sets of racks) within an AZ.<ul> <li>Partition is a set of racks.</li> <li>Up to 100s of EC2 instances.</li> <li>The instances in a partition, do not share racks with the instances in the other partitions.</li> <li>A partition failure can affect many EC2s but won\u2019t affect other partitions.</li> <li>EC2 instances get access to the partition information as metadata.</li> <li>Good for applications such as HDFS, HBase, Cassandra, Kafka, because they have their own replication machnism</li> </ul> </li> </ul> <p>Access from network and policies menu, define the group with expected strategy, and then use it when creating the EC2 instance by adding the instance to a placement group.</p>"},{"location":"infra/#ec2-network-bandwidth","title":"EC2 network bandwidth","text":"<p>Each EC2 instance has a maximum bandwidth for aggregate inbound and outbound traffic, based on instance type and size. The network bandwidth available to an EC2 instance depends on the number of vCPUs configured, and for less than 32 vCPUs the limit (single flow of 5-tuple) (source IP address/port number, destination IP address/port number and the protocol) is around 5 Gbps when instances are not in the same cluster placement group.</p> <p>Also bandwidth depends on the flow type: within one region or cross-regions.</p> <p>To meet additional demand, EC2s can use a network I/O credit mechanism to burst beyond their baseline bandwidth.</p> <p>However, instances might not achieve their available network bandwidth, if they exceed network allowances at the instance level, such as packet per second or number of tracked connections.</p> <p>Recalls that Enterprise-grade WAN and DIA links more commonly have symmetrical bandwidth, the data capacity is the same in both directions.</p>"},{"location":"infra/#ec2-instance-store","title":"EC2 Instance Store","text":"<p>See section in storage.</p>"},{"location":"infra/#optimizing-usage","title":"Optimizing usage","text":"<p>The new generation of infrastructure helps to optimize cost and capacity by looking at three key areas:</p> <ul> <li>EC2 saving plans and spot instances. Savings Plans are a flexible pricing model that offer low prices on EC2, Fargate and Lambda usage, in exchange for a commitment to a consistent amount of usage (1 or 3 years). It is accessible from EC2 console. It supports two types of Savings Plans: Compute Savings Plans and EC2 Instance SPs (EC2 instance family in a region).</li> <li>ODCR- On-Demand Capacity Reservations enable users to reserve compute capacity for the Amazon EC2 instances in a specific AZ for any duration. Create Capacity Reservation when running critical workloads with certain long term capacity (DR use cases, Events, Regulatory needs). No yearly commitment. Billing starts as soon as created. When creating the CR, users need to specify EC2 instance attributes like type, tenancy (shared HW or dedicated), OS (linux based)... that will be used to select EC2 from the pool. Capacity Reservations can be created in cluster placement groups.</li> <li>EC2 Auto scaling help to scale down.</li> <li>Compute Optimizer is a new, free service to recommend optimal compute resource to reduce costs up to 25%.  It analyzes CloudWatch metrics and considers ASG configuration for recommendations. </li> </ul>"},{"location":"infra/#documentation","title":"Documentation","text":"<ul> <li>EC2 product doc</li> <li>EC2 cheat sheet</li> </ul>"},{"location":"infra/#labs-playground","title":"Labs / Playground","text":"<ul> <li>Create keys and EC2 using console</li> <li>Create EC2 on default VPC with HTTPS server using CDK</li> <li>EC2 with Python Flask App in private subnet - CDK</li> <li>Create EC2 with Terraform</li> </ul>"},{"location":"infra/#quota","title":"Quota","text":"<p>Service Quotas is an AWS service that helps manage user's quotas for over 100 AWS services from one user interface. The AWS account has default quotas, formerly referred as limits, defined for each AWS service. Unless otherwise noted, each quota is Region specific. We can request increases for some quotas, and other quotas cannot be increased. Each EC2 instance can have a variance of the number of vCPUs, depending on its type and configuration, so it's always wise to calculate the vCPU needs to make sure developers are not going to hit quotas too early. Along with looking up the quota values, developers can also request a quota increase from the Service Quotas console.</p>"},{"location":"infra/#security-group","title":"Security group","text":"<p>Define inbound and outbound security rules.  It is like a virtual firewall inside an EC2 instance. SGs regulate access to ports, authorized IP ranges IPv4 and IPv6, control inbound and outbound network. By default all inbound traffic is denied and outbound authorized.</p> <ul> <li>They contain <code>allow rules</code> only.</li> <li>Can be attached to multiple EC2 instances and to load balancers.</li> <li>An EC2 instance can have up to five SGs</li> <li>Locked down to a region / VPC combination.</li> <li>SG lives outside of the EC2 definition so can be attached to any EC2.</li> <li>Define one separate security group for SSH access where we can authorize only one IP@.</li> <li>Connection refused is an application error or the app is not launched - Spinning wheel in the web browser is an access rules error.</li> <li>EC2 instances with the same security group can access each other.</li> <li>Security group can reference other security groups: doing so allows traffic to flow to and from instances that are associated with the referenced security group in the peered VPC. One IP address using CIDR in the form 192.45.23.12/32 but not any DNS server. The following is a bad SG as port SSH 22 is accessible from the internet: 0.0.0.0/0.</li> </ul> <p></p> <p>Figure 3: security group with inbound rules</p> <p>Important Ports:</p> <ul> <li>22 for SSH (Secure Shell) and SFTP.</li> <li>21 for FTP.</li> <li>80 for HTTP.</li> <li>443 for https.</li> <li>3389: Remote desktop protocol.</li> </ul>"},{"location":"infra/#auto-scaling-group-asg","title":"Auto Scaling Group (ASG)","text":"<p>The goal of an ASG is to scale out (add EC2 instances) to match an increased load, or scale in (remove EC2 instances) to match a decreased load. It can use different methods: predictive, dynamic, manual, instance level, or scheduled scaling. It helps to provision and balance capacity across Availability Zones to optimize availability. It uses historical data gathered from CloudWatch to build ML models to help calculating capacity requirements over time. The model continues to learn from new data to improve accuracy.</p> <p>It can also ensure,  having a minimum and a maximum number of machines running. It detects when an instance is unhealthy. And automatically registers newly created instance to a load balancer.</p> <p></p> <p>ASG has the following attributes:</p> <ul> <li>AMI + Instance Type with EC2 User Data (Can use launch template to define instances).</li> <li>EBS Volumes.</li> <li>Security Groups.</li> <li>SSH Key Pair.</li> <li>Min Size / Max Size / Initial Capacity to control number of instances .</li> <li>Network + Subnets Information to specify where to run the EC2 instances.</li> <li>Load Balancer Information, with target groups to be used as a grouping mechanism of the newly created instance.</li> </ul> <p></p> <ul> <li>Scaling Policies help to define rules to manage instance life cycle, based on CPU usage or network bandwidth usage, or count the number of request:</li> </ul> <p></p> <ul> <li> <p>ASG tries to balance the number of instances across AZs by default.</p> </li> <li> <p>The Default Termination Policy is designed to help ensure that the instances span Availability Zones evenly for high availability. Determine which Availability Zones have the most instances, and at least one instance that is not protected from scale in.</p> <ul> <li>Determine which instances to terminate so as to align the remaining instances to the allocation strategy for the on-demand or spot instance that is terminating. This only applies to an Auto Scaling Group that specifies allocation strategies. For example, after the instances launch, user may change the priority order of the preferred instance types. When a scale-in event occurs, Amazon EC2 Auto Scaling tries to gradually shift the on-demand instances away from instance types that are lower priority.</li> <li>Determine whether any of the instances use the oldest launch template or configuration: works for Auto Scaling Groups that use a launch template. Determine whether any of the instances use the oldest launch template unless there are instances that use this launch configuration. Amazon EC2 Auto Scaling terminates instances that use a launch configuration before instances that use a launch template.</li> <li>For Auto Scaling Groups that use a launch configuration: Determine whether any of the instances use the oldest launch configuration. After applying all of the above criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour. If there are multiple unprotected instances closest to the next billing hour, terminate one of these instances at random.</li> </ul> </li> <li> <p>When creating scaling policies, CloudWatch alarms are also created. Ex: \"Create an alarm if: CPUUtilization &lt; 36 for 15 data points within 15 minutes\".</p> </li> <li>To scale on unexpected peak of requests, it is better to use dynamic scaling based on the number of requests sent to each EC2, to anticipate when it goes higher, to start new instances on time. While looking at CPU will be too late.</li> <li>Target tracking scaling: we want average CPU to be under 40%.</li> <li>Scheduled action: increase capacity after 5 PM.</li> <li> <p>Predictive scaling by looking at historical behavior to build forecast rules.</p> </li> <li> <p>The capacity of the ASG cannot go over the maximum capacity we have allocated during scale out events.</p> </li> <li>Cool down period is set to 5 mn and will not change the number of instances until this period is reached.</li> <li>When an ALB validates an health check issue, ASG terminates the EC2 instance.</li> </ul> Tutorials: <ul> <li>Create your first Auto Scaling group: present how to use Launch Template, to create a single instance in one ASG. </li> <li>Set up a scaled and load-balanced application.</li> <li>Configure a lifecycle hook that invokes a Lambda function.</li> </ul> ASG, placement group cluster, launch template and capacity reservations? <p>Auto Scaling Group uses launch template to simplify instances creation, and to ensure best practices using the EC2 group. Placement group helps to place EC2s within infrastructure, and spread or partition can be used within one AZ. Capacity reservations target a specific AZ and EC2 type.</p> Change EC2 type in an already created launch configuration <p>It is not possible to modify a launch configuration once it is created. If we need to change the EC2 instance type, for example, developers need to create a new launch configuration to use the correct instance type. Modify the Auto Scaling group to use this new launch configuration. Delete the old launch configuration as it is no longer needed.</p> How to update an existing application running in EC2/ ASG? <p>Create a new launch template, still associated with the same ASG, the ALB will route to both instance versions, when confident of the new version, we can terminate the first launch template. An alternate solution is to create a second ALB's target group with a new ASG with the new launch template. ALB can do traffic shifting to new version. A 3nd solution, involves using 2 ALB -&gt; Target group -&gt; ASG, and use Route 53 CNAME with weighted record.</p> <p></p>"},{"location":"infra/#aws-outposts","title":"AWS Outposts","text":"<p>Outpost is fully managed service that extends AWS infrastructure, services, APIs, and tools to customer on-premises. It is a Rack with n server blades installed by AWS team, on site, and then maintained by AWS. </p> <p></p> <p>An Outpost extends a VPC from one AWS region, it is owned by one AZ and used to increase resiliency.</p> <p></p> <p>See pricing for Outpost rack.</p>"},{"location":"infra/#high-performance-computing-hpc","title":"High Performance Computing (HPC)","text":"<p>The services that helps to design HPC solutions are:</p> <ul> <li> <p>For data management and transfer:</p> <ul> <li>Direct Connect: moves GB of data over private secure network.</li> <li>Snowball &amp; Snowmobile at PB data transfer level.</li> <li>DataSync moves large dataset between on-premises and S3, FSx for Windows, or EFS.</li> </ul> </li> <li> <p>Computation and network:</p> <ul> <li>EC2 instance type using CPU or GPU optimized.</li> <li>Spot fleet and spot instances for cost savings and using Auto Scaling Group for auto scaling.</li> <li>Cluster Placement groups in same rack and AZ to get the best network performance.</li> <li>Enhanced Networking (SR-IOV): with high bandwidth, higher PPS, lower latency. </li> <li>Elastic Network Adapter (ENA) is up to 100 Gbps.</li> <li>Intel 82599 VF up to 10 Gbps (a legacy solution).</li> <li>Elastic Fabric Adapter (EFA), dedicated ENA for HPC, only for Linux. Improve inter-node communication, for tightly coupled workload. It leverages Message Passing Interface standard to bypass the Linux kernel to provide low-latency transport.</li> </ul> </li> <li> <p>Storage:</p> <ul> <li>Instance storage with EBS scale to 256k IOPS with io2 Block Express or instance store to scale to millions of IOPS.</li> <li>Network storage: S3, EFS and FSx for Lustre.</li> </ul> </li> <li> <p>Automate and orchestrate:</p> <ul> <li>AWS Batch to support multi-node parallel jobs. Easily schedule jobs and launch EC2 instance accordingly.</li> <li>ParallelCluster: open source project for cluster management, using infrastructure as code, and EFA.</li> </ul> </li> </ul>"},{"location":"infra/#simple-email-service-ses","title":"Simple Email Service - SES","text":"<p>Fully managed service to send and receive emails. Used to send email from applications using API, SMTP or AWS console.</p>"},{"location":"infra/#amazon-pinpoint","title":"Amazon PinPoint","text":"<p>Marketing communication services, SMS, email, push, voice, and in-app messaging. It supports customer segmentation and personalized messages. </p>"},{"location":"infra/#aws-systems-manager","title":"AWS Systems Manager","text":"<p>Systems Manager is a collection of capabilities to help manage the applications and infrastructure. Systems Manager simplifies application and resource management, shortens the time to detect and resolve operational problems, and helps SREs manage the AWS resources securely at scale.</p> <p>With System Manager developers can remote access to EC2 instances.</p> <p></p> <p>It uses activation code and activationID which are similar to ec2 access key and secret key. Each instance runs a SSM Agent (by default in Ubuntu and Linux 2), installed by the user-data script as :</p> <pre><code>sudo yum update -Y\nmkdir /tmp/ssm\ncurl https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm -o /tmp/ssm/amazon-ssm-agent.rpm\nsudo yum install -y /tmp/ssm/amazon-ssm-agent.rpm\nsudo stop amazon-ssm-agent\nsudo amazon-ssm-agent -register -code \"activation-code\" -id \"activation-id\" -region us-east-1\nsudo start amazon-ssm-agent\n</code></pre> <p>The IAM roles with the IAM policy: <code>AmazonSSMManagedInstanceCore</code> enables an EC2 instance to use AWS Systems Manager service core functionality. </p> <p>Ability to control AWS infrastructure like EC2, Amazon Relational Database Service (RDS), Amazon Elastic Container Service (ECS), and Amazon Elastic Kubernetes Service (EKS) instances,  with an unified user's experience. It includes a set of services like Session Manager, Patch Manager, Run Commands, Inventory or define maintenance windows.</p> <p>A command is a script, and can be launched on multiple instances. CloudTrail is used to audit commands.</p> <p>A managed instance is any machine configured for AWS Systems Manager. It supports machines on-premises. With System manager there is no need to use SSH, it uses the session manager, which helps to trace all actions done on the EC2 system.</p> <p>Automation is the motivation to simplify maintenance and deployment tasks of EC2 instances, like automating runbooks.</p> <p>It includes, <code>Parameter Store</code> which provides secure, hierarchical storage for configuration data and secrets management. You can store data such as passwords, database strings, Amazon Elastic Compute Cloud (Amazon EC2) instance IDs and Amazon Machine Image (AMI) IDs, and license codes as parameter values. Parameter Store is also integrated with Secrets Manager.</p> <p>Path manager uses patch baseline, patch groups defined on tags, maintenance windows and a rate control to address concurrency and error threshold. </p> <p>It is free service.</p> <ul> <li>Infrastructure Identity in AWS workshop to use Systems Manager and IAM.</li> </ul>"},{"location":"infra/#aws-health","title":"AWS Health","text":"<p>AWS Health provides ongoing visibility into our resource performance and the availability of our AWS services and accounts. </p> <p>Can be used as a way to automate the start and stop of the Amazon EC2 instance: set up an Amazon EventBridge rule that is triggered by the AWS Health event. Target a Lambda function to parse the incoming event and reference the Amazon EC2 instance, ID included. Have the function perform a stop and start of the instance.</p> <p>.</p>"},{"location":"infra/#aws-application-migration-service","title":"AWS Application Migration Service","text":"<p>AWS MGN is the primary migration service recommended for lift-and-shift migrations to AWS without having to make any changes to the applications, the architecture, or the migrated servers.</p> <p>Implementation begins by installing the AWS Replication Agent on the source servers. When launch Test or Cutover instances are launched, AWS Application Migration Service automatically converts the source servers to boot and run natively on AWS.</p> <p>Can be used to migrate Amazon Elastic Compute Cloud (EC2) workloads across AWS Regions, Availability Zones, or accounts.</p>"},{"location":"infra/networking/","title":"Networking","text":"<p>All regions are interconnected via private AWS fiber links. This drives better availability, higher performance, lower jitter and reduced costs. Each region has redundant paths to transit centers, which connect via private links to other AWS regions, to AWS Direct Connect customers' data centers, to the internet via peering and paid transit. The connections between AZs is a metro area over DWDM (Dense wavelength division multiplexing) links. 82k fibers in a region. single digit millisecond latency. 25Tbps peak inter AZs traffic. </p> <p>IPv4 allows 3.7 billions of different addresses. Private IP @ is for private network connections. Internet gateway has public and private connections. Public IP can be geo-located. When connected to an EC2, the linux prompt lists the private IP (<code>ec2-user@ip-172-31-18-48</code>). Private IP stays stable on instance restart, while public may change.</p> <p>With Elastic IP address, we can mask an EC2 instance failure by rapidly remapping the address to another instance. The mapping is manual. So better to use DNS. Elastic IP is a public IPv4 that users own as long as they want.  EIP can be attached to only one EC2 instance at a time. It is not free.</p>"},{"location":"infra/networking/#virtual-private-cloud","title":"Virtual Private Cloud","text":"<p>A virtual private cloud (VPC) is a virtual network dedicated to one AWS account. All new accounts have a default VPC. It is logically isolated from other virtual networks in the AWS Cloud. Account owner can launch AWS resources, such as Amazon EC2 instances, within the VPC. New EC2 instances are launched into the default VPC if no subnet is specified.</p> <p>When defining new VPC, user needs to specify CIDR, subnets, associate security groups (firewall type of rules), Access Control List, and configure route tables.</p> <p></p> <p>Figure 1: VPC</p> <p>By default, AWS creates a VPC with default public subnets, one per AZs, they are public because the main route table includes a route to the Internet Gateway. Internet gateway is a managed service with auto scaling, redundancy and highly availability. It is attached to a VPC in get internet traffic.</p> <p></p> <p>VPC Helps to:</p> <ul> <li>Assign static IP addresses, potentially multiple addresses for the same EC2 instance.</li> <li>Support changing security group membership for the instances while they're running.</li> <li>Control the outbound traffic from the instances (egress filtering) in addition to controlling the inbound traffic to them (ingress filtering).</li> <li>Users can have multiple VPCs per region (max to 5 but this is a soft limit). 5 maximum CIDRs per VPC.</li> <li>The IP ranges are in 10.x.x.x, 172, or 192 base and a min /28 to a max /16.</li> </ul> <p>Network Access Control List can be defined at the VPC level, so will be shared between subnets. The default network ACL is configured to allow all traffic to flow in and out of the subnets which it is associated to. Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied. </p> <p>The following diagram illustrates classical VPC, with one vpc, two availability zones, two public subnets with EC2 instances within those subnets. An internet gateway is connected to a router. Subnet is defined per availability zone. It defines an IP CIDR range: there should have less IP@ on public subnets as they are used for ELB and very few public facing resources. Most of the service instances should be in private subnets.</p> <p></p> <p>Figure 2: EC2s in public subnets</p> <ul> <li>A subnet is assigned a /24 CIDR block, which means 8 bits encoding (8=32-24) for a total of 256 = 2^8 addresses. But AWS uses 5 IP addresses in each subnet for gateway, LB,... so the number of available addresses is 256 - 5 = 251. To identify a single 32 bit IPv4 address, we can use /32 CIDR convention </li> <li>Users need to define at least one Route Table for the public subnets and one for the private subnets.</li> <li>Non-default subnet has a private IPv4 address, but no public IPv4 address.</li> <li>SRE can make a default subnet into a private subnet by removing the 0.0.0.0/0 route to the Internet Gateway. The route table in figure above defines such route, therefore the subnets are public.</li> <li>EC2 Instances should have either public IP or private IP and attached to the subnet they belong to. Subnet needs a route to the internet gateway. The figure 5 above, illustrates a route going to any IP @ (0.0.0.0/0) via the internet gateway (<code>igw-id</code>). Any host in the private network 172.31.0.0/16 can communicate with other hosts in the local network.</li> <li>Route table defines <code>172.31</code> as local with <code>/16</code> CIDR address range, for internal traffic within the VPC.</li> <li>Because the VPC is cross AZs, we need a router to route between subnets. (See TCP/IP summary)</li> </ul>"},{"location":"infra/networking/#elastic-network-interfaces","title":"Elastic Network Interfaces","text":"<p>ENI is a logical component in a VPC that represents a virtual network card. It has the following attributes:</p> <ul> <li>One primary private IPv4, one or more secondary IPv4.</li> <li>One Elastic IP (IPv4) per private IPv4.</li> <li>One Public IPv4 address.</li> <li>One or more security groups.</li> <li>A MAC address.</li> <li>It can be created independently and attach them on the fly (move them) on EC2 instances during failover. It is also attached to any elastic load balancer. ENIs are defined at the account level.</li> <li>Bound to a specific availability zone (AZ), we cannot attach ENI to an EC2 instance in a different AZ. </li> </ul> <p>New ENI doc.</p>"},{"location":"infra/networking/#routing-tables","title":"Routing Tables","text":"<p>As illustrated in the following diagram, the main routing table defines routes for VPC internal traffic, while custom tables define how inbound and outbound traffic can be structured within a subnet.</p> <p></p> <p>Figure 3: Routing tables</p> <p>EC2 instances in subnet 2 are in private subnet. The corporate network, or on-premises network, can be connected to the VPC via customer gateway deployed on-premises, vitual private GTW in AWS region, and at least one VPN connection between the two gateways.</p>"},{"location":"infra/networking/#nat-gateway","title":"NAT Gateway","text":"<p>Use a NAT gateway so that EC2 instances in a private subnet can connect to services outside the VPC but external services cannot initiate a connection with those instances. This is a managed service with auto-scaling capability within one AZ. The bandwidth is from 5 Gbps to  automatic scale up 45Gbps.</p> <p>Charged for each hour the NAT gateway is available and each Gigabyte of data that it processes.</p> <p>It is created in a specified AZ, public subnet, and uses an Elastic IP and can only be used by EC2 in other subnets. The route is from the private subnet to the NATGW to the IGW. To get HA, configure one NATG per AZ.</p>"},{"location":"infra/networking/#network-acls","title":"Network ACLs","text":"<p>Defines traffic rule at the subnet level. One NACL per subnet. A NACL specifies rules with number that defines evaluation priority. The last rule is an asterisk and denies a request in case of no rule conditions match.  As soon as a rule matches traffic, it\u2019s applied immediately regardless of any higher-numbered rule that may contradict it.</p> <p></p> <p>Figure 4: Default NACL </p> <p>It is used to block a specific external IP address.</p> <p>The VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. By default, each custom network ACL denies all inbound and outbound traffic until we add rules.</p> <p>Below is a complete figure to explain the process: A web server is initiating a connection to a DB on port 3306 and get response to an ephemeral port (allocated from 1024 to 65535):</p> <p></p> <p>Figure 5: Connection flow between public to private Apps</p> <p>Each subnet in the VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p> Security Group Network ACL Acts as firewall for EC2 Acts as firewall for subnet Controls inbound &amp; outbound traffic at the instance level Controls inbound &amp; outbound traffic at subnet level Supports allow rules Supports deny and allow rules Evaluates all rules before deciding to allow traffic Evaluate rules in order Instances associated with a SG can't talk to each other unless we add a rule to allow it Each subnet must be associated with a NACL."},{"location":"infra/networking/#vpc-peering","title":"VPC peering","text":"<p>The goal of VPC peering is to connect two VPCs using AWS network and let them behave as if they were in the same network. They also work for inter-region and cross-account interconnections.</p> <p>When defining the VPC, the CIDRs should not overlap for IPv4. There is no transitive VPC peering. And there is no edge to edge routing, which means a peered VPC could no go to on-premise server from another VPC, or could not use the internet gateway of another VPC to access internet.</p> <p>Once the VPC peering connection is defined, we still need to specify the routes to the CIDR to reach the peered VPC. VPC uses the longest prefix match to select the specific route. For example, from a VPC_C, the route to 10.0.0.77/32 will go to VPCa-instance77 while a route with 10.0.0.0/16 will go to any instances in the VPC b. Vpc_a and Vpc_b has overlapping CIDR but are not peered together, only to VPC_C.</p> <p>VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as EC2 instances, RDS databases, Redshift clusters, and Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same Organization from AWS Organizations.</p> <p>After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or to the VPC owner.</p>"},{"location":"infra/networking/#transit-gateway","title":"Transit Gateway","text":"<p>A transit gateway (TGW) is a network transit hub that interconnects attachments (VPCs and VPNs) within the same AWS account or across AWS accounts using Resource Access Manager.</p> <p>This service provides transitive peering between thousands of VPCs and even on-premises servers. It could support hub and spoke connections. It runs on the AWS global private network.</p> <p>Transit Gateway acts as a highly scalable cloud router: each new connection is made only once.</p> <p>It is a regional resource but it can run cross-region. It works with Direct Connect gateway and VPN connections.</p> <p></p> <p>Figure 6: TGW attachment to Direct Connect Gateway</p> <p>Control the connection vias Route Tables. This is also the only service supporting IP multicast. It addresses the use case of edge to edge routing, so instances in a VPC can access NAT gateway, NLB, PrivateLink, and EFS in other VPCs attached to the transit gateway.</p> <p>Transit Gateway can be peered together as intra-region peering mesh, and as inter-region peering too.</p> <p></p> <p>Figure 7: Peering mesh</p> <p>To increase the bandwidth of the connection to VPC, there is the site to site VPN ECMP (Equal-cost multi-path routing) feature, which is a routing strategy to forward packet over multiple best paths. It uses double tunnels. The VPCs can be from different accounts.</p> <p></p> <p>Figure 8: Transit Gateway between VPCs and On-premises</p> <p>A transit gateway connects VPCs and on-premises networks through a central hub. An attachment between the transit gateway and a new Direct Connect gateway will extend hybrid connectivity to any VPCs that are associated with the Direct Connect gateway.</p> <ul> <li>Centralized router use case</li> <li>Isolated VPCs with shared services.</li> </ul>"},{"location":"infra/networking/#vpc-endpoint","title":"VPC Endpoint","text":"<p>An interface VPC endpoint allows to privately connect one Amazon VPC to the supported AWS services without going over the internet. Interface VPC endpoints also connect to endpoint services hosted by other AWS accounts and to AWS Marketplace partner services.</p> <p>Two types of endpoint:</p> <ul> <li>Interface endpoints powered by PrivateLink: it provisions an ENI in the VPC, with a security group. Pay per hour and GB of data transferred. It can access AWS services such as Simple Queue Service (SQS), Simple Notification Service (SNS), Amazon Kinesis (all except DynamoDB). Can be accessed with site-to-site VPN and Direct Connect. It has a private domain name.</li> <li>Gateway endpoints: provision a GTW and setup routes in route tables. Used for S3 and DynamoDB only. Free. Cannot be extended out of the VPC. One per VPC.</li> </ul> <p></p> <p>Figure 9: VPC endpoints</p> <p>VPC Endpoints remove the need of IGW, NATGW to access AWS Services. The service is redundant and scale horizontally.</p> <p>VPC endpoints are defined in Console, SDK API or CDK.</p> <p></p> <p>Figure 10: VPC endpoints</p> <p>The VPC needs to enable DNS resolution. The same public hostname for S3 can be used, but once the route in the routing table defines the endpoint to be the target to S3 then the traffic will be over AWS private network.</p>"},{"location":"infra/networking/#vpc-endpoint-policies","title":"VPC Endpoint policies","text":"<p>Endpoint Policies are JSON documents to control access to services, controlled at the endpoint level. It does not replace IAM user policies or service specific policies.</p> <p>A endpoint policy can limit access to a SQS queue for a given user or role, then this user can still access the queue from outside of the VPC endpoint. To avoid that the SQS queue policy needs to deny any action not done through the VPC endpoint.</p> How to debug an access to S3 from EC2 in private subnet? <p></p> <ol> <li>Verify the security group has outbound rule to get traffic out</li> <li>Verify the VPC endpoint gateway has a policy to allow EC2 connection</li> <li>Be sure a route exists in VPC route table to the S3 bucket to go to the endpoint gateway</li> <li>Verify DNS resolution is enabled in VPC</li> <li>Verify the s3 bucket policy allows the EC2 access</li> <li>Verify the IAM role used by the EC2 has access to the bucket.</li> </ol> <p>See this lab with CDK to set this up.</p>"},{"location":"infra/networking/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>Capture IP traffic at the ENI, VPC, subnet level. This is used to monitor and troubleshoot connectivity issues. Can be saved in S3 and CloudWatch logs. It can work on managed service like ELB, RDS, ElastiCache, RedShift, NATGW, Transit Gateway.</p> <p>The log includes src and destination addresses, port number and action done (REJECT/ ACCEPT).</p> <p>When an inboud request is rejected it could be a NACL or SG issue, while if the inbound is accepted but outbound rejected, it is only a NACL issue, as Security Groups are stateful, meaning an accepted inbound connection, makes the outbound also accepted.  </p> <p>VPC Flow log is defined within the VPC:</p> <p></p> <p>Figure 11: VPC flow logs</p> <p>The Flow can target S3, Kinesis FireHose or CloudWatch</p> <p></p> <p>Figure 12: VPC flow definition to CloudWatch</p> <p>VPC Flow logs are sources for GuardDuty.</p>"},{"location":"infra/networking/#bastion-host","title":"Bastion Host","text":"<p>The goal is to be able to access any EC2 instances running in the private subnets from outside of the VPC, using SSH. The bastion is running on public subnet, and then connected to the private subnets. </p> <ul> <li> <p>To use a Bastion Host, we attach a security group, to the EC2, to authorize inbound SSH and outbound HTTP traffic. CDK creates this SG automatically. So we can use Instance Connect to this instance, and within the terminal a ping to amazon.com will work. The bastion has a public IP address, and the VPC has a IGW with a route table.</p> <p></p> <p>Figure 13: Security group for bastion host authorize public access to port 22</p> </li> <li> <p>In the EC2 instance running in the private network, we need to add a Security Group with an inbound rule to specify SSH traffic from the SG of the Bastion. With this settings a SSH to the Bastion, then a copy of the target EC2 pem file in the bastion host and a command like: <code>ssh ec2-user@10.10.2.239 -i ec2.pem</code> on the private IP @ of the EC2 instance (10.10.2.239) will make the connection from Bastion to EC2.</p> <p></p> <p>Figure 14: Security group for EC2 to accept port 22 from Bastion only</p> </li> </ul> <p>The security group for the Bastion Host authorizies inbound on port 22 from restricted public CIDR. Security group of the EC2 instance allows the SG of the bastion host to accept connection on port 22.</p>"},{"location":"infra/networking/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>VPC FAQs.</li> <li>NAT gateway.</li> </ul>"},{"location":"infra/networking/#hybrid-networking","title":"Hybrid networking","text":""},{"location":"infra/networking/#client-vpn","title":"Client VPN","text":"<p>Managed client-based VPN service to securaly connect to AWS resources.</p> <p>How it works</p>"},{"location":"infra/networking/#site-to-site-vpn","title":"Site-to-Site VPN","text":"<p>Connect on-premises network to a VPC, and support IPsec. It uses a customer gateway device or software app to support the site to site connection. It is connected to the target gateway on Amazon side (a virtual private gateway).</p>"},{"location":"infra/networking/#direct-connect","title":"Direct Connect","text":"<p>It provides a dedicated connection, over fiver optic cable, from a remote network to the VPC bypassing public internet. The main components are the connections (to a region) and the virtual interfaces to AWS services. 1, 10, 100 Gbps ethernet connection. Supports BGP. \\Install a direct connection setup can take more than a month, and it involves AWS Direct Connect Partners. Direct Connect provides three types of virtual interfaces: public, private, and transit:</p> <ul> <li>Public virtual interface: To connect to AWS resources (like S3, EC2) that are reachable by a public IP address.</li> <li>Private virtual interface: To connect to resources hosted in a VPC using their private IP addresses.</li> <li>Transit virtual interface: To connect to resources hosted in VPC through a transit gateway. </li> </ul> <p>To connect to a VPC in the same region, we need to setup a Virtual Private GTW. It is a private connection so it supports better bandwidth throughput at lower cost. It is not encrypted by default.</p> <p></p> <p>Figure 15: Direct Connect between on-premise and VPC</p> <p>If we need to access two VPCs in two different regions from the corporate data center then we need a Direct Connect Gateway.</p> <p></p> <p>As another solution we can connect DC to a transit gateway through a central hub. An attachment between the transit gateway and a new Direct Connect gateway will extend hybrid connectivity to any VPCs that are associated with the Direct Connect gateway.</p> <p></p> <p>When we want to encrypt the traffic flowing through Direct Connect, we need VPN to provide an IPsec-encrypted private connection. VPN over DC will use the public virtual interface.</p> <p>To get reliability, at lower cost, we can setup a VPN site-to-site connection in parallel to the Direct Connect link. For maximum resiliency for critical workloads, it is  recommended to have double connections per data center.</p> <p></p> <p>Figure 16: Direct Connect HA</p> <p>The Link Aggregation Groups or LAG, helps to get increased speed and failover by aggregate up to 4 connections on active/active mode.</p> <p>Hosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps, while Dedicated offers higher bandwidth.</p> <p>The main pricing parameter while using the Direct Connect connection is the Data Transfer Out (DTO) from AWS to the on-premises data center, and the port hours. DTO refers to the cumulative network traffic that is sent through AWS Direct Connect to destinations outside of AWS. This is charged per gigabyte (GB), and unlike capacity measurements, DTO refers to the amount of data transferred, not the speed.</p>"},{"location":"infra/networking/#extended-picture","title":"Extended picture","text":"<p>The following animation is presenting external integration from on-premises servers to AWS services and VPCs via site to site VPN, Private Gateway, Customer Gateway.</p> <p></p> <p>Figure 17: Full VPC diagram</p> <ul> <li>We need to have VPC endpoint service to access the AWS services, like S3, privately as they are in our VPC. </li> <li>We need to ensure there is one interface endpoint for each availability zone. </li> <li>We need to pick a subnet in each AZ and add an interface endpoint to that subnet. </li> <li> <p>TCP traffic is isolated. It is part of a larger offering called AWS PrivateLink to establish private connectivity between VPCs and services hosted on AWS or on-premises, without exposing data to the internet (No internet gateway, no NAT, no public IP @).</p> </li> <li> <p>CIDR Blocks should not overlap between VPCs for setting up a peering connection. Peering connection is allowed within a region, across regions, across different accounts.</p> </li> <li> <p>We can optionally connect our VPC to our own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of our data center.\u00a0A VPN connection consists of a virtual private gateway (VGW) attached to our VPC and a customer gateway located in the data center. </p> </li> <li>A Virtual Private Gateway is the VPN concentrator on the Amazon side of the VPN connection. </li> <li> <p>A customer gateway is a physical device or software appliance on the on-premise side of the VPN connection. We need to create a Site-to-site VPN connection between the CP GTW and Customer GTW.</p> </li> <li> <p>As seen in Figure 18 \"Full VPC diagram\", the <code>VPC peering</code> helps to connect between VPCs in different region, or within the same region. And Transit GTW is used to interconnect our virtual private clouds (VPCs) and on-premises networks. In fact Transit Gateway is a more modern and easier approach to link VPCs. Using Transit Gateway route tables, We can control the traffic flow between VPCs. The peering connection would work; however, it requires a lot of point-to-point connections.</p> </li> <li>If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub. This enables our remote sites to communicate with each other, and not just with the VPC. </li> <li>AWS VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC.</li> </ul>"},{"location":"infra/networking/#elastic-load-balancers","title":"Elastic Load balancers","text":"<p>Routes traffic into the different EC2 instances, containers or any IP @ end points in one or more AZs. As a managed service, Elastic Load Balancing scales automatically in response to changes in incoming traffic. It is deployed per region.</p> <p>The goal is to improve availability and fault tolerance. It also exposes a single point of access (DNS) to the deployed applications.</p> <p>It uses health check (<code>/health</code> on the app called the <code>ping path</code>) to assess instance availability. It also provides SSL termination. It is used to separate private (internal) to public (external) traffic.</p> <p></p> <p>Figure 18: ELB</p> <p>When SREs create a load balancer, they must choose whether to make it an internal load balancer or an internet-facing load balancer, and select the availability zones to route traffic to.</p> <p>Internet-facing load balancers have public IP addresses. The DNS name of an internet-facing load balancer is publicly resolvable to the public IP addresses of the nodes. Internal load balancers have only private IP addresses.  They can only route requests from clients with access to the VPC of the load balancer.</p> <p></p> <p>Figure 19: Public and private ELBs</p> <p>For certain needs, it also supports stickiness cookie to route to the same EC2 instance. ELB has security group defined for HTTP and HTTPS traffic coming from the internet, and the EC2 security group defines HTTP traffic to the ELB only.</p>"},{"location":"infra/networking/#four-types-of-elb-supported","title":"Four types of ELB supported","text":"<ul> <li>Classic load balancer: older generation. TCP and HTTP layer. For each instance created, update the load balancer configuration so it can route the traffic.</li> <li> <p>Application load balancer: HTTP, HTTPS (layer 7), Web Socket. </p> <ul> <li>It specifies availability zones: it routes traffic to the targets within these Availability Zones. To increase availability, use at least two AZs.</li> <li>It uses target groups, to group applications.</li> <li>Routes on URL, hostname and query string.</li> <li>Gets a fixed hostname in DNS.</li> <li>The applications do not see the IP address of the client directly (ELB does a connection termination), but ELB puts client information in the header <code>X-Forwarded-For</code> (IP @), <code>X-Forwarded-Port</code> (port #) and <code>X-Forwarded-Proto</code> (protocol).</li> <li>Great for microservices or for container based apps (ECS).</li> <li>Support dynamic port mapping for ECS container.</li> <li>Support HTTP/2 and WebSocket.</li> </ul> </li> </ul> <p>Info</p> <p>Target group: groups EC2 instances by specifying a Auto Scaling Group. They may be tasks or containers in ECS, or lambda functions. Health check is done at the EC2 or container level. Application Load Balancers are the only load balancers that support the lambda target type.</p> <ul> <li> <p>Network load balancer: TCP, UDP (layer 4), TLS</p> <ul> <li>Handles millions request/s.</li> <li>Reaches less than 100ms latency while ALB is at 400ms.</li> <li>Uses one public static IP address per availability zone.</li> <li>Routes each individual TCP connection to a single target for the life of the connection. If we specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. If we specify targets using IP addresses, we can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port.</li> <li>Not free.</li> <li>Can reach target groups of EC2s, IP @, or ALB.</li> <li>Health check is based on TCP, HTTP, and HTTPS.</li> <li>For NLB we need to add a rule in the security group attached to the EC2 to get HTTP:80 to anywhere.</li> </ul> </li> <li> <p>Gateway LB: </p> <ul> <li>Used to analyze in traffic before routing to applications.</li> <li>Applies firewalls rules, intrusion detection, deep packet inspection.</li> <li>Works at layer 3: IP packet.</li> <li>Combine NLB and gateway service.</li> <li>Also use target groups.</li> <li>Use the Geneve protocol (support network virtualization use cases for data centers ) on port 6081 </li> </ul> </li> </ul> <p>To route traffic, first the DNS name of the load balancer is resolved. (They are part of the <code>amazonaws.com</code> domain). 1 to many IP Addresses are sent back to the client. With NLBs, Elastic Load Balancing creates a network interface for each Availability Zone that is enabled. Each load balancer node in the Availability Zone uses this network interface to get a static IP address. ELB scales the load balancer nodes and updates the DNS entry. The time to live is set to 60s.</p> <p>To control that only the load balancer is sending traffic to the application, we need to set up an application security group on HTTP, and HTTPS with the source being the security group id of the ELB. LBs can scale but need to engage AWS operational team.</p> <p>HTTP 503 means LB is at capacity or target app is not registered. Verify security group in case of no communication between LB and app.</p> <p>Target group defines the protocol to use, the health check parameters and what applications to reach (instance, IP or lambda). Below is an example of listener rule for an ALB:</p> <p></p> <p>Figure 23: ALB rule</p> <p>ALB and Classic can use HTTP connection multiplexing to keep one connection with the backend application. Connection multiplexing improves latency and reduces the load on our applications.</p>"},{"location":"infra/networking/#load-balancer-stickiness","title":"Load balancer stickiness","text":"<p>Used when the same client needs to interact with the same backend instance. A cookie, with expiration date, is used to identify the client. The classical LB or ALB manages the routing. This could lead to unbalance traffic so overloading one instance.  With ALB, stickness is configured in the target group properties.</p> <p>Two types of cookie:</p> <ul> <li>Application-based cookie: generated by the target app. The cookie name is specific to the target group. </li> <li>Duration-based cookie: generated by the Load Balancer.</li> </ul> <p>The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).</p>"},{"location":"infra/networking/#cross-zone-load-balancing","title":"Cross Zone Load Balancing","text":"<p>Each load balancer instance distributes traffic evenly across all registered instances in all availability zones. </p> <p></p> <p>If one AZ has 2 targets and another one has 8 targets, then with cross-zone, the LBs in each availability zone will route to any instance, so each will receive 10% of the traffic. Without that, the 2 targets zone will receive 25% traffic each, and the instance on the othe AZ only 6.25% of the traffic. This is the default setting for ALB and free of charge. It is disabled by default for NLB.</p>"},{"location":"infra/networking/#tls-transport-layer-security","title":"TLS - Transport Layer Security,","text":"<p>An SSL/TLS Certificate allows traffic between clients and load balancer to be encrypted in transit (in-flight encryption).</p> <ul> <li>Load balancer uses an X.509 certificate (SSL/TLS server certificate).</li> <li>Manage our own certificates using ACM (AWS Certificate Manager).</li> <li>When defining a HTTPS listener in a LB, we must specify a default certificate for the HTTPS protocol, while defining the routing rule to a given target group. Need multiple certs to support multiple domains. </li> <li>Clients can use SNI (Server Name Indication) to specify the hostname they want to reach. The ALB or NLB will get the certificates for each host to support the TLS handshake.</li> </ul>"},{"location":"infra/networking/#connection-draining","title":"Connection draining","text":"<p>This is a setting to control connection timeout and reconnect when an instance is not responding. It is to set up the time to complete \u201cin-flight requests\u201d. When an instance is \"draining\", ELB stops sending new requests to the instance. The time out can be adjusted, depending of the application, from 1 to 3600 seconds, default is 300 seconds, or disabled (set value to 0).</p> <p>It is called <code>Deregistration Delay</code> in NLB &amp; ALB.</p>"},{"location":"infra/networking/#deeper-dive_1","title":"Deeper dive","text":"<ul> <li>Product documentation</li> <li>Networking Immersion Day - lvl 300</li> </ul>"},{"location":"infra/networking/#cloudfront","title":"CloudFront","text":"<p>Content Delivery Network service with DDoS protection. It caches data to the edge to improve web browsing and application performance. 410+ Edge locations. This is a global service. </p> <p>When configuring we need to specify the origin for the data which could be S3 objects, or Custom Origin resource accessible via HTTP (ALB, EC2...), the caching behavior, and the edge distribution.</p> <p>CloudFront keeps cache for the data read. For the edge to access the S3 bucket, it uses an origin access identity (OAI), managed as IAM role.</p> <p>For EC2 instance, the security group needs to accept traffic from edge location IP addresses.</p> <p>It is possible to control with geographic restriction using whitelist or blacklist.</p> <p>It also supports the concept of signed URL. When we want to distribute content to different user groups over the world, attach a policy with:</p> <ul> <li>URL expiration.</li> <li>IP range to access the data from.</li> <li>Trusted signers (which AWS accounts can create signed URLs).</li> <li>How long should the URL be valid for?</li> <li>Shared content (movie, music): make it short (a few minutes).</li> <li>Private content (private to the user): we can make it last for years.</li> <li>Signed URL = access to individual files (one signed URL per file).</li> <li>Signed Cookies = access to multiple files (one signed cookie for many files).</li> </ul> <p>When the backend content is modified, CloudFront will not get it until its TTL has expired. But we can force an entire cache refresh with CloudFront Invalidation.</p> <p>CloudFront supports HTTP/RTMP (streaming distribution from Adobe) protocol based requests only.</p> <p>The cost is based on the number of HTTP requests, and data transfer. </p> <p>Lambda@Edge is a feature of Amazon CloudFront that lets us run code closer to users of the application, which improves performance and reduces latency. Lambda@Edge runs code in response to events generated by the Amazon CloudFront content delivery network (CDN).</p> <ul> <li>CloudFront FAQs</li> <li>CloudFront Pricing</li> </ul>"},{"location":"infra/networking/#global-accelerator","title":"Global Accelerator","text":"<p>AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of the internet applications. </p> <p>It is a global service, and it provides two static anycast IP addresses that act as a fixed entry point to the application endpoints in a single or multiple AWS Regions, such as the Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions.</p> <p>A listener is a process that checks for connection requests that arrive to an assigned set of static IP addresses on a port or port range. It routes traffic to one or more endpoint groups. An endpoint group includes endpoints, such as load balancers.</p> <p>The goal is to expose quickly an application to the WW. The problem is the number of internet hops done to access the target public ALB. The solution is to get as fast as possible to a AWS global network endpoint (Edge location), nearest region to the client. It is a global service.</p> <p>With Anycast IP, a client is routed to the nearest server. All servers hold the same IP address. So for each application, we create 2 Anycast IPs, and the traffic is sent to the edge location.</p> <p>AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group. We may use it to do blue-green deployments.</p> <p>Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP.</p>"},{"location":"infra/networking/#vpn-cloudhub","title":"VPN CloudHub","text":"<p>If we have multiple AWS Site-to-Site VPN connections, we can provide secure communication between sites using the AWS VPN CloudHub. </p> <p>This enables our remote sites to communicate with each other, and not just with the VPC. </p> <p>Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. </p> <p>The VPN CloudHub operates on a simple hub-and-spoke model that we can use with or without a VPC. </p> <p>This design is suitable if we have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.</p>"},{"location":"infra/networking/#faqs","title":"FAQs","text":"What differentiates a public subnet from a private subnet? <p>A public subnet is a subnet that's associated with a route table that has a route to an internet gateway</p>"},{"location":"infra/networking/#traffic-mirroring","title":"Traffic mirroring","text":"<ul> <li>Copies inbound and outbound traffic from the network interfaces attached to EC2 instances.</li> <li>Used to mirror inbound TCP traffic to a single monitoring appliance.</li> <li>Some open source tools to be used as monitoring: Zeek, or Suricata.io</li> </ul> <ul> <li> <p>Classical steps to get started:</p> <ul> <li>Step 1: Create the traffic mirror target</li> <li>Step 2: Create the traffic mirror filter</li> <li>Step 3: Create the traffic mirror session</li> <li>Step 4: Analyze the data</li> </ul> </li> </ul>"},{"location":"infra/networking/#cloud-wan","title":"Cloud WAN","text":"<p>The goal is to build a global network to interconnect VPCs via transit gateways oand other Cloud WAN, on-premises environments, using a single central dashboard. It uses the concept of core network policy to define what we want the network to be.</p> <p>AWS Cloud WAN and AWS Transit Gateway migration and interoperability patterns</p>"},{"location":"infra/route53/","title":"Route 53","text":"<p>A highly available, scalable, fully managed, and authoritative (we can update public DNS records) DNS. It is also a Domain Registra and a health checking services to route traffic to healthy endpoints. It supports a lot of routing types to respond to DNS query by taking into account % of traffic, latency, health checks...</p> <p>This is a global WW service, which is a globally distributed anycast network (networking and routing technology to get DNS query answered from the most optimal server) of DNS servers, deployed around the world.</p> <p>When registering a domain to a registrar like Godaddy or Squarespace, at least 4 domain nameservers are defined to do name resolution.</p> <p>It uses the concept of <code>hosted zone</code> which is a \"container\" that holds information about how we want to route traffic for a single parent domain or subdomain. The zone can be public (internet facing) or private (inside a VPC). All resource record sets within a hosted zone must have the hosted zone\u2019s domain name as a suffix. </p> <p></p> <p>Each Amazon Route 53 hosted zone is served by its own set of virtual DNS servers.</p> <p></p> <p>A domain is at least 12$ a year and Route 53 fees is $0.5 per month per hosted zone. See Pricing page, but users pay as they go and only for what it is used: monthly charge for each hosted zone managed, charges for every DNS query answered by the Route 53.</p>"},{"location":"infra/route53/#dns","title":"DNS","text":"<p>DNS is a collection of rules and records which helps client apps understand how to reach a server through URLs. Below figure presents the DNS name resolution process, which in fact, should also has the Top Level Domain server (for <code>com</code> or <code>app</code>). The Second Level Domain server (amazon, google, ibm) is the one presented in the figure below.</p> <p></p>"},{"location":"infra/route53/#route-53-faqs","title":"Route 53 FAQs","text":""},{"location":"infra/route53/#dnssec","title":"DNSSEC","text":"<p>Is a protocol to secure DNS traffic, to verify DNS data integrity and origin. It protects for man in the middle (MITM) attacks. Route 53 supports DNSSEC for domain registration and signing. It works only for public hosted zone.</p>"},{"location":"infra/route53/#records","title":"Records","text":"<p>Record defines how to route traffic for a domain. Each record contains:</p> <ul> <li>a Domain name.</li> <li>a record type A (maps to IPv4) or AAAA(for IPv6), CNAME (canonical name of a hostname), NS (name server, for hosted zone).</li> <li>a value.</li> <li>a routing policy.</li> <li>a Time to Live (TTL): is set, on the client side, to get the web browser to keep the DNS resolution in cache. High TTL is around 24 hours, low TTL, at 60s, which leads to make more DNS calls. TTL should be set to strike a balance between how long the value should be cached vs how much pressure should go on the DNS server. Need to define the TTL for the app depending on the expected deployment model.</li> </ul>"},{"location":"infra/route53/#cname-vs-alias","title":"CNAME vs Alias","text":"<p>CNAME is a DNS record to maps one domain name to another. CNAME should point to a ALB. Works on non root domain. We could not create a CNAME record for the top node of a DNS namespace (No APEX domain).</p> <p>Any public facing AWS resources expose an AWS DNS name: <code>.....us-west-2.elb.amazonaws.com</code> for example that can be mapped using CNAME.</p> <p>Alias is used to point a hostname of an AWS resource only, and can work on root domain (domainname.com). The alias record target could be: ELB, CloudFront distributions, API gateway, Elastic Beanstalk environments, S3 static Websites, VPC Interface endpoints, global accelerator, route 53 record (in the same hosted zone). EC2 DNS name could not be a target of alias.</p> <p>Use <code>dig &lt;hostname&gt;</code> to get the DNS resolution record from a Mac or for linux EC2 do the following.</p> <pre><code>sudo yum install -y bind-utils\nnslookup domainname.com\n# or\ndig domainname.com\n</code></pre>"},{"location":"infra/route53/#demonstration","title":"Demonstration","text":"<ul> <li>Create three EC2 t2.micro instances, in different AZs / regions, using security group to authorize SSH and HTTP from anywhere, with the following <code>User data</code> script, so the returned HTML page includes the AZ name:</li> </ul> <pre><code>#!/bin/bash\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\nEC2_AVAIL_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)\necho \"&lt;h1&gt;Hello World from $(hostname -f) in AZ $EC2_AVAIL_ZONE &lt;/h1&gt;\" &gt; /var/www/html/index.html\n</code></pre> <ul> <li>Add at least, one ALB to route to instances in a region to different AZs</li> <li>Define a domain name (5$ to 12$ a year)</li> <li>Create a hosted zone in Route 53.</li> <li>Define DNS records in Route 53 for each ALB and EC2 IP @ based on a subdomain name, using one of the type as specified in next section.</li> </ul>"},{"location":"infra/route53/#routing-policies","title":"Routing policies","text":"<p>It defines how Route 53 responds to DNS queries. It can be used to apply A/B testing, or looking query.</p> <p>Eight routing types:</p> <ol> <li>A simple routing policy to get an IP @ from a single resource (still can specify multiple IP@ to be returned in the response, and the client will pick one of the address randomly). There is no health check associated to this record.</li> <li>The weighted routing policy controls the % of the requests that go to specific endpoint. Can do blue-green traffic management. It can also help to split traffic between two regions. It can be associated with Health Checks</li> <li>The latency routing Policy redirects to the server that has the least latency close to the client. Latency is based on traffic between users to AWS Regions.</li> <li>Health check monitors the health and performance of the public resources and assesses DNS failure with automatic failover. We can have HTTP, TCP or HTTPS health checks. We can define from which region to run the health check. They are charged per HC / month. 15 Health checkers exist WW. Send every 30s. Need at least 18% health checkers to report the endpoint is healthy. HTTP RC code 2xx or 3xx. It is recommended to have one HC per app deployment. It can also monitor latency. To assess private endpoint within a VPC, we need to add a CloudWatch metric and alarm, then create a Health Check to the alarm itself.</li> <li>The failover routing policy helps us to specify a record set to point to a primary and then a secondary instance for DR purpose.</li> <li>The Geo Location routing policy is based on user's location, and we may specify how the traffic from a given country should go to a specific IP@. Need to define a \u201cdefault\u201d policy in case there\u2019s no match on location. It is interesting for website localization, restrict content distribution, load balancing,...</li> <li>Geoproximity takes into account the user and AWS resources locations. It also supports shifting more traffic to resources based on the defined bias. It is part of Route 53 Traffic Flow feature.</li> <li>The Multi Value routing policy is used to access multiple resources. The record set, associates a Route 53 health checks with the records. The client on DNS request gets up to 8 healthy records returned for each Multi Value query. If one fails then the client can try one other IP @ from the list.</li> <li>IP origin based routing. Use clients IP @ to define destination location. Can be useful to isolate traffic from one ISP.</li> </ol>"},{"location":"infra/route53/#health-checks","title":"Health checks","text":"<p>We can use Route 53 health checking to configure active-active and active-passive failover configurations. We configure active-active failover using any routing policy (or combination of routing policies) other than failover, and we configure active-passive failover using the failover routing policy.</p> <p>In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. With Active-Active Failover, it uses all available resources all the time without a primary nor a secondary resource.</p> <p>If we used another domain registra, it is possible to get the list of AWS DNS servers associated to the hosted public zone, and then configure the registra for the NS records to go to those DNS servers.</p> <p>Health checks can monitor CloudWatch alarms, for example, alarm on RDS, or any custom metrics. It is the only way to monitor internal to VPC resources as health checks are outside the VPC and control public end-points. </p>"},{"location":"infra/route53/#hybrid-dns","title":"Hybrid DNS","text":"<p>By default, Route 53 Resolver automatically answers DNS queries for local domain names for EC2 instances, or records in Private Hosted Zones or records in public Name Servers.</p> <p>Hybrid DNS helps to resolve DNS queries between VPCs, and on-premises network:</p> <p></p> <ul> <li><code>aws.priv</code> is the domain in private VPC.</li> <li>Inbound resolver endpoints forward DNS queries to Route 53 resolver.</li> <li>The DNS records for resources in private subnets are in the route 53 private hosted zones.</li> <li>Outbound resolver endpoints are for Route 53 resolver to forward (with  conditional rules) the DNS queries to an on-premises DNS resolvers.</li> <li>Resolver endpoints are associated with one or more VPCs in the same Region. They are HA when used on 2 AZs.</li> <li>Each resolver endpoints have different ENIs, one in each AZ. The outboud ENIs are not represented in the figure above (they should have an IP of 10.0.0.20 and 10.0.1.20 for example).</li> <li>For <code>app.aws.priv</code> query, the resolver inbound endpoint is linked to the Route 53, which lookup in the private hosted zone to get IP address of the EC2 instance.</li> <li>Each Endpoint supports 10,000 queries per second per IP address.</li> <li>Direct connect or VPN is set up to do the network connection.</li> <li>Forward rules can be shared between AWS accounts via RAM.</li> </ul>"},{"location":"infra/route53/#route-53-arc","title":"Route 53 ARC","text":"<p>For multi-site, the Amazon Route 53 Application Recovery Controller helps SREs to assess when applications are ready for recovery.</p> <ul> <li>Zonal shift enable fast recovery from an AZ failure by moving the traffic to other resources. This is a multi-AZ recovery. Usable on any Route 53 registered resources within a region. Zonal shift is triggered by SRE and has an expiration date. ALB or NLB should not use cross-zone load balancing.</li> <li>Readiness check to monitor resource quotas, capacity, and network routing policies, suggesting remediation when changes are made that would affect the ability to fail over to a standby replica.</li> <li>Routing controls are like on/off switches that control ARC health check. To re-balance traffic across application replicas during failures. This is a Multi-Region recovery. The routing control components in Route 53 ARC are: clusters, control panels, routing controls, and routing control health checks. Each cluster in Route 53 ARC is a data plane of endpoints in five AWS Regions.</li> </ul> <p>Resources are grouped into cells (in region) and nested cells (AZ) as recovery group. Cell represents a coherent set of components running for the application and can be replicated. Readiness check monitors a resource set: which include auditing configuration, quotas or routing policies. See product doc.</p> <p>For EC2 recovery we need to build a golden AMI and distributed in different regions.</p>"},{"location":"infra/route53/#resources","title":"Resources","text":"<ul> <li>Private hosted zone with different account</li> <li>Associate VPC with privated hosted zone</li> <li>Associate VPC from different accounts.</li> <li>Amazon Route 53 Dojo Cheat Sheet</li> </ul>"},{"location":"infra/security/","title":"Security","text":"<p>Info</p> <p>Created Oct/2022  - Updated 10/25/2023 - Between level 100 to 400.</p>"},{"location":"infra/security/#introduction","title":"Introduction","text":"<p>With the AWS Cloud, managing security and compliance is a shared responsibility between AWS and the customer:</p> <ul> <li>AWS is responsible of security of the cloud and offers the most flexible and secure cloud computing environment available today. AWS is responsible for patching their managed services and infrastructure security.</li> <li>Customers are responsible for the security in the cloud: secure workloads and applications that are deployed onto the cloud. When using EC2, we are responsible to patch OS for security (but AWS helps by providing patched AMIs, or tools such as Systems Manager, or Inspector for continuous vulnerability testing).</li> </ul> <p>AWS runs highly secured data centers. Multiple geographic regions and Availability Zones allow customers to remain resilient in the face of most failure modes, from system failure to natural disasters.</p> <p>For highly regulated industry, AWS helps by getting more than 50 certifications for the infrastructure, globally but also regionaly for specific countries. At global, these include the ISO 27001, SOC 1 and 2 certifications. For regional in the USA, AWS aligns with FISMA and FedRAMP, in UK with Cyber Essentials, in Australia with IRA... The audits are done by a 3nd party and we can find reports in aws/artifact.</p> <p>AWS Compliance Center is a central location to research cloud-related regulatory requirements</p> <p>Fine-grain identity and access controls combined with continuous monitoring for near real-time security information (CloudTrail) ensures that the right resources have the right access at all times, wherever the information is stored</p> <p>The following figure illustrates the AWS security domains and the services to support those domains:</p> <p></p> <p>Security scope needs to address all those domains, and leverage the four core security services: <code>Security Hub, GuardDuty, Shield Advanced, KMS</code>.</p>"},{"location":"infra/security/#1-identity","title":"1- Identity","text":""},{"location":"infra/security/#iam-identity-and-access-management","title":"IAM Identity and Access Management","text":"<ul> <li>Helps to control access to AWS services, for identities like users or other services, via console, CLI or SDK.</li> </ul> <ul> <li>IAM is a global service, defined at the account level, cross regions.</li> <li>IAM helps defining users (physical person), groups and roles, permissions (policies) and control access to Resources.</li> <li>An AWS account is not the same as a user. The users are defined in IAM for development or SRE needs. Identity Center users are also different.</li> </ul> <ul> <li>Resources are entities created in AWS (S3 bucket is a resource).</li> <li> <p>Users attempt to perform Actions on resources. Policy authorizes to perform action.</p> </li> <li> <p>Each account has a root user. Root user access should be set up to use Multi Factor Authentication (MFA) and use complex password. Do not delete root user. But do not use root user, create a user with administrator role.</p> </li> <li>Administrator users are part of an admin group with admin priviledges, like <code>AdministratorAccess</code> policy.</li> <li>Assign users to groups (<code>admin</code> and <code>developers</code>) and assign policies to groups and not to individual user.</li> <li>Groups can only contain users, not other groups.</li> <li>Users can belong to multiple groups.</li> <li>A classical use case: Create the user, create a group for each department, create and attach an appropriate policy to each group, and place each user into their department\u2019s group. When new team members are onboarded, create their user and put them in the appropriate group. If an existing team member changes departments, move their user to their new IAM group.</li> <li>AWS Account has a unique ID but can be set with an alias. The console URL includes the user alias.</li> <li>Principal: A person or application that can make a request for an action or operation on an AWS resource. Role sessions, IAM users, federated users, and root users are principals that make requests.</li> <li>Identity-based policies: Policies that are attached to an IAM user, group, or role. These policies let us specify what that identity can do (its permissions). Identity-based policies can be managed or inline(not shareable).</li> <li>Resource-based policies: Policies that are attached to a resource. Not all AWS resources authorize such policies.</li> <li>IAM Roles are used to get temporary credential, via STS, to do something on other services. EC2 has instance role to access remote services. Service roles is also used from service to service authorization, and finally Cross account roles are used to do action within another AWS Account.</li> </ul> Identity Center <p>This is another service to manage user for single sign on on applications deployed on AWS. This is in Identity Center that we define Organizations.</p>"},{"location":"infra/security/#multi-factor-authentication-mfa","title":"Multi Factor Authentication - MFA","text":"<ul> <li> <p>Multi Factor Authentication is used to verify a human is the real user of a service</p> <ul> <li>Always protect root account with MFA. </li> <li>MFA = password + device we own. </li> <li>The device could be a universal 2<sup>nd</sup> factor security key. (ubikey) </li> </ul> </li> <li> <p>Authy is a multi-device service with free mobile app. We can have multiple users on the same device.</p> </li> </ul>"},{"location":"infra/security/#security-policies","title":"Security Policies","text":"<ul> <li>Security Policies are written in JSON, and define permissions <code>Allow</code>, <code>Deny</code> for users or roles to perform action on AWS services, for an IAM identity such a user, user groups or role...</li> <li>AWS Managed policy (e.g. AdminstratorAccess policy) are predefined by AWS. </li> <li>Policies defined by customer (Customer managed)</li> <li>Inline policies assigned to one user or role and not shareable</li> <li> <p>Resource based policies for access resources like S3 bucket.</p> <p></p> </li> <li> <p>Policy applies to Principal: an account | user | role, list the actions (what is allowed or denied) on the given resources.</p> </li> <li> <p>It must define an ID, a version and one or more statement(s). The following policy authorizes to create any lambda function. (resource = '*' is to support any resource arn, so here any function).</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:CreateFunction\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\" : {}\n        }\n    ]\n}\n</code></pre> </li> </ul> <p>The condition part can reference specific principals or resource, and use the format:</p> <pre><code>\"Condition\" : { \"{condition-operator}\" : { \"{condition-key}\" : \"{condition-value}\" }}\n</code></pre> <ul> <li> <p>Another example to control access to IAM itself:</p> <p></p> </li> <li> <p>Use the <code>Least privilege permission</code> approach: Give users the minimal amount of permission they need, to do their job.</p> </li> <li>Developer may have the AWS managed policy name: <code>PowerUserAccess</code> to enable them to create and configure resources and services that support AWS aware application development.</li> <li>As soon as there is one <code>deny</code> in the chain of policy evaluation, any <code>allows</code> will not work. See the diagram below from the product documentation to review the decision flow for resource access authorization:</li> </ul> <p></p> IAM policy evaluation workshop <p>The IAM policy evaluation workshop is useful to understand the policy evaluation. The CF template is in labs/security/iam. The lab illustrates the Role defined and assigned to the EC2 running Cloud9, cannot list S3 bucket, but 1/ adding an inline identity-based policy will get it working. There is no resource based policy on S3. The session policy allows all access and acts as a pass through. 2/ Adding a resource-based policy on S3 denies access to the role. The role used in the workshop has action like: <code>s3:GetBucketPolicy</code> and <code>s3:PutBucketPolicy</code>, so it can create policies. Deleting the identity policy on the role and then adding a resource-policy on s3 bucket like the one below, lets the role accessing the bucket.</p> <pre><code>\"Sid\": \"AccessToLabBucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"$IAM_LAB_ROLE_ARN\"\n        },\n        \"Action\": \"s3:ListBucket\",\n        \"Resource\": \"arn:aws:s3:::$S3_LAB_BUCKET_NAME\"\n</code></pre> <p>In Part 2 of the lab, it demonstrates that a resource based policy with an Allow on all Principals, bypasses the permission boundary, so exits at column 3 in figure above. While a deny on permission boundary with a resource-based policy (This works because we are in the same AWS account):</p> <pre><code>  {\n        \"Sid\": \"AccessToLabBucket\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"AWS\": \"$IAM_LAB_ROLE_ARN\"\n        },\n        \"Action\": \"s3:ListBucket\",\n        \"Resource\": \"arn:aws:s3:::$S3_LAB_BUCKET_NAME\"\n    }\n\n    When evaluating policies for a cross-account request, the resource-based policy and all policies attached to the principal making the request are evaluated separately and both must allow access, regardless of what value is used in the Principal element of the resource-based policy.\n</code></pre> <ul> <li>Access Advisor helps to see permission granted to a user or role, and when services  were last accessed. It helps to identify unused permissions so that we can remove them.</li> <li>Access Analyzer: Analyze resources (S3 buckets, IAM roles, KMS keys, Lambda fct, SQS Queues, secrets) that are shared with external entity.</li> <li>Policy may define the password type <code>&gt; Account settings &gt; Password policy</code>, and when users are allowed to change the password.</li> <li>Inline policy can be defined at the user level, but it is recommended to use Group and Group level policies. As user can be part of multi groups, she/he will heritate to the different policies of those groups.</li> <li>IAM is not used for website authentication and authorization. AWS Cognito is a better fit for that.</li> <li>For identity federation, use SAML standard.</li> <li>We can test Policies with the policy simulator.</li> <li> <p>We can update a policy attached to a role and it takes immediate effect.</p> </li> <li> <p>We can attach &amp; detach roles to running EC2, without having to stop and restart it.</p> </li> <li>A policy has a max character count size around 10,240. </li> <li> <p>IAM policies can use variables, like <code>$aws:username</code>, and Tags like <code>aws:PrincipalTag/key-name</code>.</p> </li> <li> <p>Very interesting examples of IAM identity based policies.</p> </li> </ul>"},{"location":"infra/security/#iam-roles","title":"IAM Roles","text":"<p>An IAM role is an IAM identity (as a user is) that we can create in our account to grant it specific permissions.</p> <ul> <li> <p>Found this summary very good</p> </li> <li> <p>To get AWS services doing work on other service, we need to define IAM Role and policies. Roles are assigned per application, or per EC2 or lambda function... A lot of roles are predefined and can be reused. We can also define new role for any service intance we create. Here are example of predefined service roles:</p> <p></p> </li> <li> <p>A role is intended to be assumable by anyone who needs it. This means another identity will identify itself to be that role. The following diagram is used for Account A to access S3 bucket in Account B via a role defined in Account B with policies to access S3.</p> <p></p> </li> <li> <p>In the role definition there is <code>Link to switch roles in console</code> URL that permits to assume the role for a given user.</p> </li> <li> <p>There is no password for a role, when a service assumes a role, IAM dynamically provides temporary security credentials that expire after a defined period of time, between 15 minutes to 36 hours. Temporary Security Credentials (consisting of access key id, secret access key and a security token). The credentials are coming from AWS Security Token Service via AssumeRole API. It solves use cases like cross account access and single sign-on to AWS.</p> <p></p> </li> <li> <p>When a user, app, service, assumes a role, it gave up the original permissions it had and takes the permissions assigned to the role. While with resource based policy the principal doesn't have to give up permissions.</p> </li> <li> <p>The assumed role needs to trust the assuming identity explicitly via a Trust relationship declaration: The following statement enables EC2 to assume the role to which this trust relationship is attached to:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"ec2.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> </li> <li> <p>So when defining a role, we should define the trusted entity (principals), that may use this role, and then a policy to define what actions can be done on the service (in the role's permission settings). Trust relationship is used to share resources between accounts, it is a bidirectional relationship. Account owner may authorize to have users assuming a given role.</p> </li> <li>A role that a service assumes to perform actions on our behalf is called a service role.</li> <li>A service-linked role is a type of service role that is linked to an AWS service. They are own by the services, be visible in our AWS account but not editable.</li> <li>A role can be attached to an EC2 instance, and so any applications running on that EC2 instance, can get temporary AWS credentials to access permitted services. The role supplies temporary permissions that applications can use when they make calls to other AWS resources:</li> </ul> <p></p> EC2 control with IAM role <p>When connected to an EC2 machine via ssh or using EC2 Instance Connect tool, we need to set an IAM role to define who can use the EC2 and what command they can do within EC2. Inside the EC2 shell, a command like <code>aws iam list-users</code> will not work until a role is attached to EC2 with a policy to autorize IAM actions. For example, the following <code>DemoEC2Role</code> role is defined to let read-only IAM access:</p> <p></p> <p>This role is then used in the EC2 / Security  &gt; attach IAM role, and now read-only commands with <code>aws iam</code> will execute successfully inside this EC2.</p> IAM user and role with CDK <p>Example of creating a user to get EC2 full access via a role (see code under labs/cdk/iam-user):</p> <pre><code>user = iam.User(self,\n                \"Julie\",\n                user_name=\"Julie\",\n                password=SecretValue.unsafe_plain_text(\"Passw0rd!\")\n                )\n\nrole = iam.Role(self,\"EC2FullAccess\",\n                role_name=\"EC2FullAccess\",\n                description=\"A role to allow specified user to do EC2 work\",\n                assumed_by=user,\n                managed_policies=[iam.ManagedPolicy.from_aws_managed_policy_name(\"AmazonEC2FullAccess\")])\n</code></pre> IAM user and role using SDK <p>See product documentation and the following personal AWS Organization play examples.</p> Lamba to access S3 bucket <p>We want to implement a lambda function, that will access one S3 bucket to read files and another s3 bucket to put objects. We need a new role as illustrated in following figure, with permission to execute on lambda service, trace with XRay and R/W to S3 policy.</p> <p></p> <p>See the policy examples defined in labs/s3-lambda. <code>AWSLambdaBasicExecutionRole</code> is for logs, <code>AWSXRayDaemonWriteAccess</code> for the function to put traces into CloudWatch XRay.</p> <p>When defining the Lambda function we select the role defined with the expected permissions so the function can access other services.</p> Playing with Organizations <p>See readme and code in the aws-organization-play repo with assume role between accounts.</p>"},{"location":"infra/security/#resource-based-policy","title":"Resource-based policy","text":"<p>When user, application or service assumes a role, it takes the permissions assigned to the role, and looses its original permissions. While when we use resource-based policy, the principal doesn't have to give up his permissions. For example if a user in Account A needs to scan DynamoDB table in account A and dumps it in S3 bucket in account B, then it is important to use resource-based policy for S3 bucket, so user does not loose its access to dynamoDB.</p>"},{"location":"infra/security/#attribute-based-access-control","title":"Attribute-based access control","text":"<p>Attribute-based access control (ABAC) helps to determine access to resources based on attributes of the user, resource, application' states and environment conditions. An ABAC policy could allow access to a payment processing endpoint only for users in the finance department during business hours.</p> <p>Attributes may come from multiple sources: IAM tags, STS session tags, resource tags.</p> <p>ABAC gives a lot of flexibility compared to traditional IAM policies by enabling dynamic, context-aware authorization decisions:  who is the user? What are they trying to access? How are they trying to access it? What environment are they in? </p> <p>The disadvantage to using the traditional RBAC model is that when employees add new resources, you must update policies to allow access to those resources. </p> <p>Here is an example of such policy:</p> <pre><code>{ \"Version\": \"2012-10-17\", \n  \"Statement\": [ \n    { \"Effect\": \"Allow\", \n      \"Action\": \"s3:ListBucket\", \n      \"Resource\": \"arn:aws:s3:::example_bucket\", \n      \"Condition\": { \n        \"StringEquals\": { \"aws:PrincipalOrgID\": \"o-12345\", \"aws:PrincipalTag/Department\": \"Finance\" } } } \n    ]\n}\n</code></pre> <p>ABAC policy examples.</p> <p>sts:TagSession permission allows an IAM user or role to pass session tags when assuming a role or federating users. Session tags are custom attributes that get attached to the resulting temporary security credentials</p> <p>Session tags are key-value pairs that can be used to organize or track assumed role sessions.</p> <p>Session tags get returned with the AssumeRole response and can then be referenced in IAM policies or AWS Config rules. This allows conditional access based on session attributes. The maximum number of session tags per response is 10, with a maximum key and value size of 128 bytes each.</p>"},{"location":"infra/security/#some-important-articles","title":"Some important articles","text":"<ul> <li>Learn to create a user, a role, using the console.</li> <li>IAM tutorials with matching aws cli iam scripts in labs/security/iam</li> <li>Using IAM role product documentation</li> <li>Use IAM roles to connect GitHub Actions to actions in AWS to get an example of trusting an external system to access AWS services via our AWS account.</li> <li>Policy evaluation logic</li> </ul>"},{"location":"infra/security/#permissions-boundary","title":"Permissions boundary","text":"<p>Set a permissions boundary to control the maximum permissions a user can have using identity-based policies. This is defined at user's or role level, and we define a customer managed policy or a AWS managed policy, for example, to authorize the user to do anything on EC2, CloudWatch or S3. Permissions boundary limits the user's permissions but does not provide permissions on its own.</p> <p>The effective permission of a user is the join between Organization SCP, Permissions Boundary, and identity-based policies.</p> <p></p> IAM policy evaluation workshop <p>The IAM policy evaluation workshop lab2 demonstrates permission boundary.</p> <p>They are used to:</p> <ul> <li>Delegate responsibilities to non-admin users within their permission boundaries to create specific resources, like an IAM user.</li> <li>Allow developers to self-assign policies and manage their own permissions, while making sure they can increase their privileges (to an admin user).</li> <li>Restrict one user.</li> </ul> <p>See this blog as a good example of things to do..</p>"},{"location":"infra/security/#security-tools","title":"Security tools","text":"<ul> <li>In IAM, use <code>&gt; Credentials report</code> to download account based report.</li> <li>In IAM, use <code>&gt; Users &gt; select one user and then Access Advisor tab</code>: Access Advisor shows the services that the selected user can access and when those services were last accessed.</li> <li>Amazon GuardDuty is a security tool to continuously monitor the AWS accounts, instances, containers, users, and storage for potential threats.</li> <li> <p>Access Analyzer: Analyze resources (S3 buckets, IAM roles, KMS keys, Lambda fct, SQS Queues, secrets) that are shared with external entity. It uses the concept of Zone of Trust, which is an AWS account or Organization. Any potential violation is called a finding. It can be used to validate policy syntax, but to generate policy too. The diagram below illustrates how Access Analyzer can use CloudTrail logs from a lambda execution to help create an accurate IAM policy, to define accurate security scope:</p> <p></p> </li> </ul>"},{"location":"infra/security/#iam-identity-center","title":"IAM Identity Center","text":"<p>This is a single sign-on managed services for all AWS accounts within AWS Organizations, cloud applications, SAML2.0-enabled applications, EC2 Windows instances. IAM Identity Center requires AWS Organizations. When Identity Center is enabled, it creates a service-linked role in all accounts within the organization in AWS Organizations.</p> <p>By default IC has an identity store. But it can be connected to a remote directory service. The SSO works with a login, and users/groups are defined within IC store.</p> <p>AWS Organizations supports IAM Identity Center in only one AWS Region at a time. We need to delete IC if we want to use another region.</p> <p>The goal of identity federation is to get user outside of AWS, the permissions to access AWS resources into our accounts. Identity federation is supported by different solutions:</p> <ul> <li>SAML 2.0: Open standard used by many identity providers like Microsfodt Active Directory Federations Services (ADFS). We need to define a trust between the SAML 2.0 identity provider and AWS IAM, and then use <code>AssumeRoleWithSAML</code> STS API.</li> <li>Custom Identity Broker</li> <li>Web identity with Amazon Cognito</li> <li>SSO</li> </ul> <p>The identity provider can be an identity store in IAM Identity center or a 3nd party solution like an Active Directory, OneLogin, Okta...</p> <p>Permission sets are a way to define permissions centrally in IAM Identity Center so that they can be applied to all of our AWS accounts. These permission sets are provisioned to each AWS account as an IAM role.</p> <p></p> <p>The user can access the access portal (URL like https://d-92676f0fd8.awsapps.com/start), then the account it is associated to, and the permission sets he wants to use, then he can access the AWS management console. Depending of the permission sets the user can perform different actions to services.</p> <p>A permission sets can have customer permission, defined as part of a customer managed policies.</p> Apply permission set for an SRE team <p>A SRE team needs to access different member accounts, but only to see CloudWatch log groups. To support that do:</p> <ol> <li>Create a customer managed IAM policy to allow \"logs:CreateLogStream\"... on specific account <code>id:log-group:*</code></li> <li>Create a permission set in IC, and use the custom permission using the policy from previous step.</li> <li>Create a group in IC: <code>operations</code></li> <li>Create a user <code>operationsUser</code> and add to the group <code>operations</code></li> <li>Select the account we want those user to access, and assign the permission set to it.</li> </ol> Define attribute based access control <p>When creating a user in IC, there are attributes (tags) defined, like a Department. Those can be used to as conditions in an IAM policy, and then assigned to a permission set.</p> <pre><code>\"Condition\": {\n            \"StringEquals\": {\n             \"ec2:ResourceTag/Department\": \"${aws:PrincipalTag/Department}\"\n            }\n</code></pre> <p>Then enable attributes for access control, in IC settings. And use <code>Key: Department</code> and <code>Value:  ${path:enterprise.department}</code></p> <ul> <li>API reference</li> </ul>"},{"location":"infra/security/#aws-organizations","title":"AWS Organizations","text":"<p>AWS Organizations helps to centraly manage multiple AWS accounts, group accounts, defines security policies cross account, and simplify account creation. Using accounts helps to isolate AWS resources. It is a global service.</p> <ul> <li>We can set up a single payment method for all the AWS accounts in our organization through consolidated billing.</li> <li>It groups accounts via organizational units (OUs), on which we can define policies.</li> <li>It is well suited to define custom multi-account environments with advanced governance and management capabilities.</li> </ul>"},{"location":"infra/security/#concepts","title":"Concepts","text":"<p>Figure 5: Organization concepts</p> <ul> <li>A management account is the AWS account you use to create your organization.</li> <li>An organization is a hierarchical structure (a tree) with an administrative root and Organization Units (OU), and AWS accounts.</li> <li>The <code>root</code> user is a single sign-in identity that has complete access to all AWS services and resources in any accounts.</li> <li>Organization unit (OU) contains AWS Accounts or other OUs. It can have only one parent.</li> </ul> <p></p> <p>Figure 6: Organization Services - manage accounts</p> <ul> <li>OU can be per team, per line of business.</li> <li> <p>When creating an account, the management account may use the <code>OrganizationAccountAccessRole</code> role to grant full admin permissions in the member accounts to the management account itself. It is used to perform admin tasks in the member accounts, like creating IAM users.</p> <p></p> </li> <li> <p>Once account created, a user needs to go to the AWS login console (https://signin.aws.amazon.com/), use the email address and forgot password option, or use the one-time password defined during creation.</p> </li> <li>AWS Organizations uses IAM service-linked roles to enable trusted services to perform tasks on our behalf in our organization's member accounts.</li> <li>We can create Service Control Policies (SCPs) cross AWS accounts to filter which AWS services can be used for individuals or group of accounts in an OU. Explicit allow. The root OU will have <code>FullAWSAccess</code> SCP.</li> <li>AWS Organization exposes APIs to automate account management.</li> <li>Organization helps consolidating billing accross all the accounts and user can get pricing benefits from aggregate usage. Shared reserved instances and Saving Plans discounts apply across accounts. Can define Blocklist or Allowlist strategies.</li> <li>There is no cost to use AWS Organizations.</li> <li>We can invite an existing AWS account to our organization. But paymen t changes of ownership.</li> <li>There is a special feature called <code>All Features</code> to include consolidated billing, integration with AWS services and SCPs. To share a RI or Savings Plans discount with an account, both accounts must have sharing turned on.</li> <li>With Organizations, we can centrally orchestrate any AWS CloudFormation enabled service across multiple AWS accounts and regions.</li> </ul>"},{"location":"infra/security/#advantages","title":"Advantages","text":"<ul> <li>Group resources for categorization and discovery</li> <li>Define logical boundary for security purpose.</li> <li>Limit blast radius in case of unauthorized access.</li> <li>More easily manage user access to different environments. If team A can't support team B's app, the applications should not be in the same account.</li> <li>Can use tagging for billing purpose.</li> <li>Help to bypass the limit per account, for example the number of lambda function per account.</li> <li>Enable CloudTrail for all accounts and get report in a central S3 bucket.</li> </ul> <p>Figure 7: Organization policies</p>"},{"location":"infra/security/#service-control-policies","title":"Service Control Policies","text":"<p>They define the <code>maximum available</code> permissions for IAM entities in an account. SCPs are attached to the root org, OUs, or individual accounts. They are inherited. SCPs alone are not sufficient for allowing access to the accounts in our organization. Attaching an SCP to an AWS Organizations entity just defines a guardrail for what actions the principals can perform.</p> <p>We can define allowance policy on resources within the account using something like:</p> <pre><code>{\n   \"Effect\": \"Allow\",\n    \" Action\" : [\n        \"ec2:*\", \"ds:*\", \"s3:*\"\n    ],\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"aws:RequestedRegion\" : [ \"eu-central-1\", \"us-west-1\" ]\n        }\n    }\n}\n</code></pre> <p>Or denying like: (Be sure to have policy to allow \"*\" )</p> <pre><code>{\n   \"Effect\": \"Deny\",\n    \" Action\" : [\n        \"organizations:LeaveOrganization\", \"ec2:TerminateInstances\"\n        , \"ec2:PurchaseReservedInstancesOffering\", \"ec2:ModifyReservedInstances\"\n    ],\n    \"Resource\": \"*\"\n}\n</code></pre> <p>Account administrator must still attach identity-based policies to IAM users or roles, or resource-based policies to the resources in the accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the identity-based and resource-based policies.</p> <p>SCP defined at the management account level has no impact as management account can do anything.</p> <p>As some AWS AI services may use the data of an account, it is possible to define OptOut policies.</p> Enforce using tags to launch EC2 instances <p>Use Organizations. Create a Service Control Policy that restricts launching any AWS resources without a tag by including the Condition element in the policy which uses the <code>ForAllValues</code> qualifier and the <code>aws:TagKeys</code> condition.  <pre><code>\"Statement\": [\n{\n  \"Sid\": \"RequireTags\",\n  \"Effect\": \"Deny\",\n  \"Action\": [\"*\"],\n  \"Resource\": [\"*\"],\n  \"Condition\": {\n    \"ForAllValues:StringEquals\": {\n      \"aws:TagKeys\": [\"TagKey1\",\"TagKey2\"] \n    }\n  }\n}\n]\n</code></pre></p>"},{"location":"infra/security/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>Presentation on organizations</li> <li>User guide</li> <li>Organization API</li> <li>My Organization playground repo</li> <li>S3 storage lens with Organization playground lab</li> <li>Tutorials Dojo, cheat sheet.</li> <li>Infrastructure Identity in AWS workshop to use Systems Manager and IAM.</li> </ul> Limit creating EC2 instance without needed tag <pre><code>{   \"Effect\": \"Deny\",\n    \"Action\" : \"ec2:RunInstances\",\n    \"Resource\": \"arn:aws:ec2:*:*:instance/*\",\n    \"Condition\": {\n        \"Null\": {\n            \"aws:RequestTag/Project\": \"true\"\n        }\n    }\n}\n</code></pre>"},{"location":"infra/security/#aws-security-token-service-api","title":"AWS Security Token Service API","text":"<p>Very important service to assume role accross accounts, within the same account, and to support identity federation:</p> <ul> <li> <p>The STS is used to request temporary, limited-privilege credentials for users, via the <code>AssumeRole</code> API.</p> <p></p> </li> <li> <p>There is a global service https://sts.amazonaws.com endpoint with regional access endpoints too.</p> </li> <li>The API has the main action called <code>AssumeRole</code> which returns a AccessKeyId, SecretAccessKey, and SessionToken, so the requester can access AWS resources via API (but not sts getFederationToken or getSessionToken).</li> <li>To assume a role from a different account, the AWS account must be trusted by the role.</li> <li> <p>User must assume the role using the AWS management console, or using CLI, or AWS API.</p> <pre><code># with the user key in the .aws/config\naws configure --profile Bill\naws sts assume-role --role-arn \"arn:aws:iam::&lt;account-id-cust-A1&gt;:role/UpdateApp\" --role-session-name \"Bill-Update\"\n</code></pre> </li> <li> <p>STS offers the ability to revoke active session and credentials for a role, by adding time statement within a policy or by using API <code>AWSRevokeOlderSessions</code>.</p> </li> <li>When the AWS account is owned by 3nd party (not part of our zone of trust), STS should use an <code>externalID</code>, which is a secret between our account and the 3nd party to uniquely associate a role with the 3nd party. This id is used during the assumeRole API call to ensure this is the correct account and 3nd party, to avoid the confused deputy issue.</li> <li>When doing the API call, we can pass session policies and the returned permissions are the intersection of the role's identity-based policy and the session policies.</li> <li>Session policies are advanced policies that we pass as a parameter when we programmatically create a temporary session for a role or federated user.</li> <li>We can pass a single JSON policy document to use as an inline session policy, or pass 10 managed policy ARNs</li> <li>STS stores the data relating to the tags, policies, and ARNs within the AWS session token. All this data counts towards the PackedPolicySize limit. Inline session policies and session tags are serialized and compressed in the session token. See this re:Post note</li> <li> <p>The plaintext to reference ARN or pass inline policy is limited to 2048 chars. Size limites..</p> </li> <li> <p>Session tags are (key-value pairs) passed to the assumeRole API. Tag are added (50 tags max) to the session to control access to resources or to control what tags can be passed into a subsequent session. They are normally used for attribute-based access control (ABAC).</p> </li> </ul> <p>See code example to get a token for a role and control access via session policies</p>"},{"location":"infra/security/#amazon-cognito","title":"Amazon Cognito","text":"<p>Amazon Cognito is a managed service which offers Authentication and Authorization features, it has User pools and Identity pools. It is scalable and highly available. Allows user to add user registration, sign in, and define access control.</p> <ul> <li>Supports standards based identity providers like OAuth 2.0, SAML, OIDC.</li> <li>User pools are user directories that provide sign-up and sign-in options for our apps or mobile users. </li> <li>User pool is for sign-in functionality, and it integrates with API Gateway and ALB.</li> <li>Identity pools provide AWS credentials to grant our users access to our APIs, other AWS services via IAM permissions.</li> </ul> <p></p> <ul> <li>Free tier of 50,000 MAUs for users who sign in directly to Cognito User Pools and 50 MAUs for users federated through SAML 2.0 based identity providers.</li> <li>Users can sign in through social identity providers like Google, Facebook, and Amazon. They can also sign in through enterprise providers like ADFS and Okta with SAML 2.0 and OpenID Connect authentication.</li> <li>Use Amazon Cognito's built-in UI and options for federating with multiple identity providers to add user sign-in, sign-up into an application. </li> <li>use AWS Amplify and the aws-amplify-vue module to provide basic user sign up, sign in, and sign out functionality.</li> </ul>"},{"location":"infra/security/#examples","title":"Examples","text":"<ul> <li>Configure how user login:</li> </ul> <ul> <li>Then define if we want to use MFA, get an option to reset password..</li> <li>Configure sign-up and message delivery. We can disable auto sign-up and sending email.</li> </ul>"},{"location":"infra/security/#resource-access-manager","title":"Resource Access Manager","text":"<ul> <li>Goal: share resources with any AWS accounts. It avoid declaring duplicate resources within multiple accounts.</li> <li>No additional charge</li> <li>Share CIDR blocks via managed prefix list, so other account can define security policies using the lists</li> <li> <p>Share VPC subnets, Transit Gateway, Route 53 outbound resolver</p> </li> <li> <p>Cheat Sheet.</p> </li> <li>Product Doc</li> <li>FAQ</li> </ul>"},{"location":"infra/security/#aws-control-tower","title":"AWS Control Tower","text":"<p>AWS Control Tower orchestrates other services like AWS Organizations, IAM identity center, Service Catalog. It helps to set up and to govern multi-account environments to avoid them to drift from best practices. Drift may happen when we create an account outside of Control Tower.</p> <p>It uses the concepts of Landing zone as the multi-account environment. When creating a landing zone, it creates a <code>Security</code> OU with the log archive account, and the security audit account.</p> <p></p> <p>controls or guardrails which are rule to provide governance for the environment. Detective Controls detect noncompliant resources in our accounts and provide alerts through the Control Tower dashboard. They are implemented in AWS Config rules. Proactive Controls check compliance of the resources before they are provisioned and are implemented using CloudFormation hooks and Guard rules. Preventive Controls prevent actions from occurring and are implemented as Service Control Policies (SCPs) attached to the OUs.</p> Control example <p>If you enable the detective control Detect whether public write access to Amazon S3 buckets is allowed on an OU, you can determine whether a user would be permitted to have read access to any S3 buckets for any accounts under that OU.</p> <p>Account Factory is a configurable account template that helps to standardize the provisioning of new accounts with pre-approved account configurations (VPC default, subnets, regions...)</p> <p>It runs on top of Organization and leverages SCPs for preventative guardrails and AWS Config for detective guardrails.</p> <p>Detect policy violations and remediate them.</p> <p>It uses Service Catalog to provision new AWS accounts.</p> <p>AWS Control Tower aggregates information from all accounts into the organization trail and delivers the logged information to a specified Amazon S3 bucket.</p> <p>AWS Control Tower provides us with over 360 controls out-of-the-box to support the distinct security requirements of our workloads.</p> <p>No additional charge exists for using AWS Control Tower. We only pay for the AWS services enabled by AWS Control Tower, and the services we use in our landing zone.</p> <ul> <li>Control Tower workshop</li> <li>Flat multi-account environment</li> <li>Organizing Environments</li> </ul>"},{"location":"infra/security/#2-data-protection","title":"2- Data Protection","text":""},{"location":"infra/security/#encryption","title":"Encryption","text":"<p>Encryption is widely available through a lot of services and features on top of the platform. We will be able to develop application that can encrypt data at rest, or in transit as it flows over the network between services. S3 storage or EBS block attached storage, have a single click option to do encryption at rest with keys (using KMS or S3).</p> <p></p> <p>Figure 1: Encryption settings for EBS volume, using KMS</p>"},{"location":"infra/security/#kms","title":"KMS","text":"<p>As a managed service, AWS Key Management Service, helps centrally managing our own keys material or the one created by AWS service. User controls access and usage of the keys.</p> <p>A KMS key is a logical representation of a cryptographic key. In addition to the key material used in cryptographic operations, a KMS key includes metadata, such as the key ID, key policy, creation date, description, and key state.</p> <p>With client side encryption, the data is encrypted by the client, and never decrypted by the Server. Only client with the data key can decrypt the data.</p> <p></p> <p>Figure 2: KMS &amp; AWS keys</p> <ul> <li>Integrated with IAM and most AWS services (EBS,S3, RDS, SSM...). The keys never leave AWS.</li> <li>Audit KMS Key usage using Amazon CloudTrail.</li> <li> <p>Two types of KMS Keys:</p> <ul> <li>Symmetric (AES 265 bits) is a single key used to encrypt and decrypt data. Must call KMS API to use it.</li> <li>Asymmetrics (RSA, ECC key pairs) - Public key to encrypt and private key to decrypt. Used to encrypt outside of AWS, with no KMS API access.</li> </ul> </li> <li> <p>There are different types of KMS keys:</p> <ul> <li>Customer managed keys: full control to end-user to create, manage and use the key.</li> <li>AWS managed keys by AWS services within our account, like S3, EBS, redshift</li> <li>AWS owned keys used for AWS services managed services.</li> </ul> <p></p> </li> <li> <p>The Key material Origin is used to identify the source of the key. The Origin can be KMS, or an external service outside AWS but imported inside KMS, or a Custom Key Store which is a AWS CloudHSM cluster managed by us, and where KMS creates the key but let the external key store to manage it. </p> </li> <li> <p>Finally a Key source could be completely external, and supports the Bring Your Own Key use case. The key needs to be a 256-bit symmetric key. The process looks like in the figure below:</p> <p></p> </li> <li> <p>For AWS managed keys, they are automatically rotated every year.</p> </li> <li>KMS Keys are per region. But when doing snapshot of a EBS volume and moving it to another region, AWS will reencrypt the data with a KMS key from the target region automatically. </li> <li>KMS Key policies help to control who (users, roles ) can access the KMS keys. There is one policy per key. Without permission from the key policy, IAM policies that allow permissions to access the key have no effect. We need both. See the product doc.</li> <li>Key policies are used to do cross account access: the copy of a encrypted snapshot done from origin account to the target account will use this policy to access the key to decrypt the snapshot, and then will encrypt the copy with a new private key within the target account.</li> </ul> <p></p> <p>Figure 3: AWS key for S3 encryption, with Key policy</p> <ul> <li> <p>To encrypt a local file using a symmetric Key in KMS (referenced as <code>alias/jb-key</code>), we can use the CLI like:</p> <pre><code>aws kms encrypt --key-id alias/jb-key --plaintext fileb://ExampleSecretFile.txt --output text --query CiphertextBlob  --region eu-west-2 &gt; ExampleSecretFileEncrypted.base64\n</code></pre> </li> <li> <p>To share the encrypted file, we can do: </p> <pre><code>cat ExampleSecretFileEncrypted.base64 | base64 --decode &gt; ExampleSecretFileEncrypted\n</code></pre> </li> <li> <p>Then to decrypt this file using KMS:</p> <pre><code>aws kms decrypt --ciphertext-blob fileb://ExampleSecretFileEncrypted   --output text --query Plaintext &gt; ExampleFileDecrypted.base64  --region eu-west-2\n# back to the text version\ncat ExampleFileDecrypted.base64 | base64 --decode &gt; ExampleFileDecrypted.txt\n</code></pre> </li> </ul> <p>KMS supports also Multi-Region Keys, where primary key from one region is replicated to other regions. The Key ID stays the same. The idea is to be able to encrypt in one region and decrypt in another region. A use case will be to encrypt attribute of a DynamoDB Global Table with the Primary key, and let client, who has access to the replicated key, decrypt the attribute with lower latency.</p> <p></p> <p>Figure 4: DB table encrypted and replicated with Keys</p> <p>The attribute is decrypted only if the client has access to the KMS Key.</p> <ul> <li>See AWS kms create-key CLI for examples of creating customer managed keys.</li> </ul>"},{"location":"infra/security/#s3-encryption-considerations","title":"S3 encryption considerations","text":"<p>By default objects encrypted (or not) with SSE-S3 are replicated.</p> <p>With SSE-C, we, as customer, provide the encryption key, so the encrypted objects are not replicated.</p> <p>With SSE-KMS then we need to specify the KMS Key to encrypt the object in target bucket, adapt the KMS policy so the key is accessible in another region, and in the IAM role, be sure the actions <code>kms:Decrypt</code> is enabled for the source key and <code>kms:Encrypt</code> for the target KMS Key.</p>"},{"location":"infra/security/#encrypted-ami-sharing-process","title":"Encrypted AMI sharing process","text":"<ul> <li>AMI in source account is encrypted with KMS Key from source account.</li> <li>AMI image should authorize the Launch permission for the target account.</li> <li>Define IAM role for the target account to use, and share KMS key accesses (DescribeKey, ReEncrypted, CreateGrant, Decrypt) via the role.</li> <li>When launching EC2 instance from this AMI in the target account, it is possible to use a new, local KMS key to re-encrypt the volumes.</li> </ul>"},{"location":"infra/security/#system-management-parameter-store","title":"System management parameter store","text":"<p>Managed service, serverless, it is a secure storage for configuration and secrets. This is a key-value storage, with simple SDK for easy integration in application code. </p> <p></p> <ul> <li> <p>SSM Parameter Store has built-in version tracking capability. Each time we edit the value of a parameter, SSM Parameter Store creates a new version of the parameter and retains the previous versions.</p> </li> <li> <p>It is using KMS for seamless encryption. The security access control is via IAM and notifications can be propagated to EventBridge for routing. </p> </li> <li> <p>It stores data in a hierarchical tree. For example  </p> <pre><code>aws ssm get-parameters-by-path --path /an-app/dev/\n</code></pre> </li> <li> <p>It is integration with CloudFormation and be used to keep application parameters. </p> </li> <li>Parameter store walkthroughs.</li> <li>Quarkus app to access SSM parameter store with Java SDK.</li> </ul>"},{"location":"infra/security/#amazon-macie","title":"Amazon Macie","text":"<p>Amazon Macie is a fully managed data security service that uses Machine Learning to discover and protect our sensitive data stored in S3 buckets. It automatically provides an inventory of S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with other AWS accounts. It allows us to identify and alert us to sensitive data, such as Personally Identifiable Information (PII).</p> <p>All Macie findings are sent to Amazon EventBridge and can also be published to AWS Security Hub to initiate automated remediation such as blocking public access to our S3 storage.</p> <ul> <li>Multi account support, integrated with AWS Organization</li> <li>Automated sensitive data discovery</li> <li>Can run scan jobs</li> <li>Scan 1% of content in buckets $1 per GB for first 50TB/ month.</li> </ul>"},{"location":"infra/security/#secrets-manager","title":"Secrets Manager","text":"<p>Keep Secret information (unique key-value, or JSON doc), with automatic rotation enforced, and integration with RDS, Redshift, ECS Task, DocumentDB.... Secrets can be replicated between regions. They are encrypted, and keys are in KMS and can be a customer managed key.</p> <p>Secret has metadata like ARN of the key used. SM keeps version of the secrets .When user changes the secret value, or the secret is rotated, Secrets Manager creates a new version.</p> <p>When Secrets Manager rotates a secret, it updates the credentials in both the secret and the database or service. The Single user rotation strategy updates credentials for one user in one secret. The alternating usersv strategy updates credentials for two users in one secret. We create the first user, and during the first rotation, the rotation function clones it to create the second user. Every time the secret rotates, the rotation function alternates which user's password it updates.</p> <p>See product tutorials.</p> <p>We need two IAM roles to manage permission to the Secret. 1/ A role that manages the secrets. 2/ A role that use the credentials at runtime <code>RoleToRetrieveSecretAtRuntime</code>.</p> <p>We can control access to secrets using Resource-based Policy, specially when using cross account. The following diagram illustrates the process for getting cross account access:</p> <p></p> <p>For ECS task, at boot time, secrets are pulled and set in environment variables accessible to the container within the task.</p> <p>A simple demo of CDK to create secrets and ECS cluster with a Quarkus app accessing the Secrets.</p>"},{"location":"infra/security/#acm-certificate-manager","title":"ACM - Certificate Manager","text":"<p>Manage and deploy TLS certificates. Supports public and private certificates. Automatic renewal.</p> <p>Free of charge for public certificates. </p> <p>Integrated with ELBs, Cloudfront, APIs on API gateway. Cannot use ACM with EC2.</p>"},{"location":"infra/security/#aws-cloudhsm","title":"AWS CloudHSM","text":"<p>AWS CloudHSM (Hardware Security Module) helps  meet corporate, contractual, and regulatory compliance requirements for data security.</p> <p>HSM provides secure key storage and cryptographic operations within a tamper-resistant hardware device. HSMs are designed to securely store cryptographic key material and use the key material without exposing it outside the cryptographic boundary of the hardware.</p> <p>We can create a cluster that has from 1 to 28 HSMs (the default limit is 6 HSMs per AWS account per AWS Region).</p> <p></p> <p>Example of use case is when hosted on EC2, must encrypt the data before writing the data to a storage.</p> <p>We can also use AWS KMS if we need to encrypt data before writing to storage when an AWS managed hardware security module for the cryptographic keys is not required.</p>"},{"location":"infra/security/#3-network-and-app-protection","title":"3- Network and App Protection","text":""},{"location":"infra/security/#waf-web-application-firewall","title":"WAF Web Application Firewall","text":"<ul> <li>Used to monitor the HTTP(S) requests that are forwarded to the protected (Layer 7) web application resources.</li> <li> <p>It is deployed on ALB, API Gateway, CloudFront, AppSync GraphQL API, Cognito User Pool.</p> </li> <li> <p>Rules are defined in Web Access Control List: they can apply to IP@, HTTP headers, body, URI strings, SQL injection, Cross-site Scripting...</p> </li> <li>We can define size constraints, geographic matches, or rate-based rules for DDoS protection. For example we can implement a rule capable of blocking all IPs that have more than 2,000 requests in the last 5 minute interval.</li> <li>It provides real-time metrics and captures raw requests that include details about IP addresses, geo locations, URIs, User-Agent and Referers.</li> <li>Rules can be combined into WebACL to define action for each rule. WebACL is regional, except for CloudFront.</li> <li>Can add custom HTTP header to validate the requests.</li> <li>Charges based on the number of Web ACLs</li> </ul>"},{"location":"infra/security/#deeper-dive_1","title":"Deeper dive","text":"<ul> <li>Cheat sheet.</li> <li>Rate-based rule statement</li> <li>WAF FAQ</li> </ul>"},{"location":"infra/security/#aws-shield","title":"AWS Shield","text":"<p>AWS Shield Standard is enabled by default, and free, it provides protection from layer 3 and 4 attacks, like SYN/UDP floods, refection attacks. </p> <p>Shield Advanced is for DDoS attacks. Protects against more sophisticated attack on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, Route 53. It supports  network and transport layer protections with near real-time visibility into attacks and 24/7 access to Shield Response Team.</p> DDoS <p>Distributed Denial of Service (DDoS), includes infrastructure and Application attacks. Uses bots to makes calls to servers or applications, so normal users see not responsive server. AWS Shield standard and Advanced protect against DDoS.</p>"},{"location":"infra/security/#firewall-manager","title":"Firewall manager","text":"<p>Firewall manager simplifies the administration and maintenance tasks across multiple accounts of an AWS Organization and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall.</p> <p>It uses security policy with security rules. Those rules could be:</p> <ul> <li>WAF rules to apply to ALB, API Gateways, CloudFront</li> <li>AWS Shield Advanced to appy to ALB, CLB, NLB, Elastic IP, CloudFront</li> <li>To standardize on Security group for EC2, ALB, ENI in VPC</li> <li>AWS Network Firewall, VPC level rules.</li> <li>Rules are defined at the Region level.</li> </ul> WAF vs. Firewall Manager vs. Shield <p>WAF, Shield and Firewall Manager are used together for comprehensive protection. Start by defining Web ACL rules in WAF. When we need one resource at a time protection then WAF alone is the correct choice. If we want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF. Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield ResponseTeam (SRT) and advanced reporting. If we\u2019re prone to frequent DDoS attacks, consider purchasing Shield Advanced.</p>"},{"location":"infra/security/#4-threat-detection-monitoring","title":"4 - Threat Detection &amp; monitoring","text":""},{"location":"infra/security/#aws-security-hub","title":"AWS Security Hub","text":"<p>AWS Security Hub is a Cloud Security Posture Management service that performs security best practice checks, aggregates alerts, and enables automated remediation.</p> <p>AWS Security Hub is a single place to view all our security alerts from services like Amazon GuardDuty, Amazon Inspector, Amazon Macie, and AWS Firewall Manager. It is not used for producing audit reports.</p>"},{"location":"infra/security/#amazon-guardduty","title":"Amazon GuardDuty","text":"<p>Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect the AWS accounts, workloads, and data stored in Amazon S3.</p> <p>Security monitoring service that analyzes and processes data sources such as:</p> <ul> <li>AWS CloudTrail event logs: to track unusual API calls, unauthorized deployments, or S3 data events for get object, list object... </li> <li>CloudTrail management event logs, </li> <li>DNS logs: compromized EC2 instances sending encoded data with DNS queries...</li> <li>Amazon EBS volume data, </li> <li>Kubernetes audit logs, </li> <li>Amazon VPC Flow Logs, to track unusual internal traffic.</li> <li>RDS login activity</li> </ul> <p>It uses ML/AI model, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats..</p> <p>GuardDuty can detect compromised EC2 instances and container workloads serving malware, or mining crypto currency.</p> <p>By integrating with Amazon EventBridge Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p>"},{"location":"infra/security/#amazon-detective","title":"Amazon Detective","text":"<p>When security findings coming from tools like GuardDuty, Macie, or Security Hub, require deeper analysis to isolate the root cause and take action, Amazon Detective can be used.</p> <p>Detective collects logs (VPC Flow logs, CloudTrail, GuardDuty) data from our AWS resources and uses machine learning, statistical analysis, and graph theory for security investigations.</p> <ul> <li>Cheat sheet.</li> <li>FAQs</li> </ul>"},{"location":"infra/security/#amazon-inspector","title":"Amazon Inspector","text":"<p>Vulnerability management service that continuously scans our AWS workloads for software vulnerabilities and unintended network exposure. Includes EC2 instance scan and container images in Amazon ECR when pushed to the registry, and the same for Lambda function.</p> <p>It uses a database of CVE vulnerabilities.</p> <p>The reports are sent to Security Hub and to EventBridge.</p>"},{"location":"infra/security/#privacy-and-compliance","title":"Privacy and Compliance","text":""},{"location":"infra/security/#aws-audit-manager","title":"AWS Audit Manager","text":"<p>AWS Audit Manager is an automated service to continually audit our AWS usage to simplify how we manage risk and compliance with regulations and industry standards. It eifjcbfdjifhurnicntgjklgrihfkfcdfierkgkghcdi produces reports specific to auditors for PCI compliance, GDPR, and more.</p>"},{"location":"infra/security/#aws-cloudtrail","title":"AWS CloudTrail","text":"<ul> <li>Provides governance, compliance and audit for any AWS account. It is enable by defaults</li> <li>Track all the API calls made within an AWS account. This includes SDK, CLI, Console and IAM roles and users.</li> <li>A trail can be applied to all regions or a single region. </li> <li>Helpful to assess who deleted a resource. </li> <li>The events can be sent to CloudWatch logs, EventBridge, a S3 bucket.</li> <li> <p>Different events tracked:</p> <ul> <li>Management Events: related to resources within our account. Read and write events can be separated.</li> <li>Data Events: not logged by defaults. Example: S3 object activities, lambda execution activities</li> <li>Insights Events: to detect unusual activity in our account: hitting service limits, bursts on IAM actions, gaps in periodic maintenance... It use normal behavior to create a baseline, and then continuously analyzes write events.</li> <li>Retention is for 90 days. For longer retention saved to S3 and then query with Athena.</li> </ul> </li> </ul>"},{"location":"infra/security/#security-faqs","title":"Security FAQs","text":"Sensitive data in the cloud can be accessed by everyone <p>AWS storage managed services are secured by default. We control who can access the data. For example, in S3 bucket, objects, by default, are only available by the owner who created them. IAM policies define access to buckets, and in S3 itself we can define policies. Some policies can be defined at the Account level to forbid to define public bucket. From detection services like <code>CloudTrail</code> and <code>CloudWatch  Events</code> we can route events to security team or respond in real-time to those events with code in a Lambda function.</p> Integrate with existing Directory? <p>IAM can integrate with an existing directory where we can map IAM groups to directory group membership. We can also set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens so that users in the LDAP can access S3 bucket for example. Note that IAM roles &amp; IAM policy need to be defined to access the bucket.</p> Running intrusion/penetration testing and vulnerability scan <p>We can run any of those tests on our own workloads and for the following services:</p> <ul> <li>Amazon EC2 instances</li> <li>NAT Gateways</li> <li>Elastic Load Balancers</li> <li>Amazon RDS</li> <li>Amazon CloudFront</li> <li>Amazon Aurora</li> <li>Amazon API Gateways</li> <li>AWS Fargate</li> <li>AWS Lambda and Lambda Edge functions</li> <li>Amazon Lightsail resources</li> <li>Amazon Elastic Beanstalk environments</li> </ul> <p>We need to sign a \"Seek approval\" document so AWS can understand what we are doing and is not searching for malicious activities on our account or triggers security alarms. To integrate Penetration tests in our CI/CD Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure.</p> How to ensure RDS database can only be accessed using profile credentials specific to EC2 instance? <p>IAM has database authentication capabilities that would allow an RDS database to only be accessed using the profile credentials specific to the EC2 instances.</p> Specifics business oriented accreditations <p>The workload is certified by a 3nd party auditor. AWS provides the accreditation doc for infrastructure and hosting operations. We have full visibility into the logical infrastructure via API calls, so the auditors can get information on items like security group (firewall) rules or RDS configuration via API calls, or account access information through our API auditing service, <code>CloudTrail</code>.</p> How does IDS/IPS works on AWS? <p>Intrusion Detection and Prevention can be achieved in number of ways:  <code>Guard Duty</code>, 3<sup>rd</sup> party solution (AWS marketplace) as well architecture best practices. Amazon VPC traffic mirroring is a \u201cvirtual fiber tap\u201d that gives us direct access to the network packets flowing through the VPC.</p> Encrypted traffic between instances? <p>Within the AWS network, traffic between our instances stay within our VPC. Other customers cannot see our traffic. Configuring inter-instance traffic encryption would be the same as on-premises.</p> Does AWS provide DDoS protection? <p>AWS Shield Standard is automically provided to all AWS customers and AWS Shield advanced service can be bought for additional features. AWS Shield is a managed Distributed Denial of Service events protection service that safeguards applications running on AWS.</p> How to be sure data is really deleted <p>We own the data inside AWS. But on delete operation the data is destroyed and wiped out. When destroying physical hardware, AWS follows the NIST 888 requirements, and it is done in a secure zone in the data center. Audits are done by 3nd party vendor, and compliances are reported in Artifact.</p> Is serverless secured? <p>Roles are used to define who can push code, access it, and fine grained control can be define for the serverless execution. All serverless managed services inherit from the underlying platform security control. As an example, Lambda execution are authenticated, authorized and are made visible to the SREs via the common logging mechanism.</p> EC2 to access RDS MySQL <p>Create an outbound rule in the SG for the EC2 using TCP on port 3306 and the destination to be the SGP of RDS Create an inbound rule in the SG of RDS to allow TCP on port 3306 with source being the EC2's SG. There is no need to specify outbound rule for RDS, as security groups are stateful therefore the response from the DB servers would be allowed out of the DB servers.</p>"},{"location":"infra/security/#solution-design","title":"Solution Design","text":"<p>The most important questions to ask and address when designing AWS solution with security consideration are:</p> <ul> <li>who can access the resources in the account?</li> <li>what can access the resources, and how access is allowed?</li> </ul> <p>Address how to secure, web tier, app tier and database access. What firewalls to use, and where.</p> <p>For each compute resource, address access to monitoring and logging and network environment that contain those resources.</p> <p>Is there any Gateway needed?</p> <p>Finally consider data life cycle and access control over time. What are the compliance requirements? </p>"},{"location":"infra/security/#blocking-ip-address","title":"Blocking IP address","text":"<ul> <li> <p>At the VPC level, the best approach is to use Network ACL, when EC2 is exposed via public IP address, as illustrasted in the diagram below:</p> <p></p> <p>Use a denial rule in NACL for a specific IP address. Security group inbound rule specifies allowed only IP range (no deny rule), which should work, but then it blocks larger set of clients, so it will not be a solution for global application. Running a firewall on the EC2 will help, but it is more complex to administer.  </p> </li> <li> <p>If an ALB is added in the middle, then the EC2 security group will specify the Security Group of the ALB, and we are still using NACL:</p> <p></p> <p>ALB does connection termination, so client will not be able to attack EC2. ALB initiates a new connection to EC2.</p> <p>To add more control we can add a Web Application Firewall to add complex filtering on the IP @.</p> </li> <li> <p>Using a NLB, there will be no security group at the network load balancer level, so traffic reaches EC2s. In this case only the NACL rules will help to protect.</p> </li> <li> <p>When using CloudFront (outside of VPC), we need to add WAF, as NACL at the VPC level will not work, the ALB being connected to the CloudFront public IP address (via security group).</p> <p></p> <p>Geo restriction can be defined at the CloudFront level, to deny a complete country for example.</p> </li> </ul>"},{"location":"infra/security/#security-learn-more","title":"Security Learn more","text":"<ul> <li>Data Safe cloud checklist</li> <li>Top security myth - dispelled</li> <li>Security demos</li> </ul>"},{"location":"infra/storage/","title":"Storage","text":"<p>AWS storage services are grouped into three categories \u2013 block storage, file storage, and object storage.</p> Updates <p>Created Oct 2022 - Update 11/27/23</p>"},{"location":"infra/storage/#ec2-instance-store-volumes","title":"EC2 instance store volumes","text":"<p>This is a storage on volumes physically residing on the same host that provides the EC2 instance itself, acting as local disc drives, allowing to store data locally to that instance. The storage is ephemeral, so do not use it for valuable data. For rebooted instance the data are kept while stopped and terminated, data is lost. The cost is included in the use of EC2. It offer a very high I/O speed, millions read per s and 700k write IOPS. Not all instances support instance store volumes.</p> <p></p> IOPS 100/3000? <p>IOPS are a unit of measure representing input/output operations per second. 3000 is max IOPS  and 100 min. Consider the hardware capacity to get the maximum amount of data in a single I/O measured in KiB. I/O size is capped at 256 KiB for SSD volumes and 1,024 KiB for HDD. Some techno like EBS tries to merge physically contiguous operations: a 1,024 KiB will be 4 operations on SSD, or 8 small 32 KiB will be one operation. However on HDD, 8 random operations of 128 KiB I/O will be 8 operations. Throughput = IOPS x allocation unit size per operation. T= 3000 x 64KiB = 187 MB/s. If application uses smaller size requests than the allocation unit, then we can reach max T. Allocation unit size of EBS volume depends of the volume type.  For gp2 it is 4KiB. But on the cloud data storage on a modern hard drive is managed through logical block addressing with a default size to 4KiB. EBS advertises 512-byte sectors to the operating system to do the mappting between logical and physical sector. Block size selection impacts storage max capacity.</p> <p>If we need to run a high-performance database that requires 210,000 IOPS for its underlying filesystem, we need <code>instance store</code> and DB replication in place.</p> <p>Backup and replication are the user's responsability.</p>"},{"location":"infra/storage/#different-storage-technologies","title":"Different storage technologies","text":""},{"location":"infra/storage/#file-storage","title":"File Storage","text":"<p>File storage is ideal when we require centralized access to files that need to be easily shared and managed by multiple host computers. Typically, this storage is mounted onto multiple hosts, and requires file locking and integration with existing file system communication protocols.</p> <p>File storage systems are often supported with a network attached storage (NAS) servers.</p>"},{"location":"infra/storage/#block-storage","title":"Block Storage","text":"<p>Block storage splits files into fixed-size chunks of data called blocks that have their own addresses, which improves read access.</p> <p>Outside of the address, no additional metadata is associated with each block.</p> <p>Block storage in the cloud is analogous to direct-attached storage (DAS) or a storage area network (SAN).</p> <p>Amazon EC2 <code>instance store</code> provides temporary block-level storage for an instance.</p>"},{"location":"infra/storage/#object-storage","title":"Object Storage","text":"<p>Objects are stored in a flat structure instead of a hierarchy. We can store almost any type of data, and there is no limit to the number of objects stored, which makes it readily scalable.</p>"},{"location":"infra/storage/#amazon-elastic-block-storage-ebs","title":"Amazon Elastic Block Storage EBS","text":"<p>Elastic Block Store Volume is a network drive attached to the EC2 instance. It is locked to one AZ, and uses provisioned capacity in GBs and IOPS. It is HA, every write is replicated multiple times within the same AZ, and can be backed up to other AZ or Region. Below are the main attributes for a EBS volume.</p> <p></p> <ul> <li>Create a EBS while creating the EC2 instance and keep it. It is not deleted on EC2 shutdown.</li> <li>Existing EBS volume can be attached to a new EC2 instance, normally there is a 1 to 1 relation between volume and EC2 instance. Except for multi-attached EBS.</li> <li>The maximum amount of storage is 16 TB.</li> <li>Support encryption at rest and in transit using AES-256 encryption, with keys managed in KMS. Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted.</li> <li> <p>Creating EBS volume can be done as a standalone volume later attached to an instance. Below is an example of the type of parameters to enter:</p> <p></p> <p>Same with  <code>boto3</code> SDK:</p> <pre><code>volume = ec2.create_volume(\n    AvailabilityZone='us-west-2a',\n    Size=10,  # in GB\n    VolumeType='gp2',\n    MultiAttachEnabled=True,\n    Encrypted=False,\n    DryRun=True,\n    Iops=100, # IOPS provisioned for the volume, represents the rate at which the volume accumulates I/O credits for bursting\n)\n</code></pre> </li> <li> <p>Once logged to the EC2 instance, add a filesystem, mount to a folder and modify boot so the volume is mounted at start time. See below shell commands:</p> <pre><code># List existing block storage, verify our created storage is present\nlsblk\n# Verify file system type\nsudo file -s /dev/xdvf\n# Create a ext4 file system on the device \nsudo mkfs -t ext4 /dev/xvdb\n# make a mount point\nsudo mkdir /data\nsudo mount  /dev/xvdb /data\n# Add entry in /etc/fstab with line like:\n/dev/xvdb /data ext4 default,nofail 0 2\n</code></pre> </li> <li> <p>For cross-region replication we need to use Data Lifecycle Manager.</p> </li> <li>Backup is done by using snapshot, and we can rebuild a volume from a snapshot, in case of AZ failure.</li> <li>EC2 instance has a logical volume that can be attached to two or more EBS RAID 0 volumes, where write operations are distributed among them. It is used to increase IOPS without any fault tolerance. If one fails, we lost data. It could be used for database with built-in replication mechanism or for Kafka node.</li> <li>RAID 1 is for better fault tolerance: a write operation is going to all attached volumes. RAID 1 is also not recommended for use with EBS.</li> </ul> <p>RAID</p> <p>Creating a RAID 0 array allows to achieve a higher level of performance for a file system than on a single EBS volume. I/O is distributed across the volumes in a stripe. If we add a volume, we get the straight addition of throughput and IOPS. Loss of a single volume in the set results in a complete data loss for the array. The resulting size is the sum of the sizes of the volumes within it, and the bandwidth is the sum of the available bandwidth of the volumes within it.</p>"},{"location":"infra/storage/#volume-types","title":"Volume types","text":"<p>There are two type of volumes: SSD or HDD. SSD is to work with smaller blocks (like database, or boot volumes for EC2 instance), HD is for higher rate of throughput (like logging, streaming, big data,...)</p> <p>When creating EC2 instances, we can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard)</p> <ul> <li>gp2 or gp3: SSD, used for most workloads, it goes up to 16 TB at 16000 IOPS max  (3 IOPS per GB brustable up to 3000 IOPS).</li> <li>io 1 or io 2: critical app with large database workloads. max ratio 50:1 IOPS/GB. Min 100 iops and 4G to 16T. 99.9% durability and even 99.999% for io2. EBS Provisioned IOPS SSD (io2 Block Express) is the highest-performance SSD volume designed for business-critical latency-sensitive transactional workloads.</li> <li>st 1: HDD. Streaming workloads requiring consistent, fast throughput at a low price. For Big data, Data warehouses, Log processing. Up to 16 TiB. 99.9% durability.</li> <li>sc 1: throughput oriented storage.  500G- 16T, 500MiB/s. Max IOPs at 250. Used for cold HDD, and infrequently accessed data. 99.9% durability.</li> </ul> <p>Encryption has a minimum impact on latency. It encrypts data at rest and during snapshots.</p> <ul> <li>Provisioned IOPS (PIOPS) SSD: used for critical apps with sustained IOPS performance, even more than 16k IOPS.</li> </ul> Use case examples <ul> <li>App requires up to 400 GB of storage for temporary data that is discarded after usage. The application requires approximately 40,000 random IOPS to perform the work on file. =&gt; Prefer a SSD-Backed Storage Optimized (i2) EC2 instances to get more than 365,000 random IOPS. The instance store has no additional cost, compared with the regular hourly cost of the instance. Provisioned IOPS SSD (io1 or io2) EBS volumes can deliver more than the 40,000 IOPS that are required in the scenario. However, this solution is not as cost-effective as an instance store because Amazon EBS adds cost to the hourly instance rate. This solution provides persistence of data beyond the lifecycle of the instance, but persistence is not required in this use case.</li> <li>A database must provide at least 40 GiB of storage capacity and 1,000 IOPS. The most effective storage is gp2 with 334 GB storage: Baseline I/O performance for General Purpose SSD storage is 3 IOPS for each GiB. For 334 GiB of storage, the baseline performance would be 1,002 IOPS. Additionally, General Purpose SSD storage is more cost-effective than Provisioned IOPS storage.</li> </ul>"},{"location":"infra/storage/#snapshots","title":"Snapshots","text":"<p>EBS snapshots are incremental backups that only save the blocks on the volume that have changed after our most recent snapshot. Used to backup disk at any point of time of a volume and stores it on S3. Snapshot Lifecycle policy helps to create snapshot with scheduling it by defining policies. To move a volume to another AZ or data center we can create a volume from a snapshot.</p> <p>EBS snapshots can be used to create multiple new volumes, whether they\u2019re in the same Availability Zone or a different one.</p> <p>For a consistent snapshot of an EBS Volume, we need to ensure the application flushes any cached data to disk and no other write I/O is performed by the file system on that volume. Once that is taken care of, we can issue a snapshot command. The snapshot command needs only a couple of seconds to capture a point-in-time. We can start using the volume after this. The actual data backup happens in the background, and we don\u2019t have to wait for the data copy to complete. </p>"},{"location":"infra/storage/#ebs-multi-attach","title":"EBS Multi-attach","text":"<p>Only for io1 or io2 EBS type, a volume can be attached to multiple EC2 instances (up to 16) running in the same AZ. Each instance has full R/W permission.  The file system must be cluster aware.</p>"},{"location":"infra/storage/#data-lifecyle-manager","title":"Data lifecyle manager","text":"<p>Used to define EBS backup policies with backup schedule requirement definitions.</p> <ul> <li>Default policies back up all volumes and instances in a Region that do not have recent backups. Snapshot or EBS backed AMIs.</li> <li>Custom policies supports advanced features such as, fast snapshot restore, snapshot archiving, cross-account copying, and pre and post scripts. It uses resource tags to identify the resources to back up.</li> </ul> <p>Policy must be associated with an IAM role that has the appropriate permissions to create snapshot, AMI, describe to EC2 and EBS volume.</p> <p>Cross-account can go cross region too.</p>"},{"location":"infra/storage/#other-resources","title":"Other resources","text":"<ul> <li>Reduce EBS volume size.</li> </ul>"},{"location":"infra/storage/#s3-simple-storage-service","title":"S3 - Simple Storage Service","text":"<p>Amazon S3 is a managed service for high scaling, unlimited object storage. S3 allows people to store objects (files) in buckets (root directory), which must have a globally unique name (cross users, cross regions!). Buckets are defined at the region level. Object in a bucket, is referenced as a key (folder + filename )which can be seen as a file path in a file system. An account has a soft limit of 200 buckets. The max size for an object is 5 TB but big file needs to be uploaded in multi-part using 5GB max size.</p> <p>S3 has 11x9's high durability of objects as it uses replications across multiple AZ (At least 3). Service Availability varies with storage class, from 99.5% to 99.99%.</p> <p>S3 supports strong consistency for all operations with a read-after-write consistency.</p> <p>S3 supports versioning at the bucket level. So file can be restored from previous version, and even deleted file can be retrieved from a previous version.</p> <p>Within the S3 console we will see all buckets in one view (it is a global service). But the buckets are created within a region and are local to the region. </p>"},{"location":"infra/storage/#use-cases","title":"Use cases","text":"<ul> <li>Backup and restore.</li> <li>Disaster Recovery.</li> <li>Archive.</li> <li>Data lakes and big data analytics.</li> <li>Hybrid cloud storage: seamless connection between on-premises applications and S3 with AWS Storage Gateway.</li> <li>Cloud-native application data.</li> <li>Media hosting.</li> <li>Software delivery.</li> <li>Static website.</li> </ul> <p>GETTING started</p>"},{"location":"infra/storage/#s3-storage-classes","title":"S3 Storage classes","text":"<p>When uploading a document into an existing bucket, we can specify the storage class used to keep data over time. Different levels are offered with different cost and SLA.</p> <p></p> <p>Standard is the general purpose class, for low latency and high throughput use cases, but at higher cost. HA is supported by copying to different AZs.</p> <p>S3 automates the process of moving objects to the most cost-effective access tier based on access frequency.</p> <p>With Intelligent Tiering, depending on the data access patterns of objects, S3 will move the objects between two different tiers. It is used when the access frequency is not known.</p> <p>With One Zone Infrequent Access, there is a risk of data loss in the event of availability zone destruction, and some objects may be unavailable when an AZ goes down.  Standard IA has a separate retrieval fee.</p> <p>Amazon Glacier is for archiving, like writing to tapes. The pricing includes storage and object retrieval cost. This is the lowest cost to persist objects for long time period.  It can take up to several hours to gain access to objects. The structure uses vault and archives and no more bucket and folders.</p> <p>Glacier Deep Archive (also named Bulk) is the lowest cost storage option for long-term archival and digital preservation. Deep Archive may take several hours (from 12 to 48 hours) depending on the retrieval tier.</p> <p>We can transition objects between storage classes. For infrequently accessed object, move them to STANDARD_IA. For archive objects, that we don\u2019t need in real-time, use GLACIER or DEEP_ARCHIVE. Moving objects can be automated using a lifecycle configuration.</p> <p>At the bucket level, a user may define lifecycle rules for when to transition an object to another storage class.</p> <p></p> <p>and</p> <p></p> <p>To prevent accidental file deletions, we can setup MFA Delete to use MFA tokens before deleting objects.</p> <p>To improve performance, a big file can be split and then uploaded with local connection to the closed edge access and then use AWS private network to copy between buckets in different region. In case of unfinished parts, use S3 Lifecycle policy to automate old/unfinished parts deletion.</p> <p>Storage Class Analysis can continuously monitor the bucket and track how the objects are accessed over time. This tool generates detailed reports on the percentage of data retrieved and by age groups. We can use this report to manage lifecycle policies.</p>"},{"location":"infra/storage/#security-control","title":"Security control","text":"<ul> <li> <p>By default a bucket access is not public, see the <code>Block Public Access</code> settings. Access can be enforced at the account level and needs to be disabled at the account level, before doing it at the bucket level (Amazon S3 &gt; block public access settings for this account &gt; edit block public access settings for this account).</p> <p></p> </li> <li> <p>To control access with policies, we need to disable this setting, and then define Bucket policies.</p> </li> <li>S3 Bucket Policy: is a security policy defined in S3 console. It allows cross-account access control. Can be set at the bucket or object level.</li> <li>Explicit DENY in an IAM policy will take precedence over a bucket policy permissions.</li> <li> <p>Define policies from examples or using policy generator tool:</p> <p></p> <p>And copy paste the generated policy</p> <p></p> </li> <li> <p>By default, when another AWS account uploads an object to our S3 bucket, that account (the object writer) owns the object, has access to it, and can grant other users access to it through ACLs. Bucket owner can take ownership of all objects. It is recommended to disable ACL and use IAM identity and bucket policies. </p> </li> <li>Traffic coming from our VPC to our S3 bucket is going to the public internet. So to avoid that we need to add a VPC gateway endpoint to keep traffic within AWS network. See this EC2 to VPC endpoint gateway to S3 example.</li> <li> <p>Objects can also be encrypted, and different mechanisms are available:</p> <ul> <li>SSE-S3: server-side encrypted S3 objects using keys handled &amp; managed and own by AWS using AES-256 protocol must set <code>x-amz-server-side-encryption: \"AES256\"</code> header in the POST request to upload the file.</li> </ul> <p></p> <ul> <li>SSE-KMS: leverage AWS Key Management Service to manage encryption keys. Use <code>x-amz-server-side-encryption: \"aws:kms\"</code> header in POST request. Server side encrypted. It gives user control of the key rotation policy and audit trail with CloudTrail.</li> <li>SSE-C: when we want to manage our own encryption keys. Server-side encrypted. Encryption key must be provided in HTTPS headers, for every HTTPS request made. HTTPS is mandatory.</li> </ul> <p></p> <ul> <li>Client Side Encryption: encrypt before sending objects to S3. Only owner of the key can decrypt the objects.</li> </ul> </li> <li> <p>Encryption can be done at the bucket level, or using bucker policies to refuse any PUT calls on S3 object without encryption header.</p> </li> </ul>"},{"location":"infra/storage/#s3-website-hosting","title":"S3 Website hosting","text":"<p>We can have static website on S3. Once html pages are uploaded, setting the properties as static web site from the bucket. The bucket needs to be public, and have a security policy to allow any user to <code>GetObject</code> action. The URL may look like: <code>&lt;bucket-name&gt;.s3-website.&lt;AWS-region&gt;.amazonaws.com</code></p> <p></p> Publishing a mkdocs site to s3 after enabling public access <pre><code>mkdocs build\naws s3 sync ./site s3://jbcodeforce-aws-studies \n# The url is at the bottom of the bucket in the website under the Bucket website endpoint for example:\nhttp://jbcodeforce-aws-studies.s3-website-us-west-2.amazonaws.com\n</code></pre> <ul> <li>To securely serve static content for specific end-user group (premium for ex.) we can use CloudFront signed URL or signed cookie. Better to use signed URL for file based access, while cookies are to access multiple restricted files.</li> <li> <p>Cross Origin Resource Sharing CORS: The web browser requests won\u2019t be fulfilled unless the other origin allows for the requests, using CORS Headers <code>Access-Control-Allow-Origin</code>. If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers: this is done by adding a security policy with CORS configuration like:</p> <pre><code>&lt;CORSConfiguration&gt;\n    &lt;CORSRule&gt;\n        &lt;AllowedOrigin&gt;enter-bucket-url-here&lt;/AllowedOrigin&gt;\n        &lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;\n        &lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt;\n        &lt;AllowedHeader&gt;Authorization&lt;/AllowedHeader&gt;\n    &lt;/CORSRule&gt;\n&lt;/CORSConfiguration&gt;\n</code></pre> </li> </ul>"},{"location":"infra/storage/#s3-replication","title":"S3 replication","text":"<p>Once versioning enabled on source and target, a bucket can be replicated in the same region (SRR) or cross regions (CRR). S3 replication is done on at least 3 AZs. One AZ down does not impact S3 availability. The replication is done asynchronously. </p> <ul> <li>SRR is used for log aggregation for example, or for live replication between production and test environments</li> <li>CRR is used for compliance and DR or replication across AWS accounts. </li> <li>Delete operations are not replicated. But can be enabled with the advanced options.</li> <li>Use S3 Replication Time Control (S3 RTC) to replicate with a predictable time frame. S3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15 minutes.</li> <li>Must give proper IAM permissions to S3. When replication is set, only new objects are replicated. To replicate exiting objects use S3 Batch Replication.</li> <li>We can specify a different storage class for the replicas.</li> <li>Before objects transition from standard class to another class we need to store them for 30 days.</li> <li>S3 Batch Replication replicates existing objects before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication.</li> </ul> <p>The AWS S3 sync command uses the CopyObject APIs to copy objects between S3 buckets in same region. The sync command lists the source and target buckets to identify objects that are in the source bucket but that aren't in the target bucket. The command also identifies objects in the source bucket that have different LastModified dates than the objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of the object (previous versions aren't copied).</p> <p>Storage Lens provides a dashboard on all S3 activities and is automatically enabled.</p>"},{"location":"infra/storage/#s3-access-points","title":"S3 Access Points","text":"<p>Access points are named network endpoints that are attached to buckets that we can use to perform S3 object Get and Put operations. It simplify data access, as user can create unique access control policies for each access point to easily control access to shared datasets. It is possible to define one access point per application, or group of apps, which needs access to dataset.</p> <p></p> <p>Every access point is associated with a single bucket and contains a network origin control, and a Block Public Access control. We can attach IAM resource based policies directly to the S3 access points.</p> <p>We can have 10,000 access points per Region for each AWS account. An access point can reference a bucket in another account. An access point can be accessible from the internet or from a specific VPC, Amazon S3 rejects any request made to the access point that doesn't originate from that VPC. We can also block public access at the access point level.</p> <p>See S3 playground for example of access point definition.</p> <p>Access point and bucket, both, must permit access. Users would have a single URL to access their objects. Access points may have public access or not.</p> <p>An Access Point alias provides the same functionality as an Access Point ARN and can be substituted for use anywhere an S3 bucket name is normally used for data access.</p> <p>Access points support access only over HTTPS.</p> <p>No additional cost, and no latency impact.</p>"},{"location":"infra/storage/#s3-express-zone","title":"S3 Express Zone","text":""},{"location":"infra/storage/#other-features","title":"Other features","text":"<ul> <li>Secure FTP: server to let us send file via SFTP.</li> <li>Requester Pay: The requester (AWS authenticated) of the data pay for the cost of the request and the data download from the bucket, not the owner.</li> <li>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between our client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</li> <li>Pre-signed URL: share object with URL with temporary access. Can be done with the command: <code>aws s3 presign</code>. Up to 168 hours valid.</li> <li>S3 Select and Glacier Select: to retrieve a smaller set of data from an object using server-side SQL. Can filter by rows and columns. 80% cheaper and 400% faster as it uses less network transfer and less CPU on client side.</li> <li>Event Notifications: on actions like S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore. Can be combined with name filtering. Events may be sent to SNS, SQS, Lambda function, and EventBridge.</li> <li>Amazon Macie: is a machine learning security service to discover, classify and protect sensitive data stored in S3. </li> <li>S3 Object lock: to meet regulatory/compliance requirements of write once read many, storage. Set only on newly created bucket. Object Lock with a 5-year retention period would lock the objects and prevent the objects from being overwritten or deleted. In compliance mode, objects cannot be overwritten or deleted by any user, including the root user of the account, for the duration of the retention period. Use Legal Hold to prevent an object or its versions from being overwritten or deleted indefinitely and gives the ability to remove it manually.</li> <li>S3 Inventory used to audit and report on replication or encryption status. It can generate CSV, ORC, or Parquet files to list the objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or objects with a shared prefix.</li> <li>S3 Byte-Range Fetches: parallelize GET by requesting specific byte ranges. Used to speed up download or do partial download.</li> <li>S3 Batch operations: perform bulk operations on existing S3 objects with a single request. To get the list of object, use S3 Inventory. Could not integrate custom code.</li> <li>Server Access Logs: used for audit purpose to track any request made to S3 in the same region, from any account. Logs are saved in another bucket.</li> <li> <p>S3 Glacier Vault Lock: Adopt a Write Once Read Many model, by creating a Vault Lock Policy. Data will never be deleted.</p> </li> <li> <p>Multi-region access point offers a global S3 hostname that provides access to multiple S3 buckets across AWS Regions (1 bucket per region pe MRAP) with automatic routing and failover between buckets. Application requests made to a Multi-Region Access Point global endpoint use AWS Global Accelerator to automatically route over the AWS global network to the closest-proximity S3 bucket with an active routing status. Each access point has an automatically-generated and globally unique alias, and an ARN. We can configure your Amazon S3 Multi-Region Access Point to be in an active-active state or active-passive state. To access a MRAP from a private subnet we need to configure a VPC interface endpoint so the traffic is routed to the MRAP with PrivateLink: a DNS query from an EC2 instance within the subnet(s) for <code>accesspoint.s3-global.amazonaws.com</code> should resolve to an IP address within that subnet.</p> </li> </ul> <p>See Multi-region access point tutorial. </p>"},{"location":"infra/storage/#s3-faq","title":"S3 FAQ","text":"The last one MB of each file in a bucket contains summary information that we want to expose in a search, what function to use? <p>Byte-Range fetch allows to read only a portion of data from the object. Since the summary is a small part of each object, it is efficient to directly read the summary rather than downloading an entire object from S3.</p> Pricing factors <p>Frequency of access, storage cost, retrieval cost and retrieval time. The S3 Intelligent Tiering automatically changes storage class depending on usage to optimize cost. S3 lifecycle is based on age and can be defined with rules. See how to reduce S3 costs.</p> Expected performance? <p>S3 automatically scales to high request rates and latency around 100 to 200ms. 5500 GET/HEAD requests per s per prefix in a bucket. 3500 PUT/COPY/POST/DELETE. When uploading files from internet host, it is recommended to upload to AWS edge location and then use AWS private backbone to move file to S3 bucket in target region. This will limit internet traffic and cost.</p> How to be informed if an object is restored to S3 from Glacier? <p>The Amazon S3 notification feature enables us to receive notifications when certain events happen in the bucket. To enable notifications, we must first add a notification configuration that identifies the events we want Amazon S3 to publish and the destinations where we want Amazon S3 to send the notifications.</p> How to upload local file to s3 bucket using CLI? <p>We can use python and the boto3 library or <code>aws s3</code> CLI. Be sure to have a IAM user (e.g. s3admin) with <code>AmazonS3FullAccess</code> managed policy. The aws config may have added access Key and secret, may be in a dedicated profile.</p> <pre><code>aws s3 cp $PWD/companies.csv s3://jb-data-set/ --profile s3admin\n</code></pre> <p>For boto3 example see code under big-data-tenant-analytics / copyToS3.py  project.</p> How to retrieve in minutes up to 250MB of archive from Glacier? <p>Expedited retrievals allow to quickly access (1 to 5 minutes) the data when occasional urgent requests for a subset of archives are required. It provides up to 150 MB/s of retrieval throughput. If we require access to Expedited retrievals under all circumstances, we must purchase <code>provisioned retrieval capacity</code>.</p> What is provisioned capacity in Glacier? <p>Provisioned capacity ensures that our retrieval capacity for expedited retrievals is available when we need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput.</p> Run S3 locally <p>See quarkiverse S3 guide using Localstack image. </p> <pre><code>docker run --rm --name local-s3 -p 8008:4566 -e SERVICES=s3 -e START_WEB=0 -d localstack/localstack\naws configure --profile localstack\naws s3 mb s3://quarkus.s3.quickstart --profile localstack --endpoint-url=http://localhost:8008\n</code></pre>"},{"location":"infra/storage/#amazon-appflow","title":"Amazon AppFlow","text":"<p>AppFlow offers a fully managed service for easily automating the bidirectional exchange of data to SaaS vendors from AWS services like Amazon S3. This helps avoid resource constraints.</p>"},{"location":"infra/storage/#elastic-file-system-efs","title":"Elastic File System (EFS)","text":"<ul> <li>Fully managed NFS (4.1 and 4.0) file system, like a NAS or SAN on-premises. </li> <li>Only Linux based AMI. POSIX filesystem. AMI Linux v2 has the necessary NFS client. </li> <li> <p>EC2 accesses file system via mount points. EFS can scale to petabytes in size, with low latency access.</p> </li> <li> <p>Support different performance mode, like max I/O or general purpose</p> </li> <li> <p>When creating a file system we need to select a storage class that could be regional (multi AZs) or One Zone.</p> <p></p> </li> <li> <p>To access EFS in a VPC, create one or more mount targets in the VPC. The target properties include and ID, the subnet ID in which it is created, the file system ID for which it is created, an IP address at which the file system may be mounted, VPC security groups, and the mount target state. The IP @ is maaped to a DNS name (e.g. <code>file-system-id.efs.aws-region.amazonaws.com</code>), one in each AZ. All EC2 instances in that Availability Zone share the mount target.It is defined in a subnet, so the EC2 needs to specify in which subnet it runs. Use Amazon EFS tool in each EC2 instance to mount the EFS to a target mount point.</p> </li> <li> <p>EFS can be accessed from on-premises servers via Direct Connect and Virtual Private gateway</p> <p></p> </li> </ul> <p>(3x gp2 cost), controlled by using security group. This security group needs to add in bound rule of type NFS connected / linked to the SG of the EC2.</p> <ul> <li>Encryption is supported using KMS.</li> <li>1000 concurrent clients.</li> <li>10GB+/s throughput. Different model: Elastic, bursting or provisioned. It grows to petabyte.</li> <li>Billed for what we use.</li> <li>Is integrated with IAM so user need to have credentials to create file systems, and role needs to be used for NFS clients.</li> <li>Support storage tiers to move files after n days. Infrequent access (IA) used for access few times a quarter, where Archive storage is for few times a year. so lifecycle management move file to EFS-IA.</li> <li>EFS provides the close-to-open consistency semantics: write operations, in a region, are durably stored across AZs, if the app uses synchronous write, or if it closes a file.But applications that perform synchronous data access and perform non-appending writes have read-after-write consistency for data access.</li> <li>use AWS DataSync to transfer files from an existing file system to EFS</li> <li>Can be used with AWS Backup.</li> </ul> <p>The FAQ for multi AZs support within a region.</p>"},{"location":"infra/storage/#snowball","title":"Snowball","text":"<p>Move TB to PB of data in and out AWS using physical device to ship data as doing over network will take a lot of time, and may fail. To get the is setup it takes multiple day between ordering, hardware delivery, and get the device back.</p> <ul> <li> <p>The Snowball Edge device has 100TB and compute power to do some local data processing.  With Compute Optimized version there are 52 vCPUs, 200GB of RAM, optional GPU, 42TB capacity. And for Storage Optimized version, 40 vCPUs,, 80 GB RAM, and object storage clustering. We can use Snowball Edge Storage Optimized if we have a large backlog of data to transfer or if we frequently collect data that needs to be transferred to AWS and the storage is in an area where high-bandwidth internet connections are not available or cost-prohibitive.</p> </li> <li> <p>Snowcone: smaller portable, secured, rugged, for harsh environments. Limited to 8TB. We can use AWS DataSync to sned data. 2 CPUs, 4GB of mem.</p> </li> <li>SnowMobile is a truck with 100 PB capacity. Once on site, it is transferred to S3.</li> </ul> <p>Can be used for Edge Computing when there is no internet access.</p> <ul> <li>All can run EC2 instances and AWS lambda function using AWS IoT Greengrass.</li> <li>For local configuration of the Snowball, there is a the AWS OpsHub app.</li> </ul>"},{"location":"infra/storage/#fsx","title":"FSx","text":"<p>A managed service for file system technology from 3nd party vendors like Lustre, NetApp ONTAP, Windows File Server, OpenZFS...</p> <ul> <li>For Windows FS, supports SMB protocol and NTFS. Can be mounted to Linux EC2. 10s GB/s IOPS and 100s PB of data. Can be configured on multi AZ.</li> <li>Data is backed-up on S3</li> <li>Lustre is a linux clustered FS and supports High Performance Computing, POSIX... sub ms latency, 100s GB/s IOPS.</li> <li>NetApp ONTAP: supports NFS, SMB, iSCSI protocols. Supports storage auto scaling, and point in time instantaneous cloning.</li> <li>OpenZFS compatibles with NFS, scale to 1 million IOPS with &lt; 0,5ms latency, and point in time instantaneous cloning. </li> </ul> <p>Deployment options:</p> <ul> <li>Scratch FS is used for temporary storage with no replication. Supports High burst</li> <li> <p>Persistent FS: long-term storage, replicaed in same AZ</p> </li> <li> <p>FSx Lustre FAQ</p> </li> </ul>"},{"location":"infra/storage/#aws-storage-gateway","title":"AWS Storage Gateway","text":"<p>AWS Storage Gateway exposes an API in front of S3 to provide on-premises applications with access to virtually unlimited cloud storage. It is a VM as a software appliance. </p> <p>Three gateway types:</p> <ul> <li> <p>file: FSx or S3 . </p> <ul> <li>S3 buckets are accessible using NFS or SMB protocols, so mounted in a filesystem. Controlled access via IAM roles. File gateway communicates with AWS via HTTPS, with SSE-S3 encryption.</li> <li>FSx, storage gateways brings cache of last accessed files.</li> </ul> </li> <li> <p>volume: this is a block storage using iSCSI protocol. On-premise and visible as a local volume, backed up asynchronously to S3. Two volume types:</p> <ul> <li>Cached volumes: The primary data is stored in Amazon S3 while the frequently acccessed data is retained locally in the cache for low-latency access. Each volume is up to 32 TB, and the total capacity can be up to 1024TB per cached volume gateway. </li> <li>Stored volumes: the entire dataset is stored locally while also being asynchronously backed up to Amazon S3. It uses EBS snapshot uploaded to S3 via HTTPS, with incremental updates.</li> </ul> </li> <li> <p>virtual tape: same approach but with virtual tape library. Can go to S3 and Glacier. Works with existing tape software. Support 1500 virtual tape. each gateway-VTL is preconfigured with a media changer and tape drives, which are presented as iSCSI device.</p> </li> <li>Hardware appliance to run a Storage gateway in the on-premises data center. </li> </ul>"},{"location":"infra/storage/#transfer-family","title":"Transfer Family","text":"<p>To transfer data with FTP, FTPS, SFTP protocols to AWS Storage services like S3, EFS</p>"},{"location":"infra/storage/#datasync","title":"DataSync","text":"<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between on-premises storage systems and AWS Storage services, as well as between AWS Storage services.</p> <p>Move a large amount of data to and from on-premises (using agent) to AWS, or to AWS to AWS different storage services.</p> <p>Can be used for:</p> <ul> <li>Data Migration with automatic encryption and data integrity validation.</li> <li>Archive cold data.</li> <li>Data protection.</li> <li>Data movement for timely in-cloud processing.</li> <li>Support S3, EFS, FSx for Windows.</li> <li>making an initial copy of the entire dataset and schedule subsequent incremental transfers of changing data towards Amazon S3. Enabling S3 Object Lock prevents existing and future records from being deleted or overwritten.</li> </ul> <p>Replication tasks can be scheduled.</p> <p>It keeps the metadata and permissions about the file. </p> <p>One agent task can get 10 GB/s</p> <p></p> <p>As an example to support the above architecture, we can configure an AWS DataSync agent on the on-premises server that has access to the NFS file system. Transfer data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private Virtual InterFace (VIF). Set up a DataSync scheduled task to send the video files to the EFS file system every 24 hours.</p> <p>The DataSync agent is deployed as a virtual machine that should be deployed on-premises in the same LAN as the source storage to minimize the distance traveled.</p> <ul> <li>Transferring files from on-premises to AWS and back without leaving our VPC using AWS DataSync.</li> <li>DataSynch FAQ.</li> <li>Public and Private interface.</li> </ul>"},{"location":"infra/storage/#storage-comparison","title":"Storage comparison","text":"<ul> <li>S3: Object Storage.</li> <li>Glacier: Object Archival.</li> <li>EFS: When we need distributed, highly resilient storage, using Network File System for Linux instances, POSIX filesystem. Across multiple AZs.</li> <li>FSx for Windows: Network File System for Windows servers. Central storage for Windows based applications.</li> <li>FSx for Lustre: High Performance Computing (HPC) Linux file system. It can store data directly to S3 too.</li> <li>FSx for NetApp: High OS compatibility.</li> <li>FSx for OpenZFS: for ZFS compatibility.</li> <li>EBS volumes: Network storage for one EC2 instance at a time.</li> <li>Instance Storage: Physical storage for our EC2 instance (high IOPS). But Ephemeral.</li> <li>Storage Gateway: File Gateway, Volume Gateway (cache &amp; stored), Tape Gateway.</li> <li>Snowcone, Snowball / Snowmobile: to move large amount of data to the cloud, physically.</li> <li>Database: for specific workloads, usually with indexing and querying.</li> <li>DataSync: schedule data sync from on-premises to AWS or AWS to AWS services.</li> </ul>"},{"location":"kinesis/","title":"Kinesis services","text":"<p>Updates</p> <p>Created 12/2022 - Updated 12/04/2023</p>"},{"location":"kinesis/#streaming","title":"Streaming","text":"<p>Designed to process real-time streaming data. Streaming means unbounded data sets and continuous arrival.  The streaming computation involves processing recent data, with less memory needs compare to big data processing, with low latency.</p> <p>Stream storage uses append-only log semantic. Messages are immutables and could not be deleted in the stream.</p>"},{"location":"kinesis/#kinesis-basic","title":"Kinesis basic","text":"<p>Three main different components are:</p> <ul> <li>Kinesis Streams: low latency streaming ingest at scale. They offer patterns for data stream processing. It looks similar to Kafka, but MSK is the Kafka deployment.</li> <li>Amazon Managed Service for Apache Flink: perform real-time analytics on streams using SQL. This Apache Flink as managed service.</li> <li>Kinesis Firehose: load streams into S3, Redshift, ElasticSearch. No administration, auto scaling, serverless.</li> </ul>"},{"location":"kinesis/#kinesis-data-streams","title":"Kinesis Data Streams","text":"<p>It is a distributed data stream into Shards for parallel processing.</p> <p></p> <p>It uses a public endpoint and applications can authenticate using IAM role. Kinesis Data Streams is using a throughput provisioning model, a shard can inject 1 Mb/s or 1000 msg /s with an egress of 2Mb/s. Adding shards help to scale the throughput. A single shard supports up to 5 messages per second, so a unique consumer gets records every 200ms. Adding more consumers on the same shard, the propagation delay increases and throughput per consumer decreases. With 5 consumers, each receives 400kB max every second.</p> <p>Producer sends message with <code>Partition Key</code>. A sequence number is added to the message to note where the message is in the Shard.</p> <ul> <li>Retention from 1 to 365 days.</li> <li>Capable to replay the messages.</li> <li>Immutable records, not deleted by applications.</li> <li>Message in a shard, can share partition key, and keep ordering.</li> <li>It is possible to support exactly-once delivery.</li> <li>Producer can use SDK, or Kinesis Producer Library (KPL) or being a Kinesis agent.</li> <li>Kinesis Agent is a stand-alone Java app that offers an easy way to collect and send data to KDS, as it monitors a set of files. It runs on RHEL, or Amazon Linux 2.</li> <li>Consumer may use SDK and Kinesis Client Library (KCL), or being one of the managed services like: Lambda, Kinesis Data Firehose, Managed Service for Apache Flink.</li> <li>For consuming side, each Shard gets 2MB/s out.</li> <li>If consumer fails on one record in a batch, the entire batch (and thus the shard) is blocked until either the message processes successfully or the retention period for the data records in the batch expires.</li> <li>It uses enhanced fan-out if we have multiple consumers retrieving data from a stream in parallel. This throughput automatically scales with the number of shards in a stream.</li> <li>Pricing is per Shard provisioned per hour.</li> <li>The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. We should increase the number of shards within our data stream to provide enough capacity.</li> </ul> <p>There is an On-demand mode, pay as we go, with a default capacity of 4MB/s or 4000mg/s. Pricing per stream, per hour and data in/out per GB.</p> <p>Captured Metrics are:</p> <ul> <li>number of incoming/outgoing bytes,</li> <li>number incoming/outgoing records,</li> <li>Write / read provisioned throughput exceeded,</li> <li>iterator age ms.</li> </ul>"},{"location":"kinesis/#deployment","title":"Deployment","text":"<p>Using CDK, see example in cdk/kinesis, but can be summarized as:</p> <pre><code>from aws_cdk import (\n    aws_kinesis as kinesis\n)\n\nkinesis.Stream(self, \"SaaSdemoStream\",\n    stream_name=\"bg-jobs\",\n    shard_count=1,\n    retention_period=Duration.hours(24)\n)\n</code></pre> <p>Using CLI:</p> <pre><code>aws kinesis create-stream --stream-name ExampleInputStream --shard-count 1 --region us-west-2 --profile adminuser\n</code></pre>"},{"location":"kinesis/#producer","title":"Producer","text":"<p>Producer applications are done using Kinesis Producer Library (KPL) and they can batch events, and perform retries. Internally KPL uses queue to bufferize messages.  Example of python code using boto3 and KPL:</p> <pre><code>STREAM_NAME = \"companies\"\nmy_session = boto3.session.Session()\nmy_region = my_session.region_name\nkinesis_client = boto3.client('kinesis',region_name=my_region)\n\ndef sendCompanyJson():\n    company={\"companyID\" : \"comp_4\",\n            \"industry\": \"retail\",\n            \"revenu\": 29080,\n            \"employees\": 14540,\n            \"job30\": 4,\n            \"job90\":13,\n            \"monthlyFee\": 460.00,\n            \"totalFee\": 1172.00\n    }\n    companyAsString =json.dumps(company)\n    kinesis_client.put_record(\n                StreamName=STREAM_NAME,\n                Data=companyAsString,\n                PartitionKey=\"partitionkey\")\n</code></pre>"},{"location":"kinesis/#aws-cli","title":"AWS CLI","text":"<p>Produce:</p> <pre><code>aws kinesis put-record --stream-name test --partition-key user1 --data \"user signup\" --cli-binary-format raw-in-base64-out\n</code></pre>"},{"location":"kinesis/#consumer","title":"Consumer","text":""},{"location":"kinesis/#aws-sdk","title":"AWS SDK","text":"Warning <p>To be done </p>"},{"location":"kinesis/#aws-cli_1","title":"AWS CLI","text":"<p>Consume:</p> <pre><code># Describe the stream\naws kinesis describe-stream --stream-name test\n# Get some data\naws kinesis get-shard-iterator --stream-name test --shard-id shardId--00000000 --shard-iterator-type TRIM_HORIZON\n# The returned message gave the next message iterator that should be used in the next call.\naws kinesis get-records --shard-iterator &lt;the-iterator-id&gt;\n</code></pre>"},{"location":"kinesis/#kinesis-data-firehose","title":"Kinesis Data Firehose","text":"<p>Firehose is a fully managed service for delivering real-time streaming data to various supported destinations.</p> <p></p> <ul> <li>Firehose buffers incoming streaming data to a certain size (MBs) or for a certain period of time (in s) before delivering it to destinations.</li> <li>We create delivery stream, for sources that are most of the time logs from the different AWS services. Transformations are done in Lambda function.</li> <li>It is tuned to stream to the target destination, as simpler to configure than doing custom solution on top of Kinesis Data Streams or MSK.</li> <li>Data Firehose does not guarantee order delivery and exactly once semantic.</li> <li>It can delegates the record transformation processing to a custom Lambda function, but it supports different format already. It tries the invocation 3 times and then skips the batch of records. Records that failed to process are delivered to an Amazon S3 bucket in a processing_failed folder.</li> <li>It outputs batch files to the target destinations. Batch is based on 60s (or more) window or 1 MB of data. Therefore it is a near real-time service.</li> <li>As a managed services it also supports auto scaling. The number of shards is managed by the service based on data volume.</li> <li>IAM role needs to be referenced to write to S3.</li> <li>Firehose uses S3 to backup all or failed only data that it attempts to deliver to the chosen destination.</li> <li>Use an interface VPC endpoint to keep traffic between our VPC and Kinesis Data Firehose from leaving the Amazon network.</li> </ul>"},{"location":"kinesis/#managed-service-for-apache-flink","title":"Managed Service for Apache Flink","text":"<p>This is a managed service to transform and analyze streaming data in real time using Apache Flink. It can consume records from different sources, and in this demonstration we use Kinesis Data Streams.</p> <p></p> <p>The underlying architecture consists of a Job Manager and n Task Managers.</p> <ul> <li>The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers</li> <li>The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn.</li> <li>A Task slot is the unit of work executed on CPU.</li> <li>The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. </li> </ul> <p>The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager</p> <p></p> <p>See my Flink studies</p>"},{"location":"kinesis/#when-to-choose-what","title":"When to choose what","text":"<p>As Apache Flink is an open-source project, it is possible to deploy it in a Kubernetes cluster, using Flink operator. This will bring us with the most flexible solution as we can select the underlying EC2 instances needed, to optimize our cost. Also we will have fine-grained control over cluster settings, debugging tools and monitoring.</p> <p>While Managed Service for Apache Flink helps us to focus on the application logic, which is not simple programming experience, as stateful processing is challenging, there is no management of infrastructure, monitoring, auto scaling and high availability integrated in the service.</p> <p>In addition to the AWS integrations, the Managed Service for Apache Flink libraries include more than 10 Apache Flink connectors and the ability to build custom integrations.</p>"},{"location":"kinesis/#considerations","title":"Considerations","text":"<p>When connecting to Managed Service for Apache Flink, we need to consider the number of shards and the constraint on the throughput to design the Flink application to avoid getting throttled. As introduced previously, with one Flink Application, we may need to pause around 200ms before doing the next GetRecords.</p>"},{"location":"kinesis/#deployment-flink-app-to-managed-service-for-apache-flink","title":"Deployment Flink App to Managed Service for Apache Flink","text":"Warning <p>To be done</p>"},{"location":"kinesis/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Amazon Kinesis Data Analytics for SQL Applications Developer Guide</li> <li>Getting started with example on how to create application with CLI.</li> <li>AWS Kafka and DynamoDB for real time fraud detection</li> <li>Real Time Streaming with Amazon Kinesis</li> <li>Mastering AWS Kinesis Data Streams, Part 1 very good article to keep in bookmark. </li> <li>Mastering AWS Kinesis Data Streams, Part 2 consumer side of the story.</li> </ul>"},{"location":"monitoring/cost/","title":"Cost Optimization","text":""},{"location":"monitoring/cost/#resource-tagging","title":"Resource tagging","text":"<p>Use cost allocation tags to track cost in reports. User-defined tags use the <code>user:</code> prefix, and the AWS-generated tag uses the <code>aws:</code>.</p> <p>Need to activate the tags in Billing and Cost console. </p> <p>The <code>awsApplication</code> tag will be automatically added to all resources that are associated with applications that are set up in AWS Service Catalog AppRegistry.</p>"},{"location":"monitoring/cost/#budgets","title":"Budgets","text":"<p>Create a budget and send alarms when costs exceed the budget. We can use 4 types of budget: Usage, Cost, Reservation and Saving Plans.</p> <p>Budget Actions can run action on our behalf when a budget exceeds cost or usage threshold. Actions can be to apply an IAM policy to a user, group or role, to apply a Service Control Policy to an Organization Unit, or stop EC2 or RDS instances.</p> <p>Actions can be automatic or use a human workflow approval process.</p>"},{"location":"monitoring/cost/#cost-explorer","title":"Cost Explorer","text":"<p>Tool to view and analyze our costs and usage.</p>"},{"location":"monitoring/cost/#trusted-advisor","title":"Trusted Advisor","text":"<p>Trusted Advisor inspects our AWS environment, and then makes recommendations when opportunities exist for cost optimization, security, fault tolerance, performance, and service limits. Inspect all resources cross regions and aggregates are computed once a week. For full checks account needs to be in business or enterprise support plans. Check results can be consumed into CloudWatch. Priority helps us focus on the most important recommendations to optimize the cloud deployments, improve resilience, and address security gaps.</p> <p>The implication of the check will depend of the check type. Some of the items could be excluded.</p> <p>Example of check could be <code>low utilization of EC2 instances</code> when daily CPU was 10% or less in the last 14 days.</p> <p>User can filter out recommendations by resource tag, and can also exclude items. The Support API can be used to get check reports.</p> <p>The list of checks performed.</p> <p>TA cannot check for S3 object that are public inside a bucket, but can check if S3 bucket is public.</p>"},{"location":"monitoring/cost/#service-quotas","title":"Service Quotas","text":"<p>Service Quotas is an AWS service that helps you manage your quotas (also known as limits) for many AWS services in one location. Along with looking up the quota values and current utilization, you can request a quota increase from the Service Quotas console.</p>"},{"location":"monitoring/cost/#saving-plans","title":"Saving plans","text":"<p>New pricing plan to get discount. Commit to certain type of machine, on a specific region and usage. Anything above the usage will be billed with on-demand price. </p> <ul> <li>EC2 Instance Saving plan is flexible for the size, (large, XLarge...), OS and tenancy.</li> <li>Compute savings plan let change the instance family, region, compute type, OS and tenancy. It is like a convertible RI.</li> </ul>"},{"location":"monitoring/cost/#aws-compute-optimizer","title":"AWS Compute Optimizer","text":"<p>AWS Compute Optimizer allows to automate the collection of metrics for under-utilized and under-performing compute instances. It can then generate recommendations for you to save money.</p> <p>If we need to get Memory utilization, we need to deploy EC2 Cloudwatch agent to report on memory metrics to CloudWatch.</p>"},{"location":"monitoring/monitoring/","title":"Monitoring and Audit","text":""},{"location":"monitoring/monitoring/#cloudwatch","title":"CloudWatch","text":"<p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, and visualizes it using automated dashboards so we can get a unified view of our AWS resources, applications, and services that run in AWS and also on-premises. The basic features set:</p> <ul> <li>correlate the metrics and logs to better understand the health and performance of our resources.</li> <li>create alarms based on metric value thresholds or anomalous metric behavior based on machine learning algorithms. </li> <li>support grouping resources to manage at the application / solution level. </li> </ul>"},{"location":"monitoring/monitoring/#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<p>CloudWatch provides metrics for every services in AWS. Metric represents a variable to measure like CPU utilization, Network inbound traffic... </p> <p>Metrics are within a VPC and belong to a namespace. They have timestamps and up to 10 attributes or dimensions.</p> <p></p> <ul> <li>To monitor our EC2 instance memory and disk usage... we need to use a Unified CloudWatch Agent to push memory usage as a custom metric to CW. </li> <li>When creating an EC2 instance we need to enable <code>Detailed CloudWatch monitoring</code>.</li> <li>Status Check metrics give health of the system (AWS infrastructure like the host server) and instance (guest, software and network). Status check metric is use by ELB so it can stop traffic to an unhealthy EC2 instance. Auto Scaling group uses it too to restart EC2.</li> </ul>"},{"location":"monitoring/monitoring/#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<p>Alarms are used to trigger notification from any metrics. The states of an alarm are: OK, INSUFFICIENT_DATA, ALARM. A period specifies the length of time in seconds to evaluate the metric: the evaluation period. </p> <ul> <li>Alarms invoke actions for sustained state changes only. The state must have changed and been maintained for a specified number of periods</li> <li> <p>Alarm is associated with one metric. A metric alarm watches a single CloudWatch metric or the result of a math expression based on CloudWatch metrics. The alarm performs one or more actions based on the value of the metric or expression relative to a threshold over a number of time periods. Below the alarm applies to ApiGateway call metrics, over a period of 5 minutes. </p> <p></p> </li> <li> <p>The target of the alarm may be to stop, reboot, terminate, recover of an EC2 instance, trigger an Auto Scaling Action for EC2, or send notification to SNS. </p> </li> <li>We need to specify the number of datapoints within the evaluation period that must be breaching to cause the alarm. This is used to avoid reacting to spike. </li> <li>Notifications are sent to SNS and to an email recipient. It is possible to also define automatic actions like auto scaling, specific EC2 action, create a ticker or create an incident in System Manager.</li> <li> <p>CloudWatch Alarms are designed to invoke only the alarm actions when a state change happens. Here is an example of alarm set on total number of api call:</p> <p></p> </li> <li> <p>We can also combine outcome of two alarms using the CloudWatch Composite Alarm.</p> </li> <li>A composite alarm includes a rule expression that takes into account the alarm states of other alarms that we have created. The composite alarm goes into ALARM state only if all conditions of the rule are met.</li> </ul> <p>We can use the stop or terminate actions to help us save money when we no longer need an instance to be running. We can create an CloudWatch alarm that monitors an EC2 instance and automatically reboots the instance. The reboot alarm action is recommended for Instance Health Check failures. While if a System Status Check fails use the Recover the instance to find another host server for this EC2.</p>"},{"location":"monitoring/monitoring/#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>Concepts:</p> <ul> <li> <p>Log groups to groups logs, representing an application.</p> <p></p> </li> <li> <p>Log streams: instances within an application / log files or containers.</p> <p></p> </li> </ul> <p>Priced for retention period, so expiration policies can be defined. </p> <p>CloudWatch can send logs to S3, Kinesis Firehose, Kinesis Data Streams, Lambda,... </p> <p>Can define filters to reduce logs or trigger CloudWatch alarms, or add insights to query logs and for Dashboards. Here are simple filter:</p> <p></p> <p>Use Subscription Filter to get near real-time logs to targeted sink:</p> <p></p> <p>Logs Insights helps to define query to search within the logs. </p>"},{"location":"monitoring/monitoring/#cloudwatch-agent","title":"CloudWatch Agent","text":"<p>By default EC2 instances do not send logs to CloudWatch. We need to run agent on EC2 to push log files we want. We need to use an IAM role that let the EC2 instance be able to send logs to CW.</p> <p>The new Unified Agent send logs and system-level metrics. </p> <p>For memory usage, disk swap, disk space, page file usage, and log collection we need the CloudWatch Agent.</p>"},{"location":"monitoring/monitoring/#cloudwatch-event","title":"CloudWatch Event","text":"<p>Is now EventBridge, a more generic event-driven, serverless managed service.</p>"},{"location":"monitoring/monitoring/#cloudwatch-insight","title":"CloudWatch Insight","text":"<p>CloudWatch Container Insights collects, aggregates, and summarizes metrics and logs from our containerized applications and microservices. Available for ECS, EKS, K8S on EC2s. </p> <p>CloudWatch Lambda Insights simplifies the collection, visualization, and investigation of detailed compute performance metrics, errors, and logs to isolate performance problems and optimize our Lambda environment.</p> <p>Application Insight is to set up monitoring and gain insights to our application health so we can quickly detect and diagnose problems and reduce the mean time to resolution</p> <p>Contributor Insights allows us to create real time Top N time series reports by analyzing CloudWatch Logs based on rules we define. The rule matches log events and reports the top Contributors, where a \"Contributor\" is a unique combination of the fields defined in the rule. It can be used to identify the heaviest network users, find the URLs that generate the most erroes.</p> <ul> <li>EKS workshop with CloudWatch container insight</li> </ul>"},{"location":"monitoring/monitoring/#synthetic-canaries","title":"Synthetic canaries","text":"<p>Canaries are Nodejs or Python scripts that run on a schedule, to monitor (availability and latency) our endpoints and APIs. They follow the same routes and perform the same actions as a customer. They work on HTTP or HTTPs. </p> <p>An example of creating a canary for API endpoint:</p> <p></p> <p>The API is in the API Gateway so select the domain and stage:</p> <p></p> <p>Once started the canary show if it passes or fails the API call.</p> <p></p>"},{"location":"monitoring/monitoring/#cloudwatch-deeper-dive","title":"CloudWatch Deeper Dive","text":"<ul> <li>CloudWatch Product documentation</li> <li>CloudWatch CDK sample: CDK Python Backup &amp; Cloudwatch event to illustrate a cloudwatch event rule to stop instances at UTC 15pm everyday</li> <li>CloudWatch Container Insights for EKS cluster workshop</li> <li>AWS CloudWatch sample: repeats an alarm with Event Bridge has some CDK examples of alarm, cw groups.. but very bad implementation as Step function and lambda code are inside the CDK code. </li> </ul>"},{"location":"monitoring/monitoring/#aws-cloudtrail","title":"AWS CloudTrail","text":"<p>A managed service to provides governance, audit capabilities for all activities (API calls) and events within an Account. Can be across regions and accounts on a single, centrally controlled platform. We can use CloudTrail to detect unusual activity in our AWS accounts.</p> <p></p> <p>By default, trails are configured to log management events (operations performed on AWS resources). Data events are not logged.</p> <p>This is a usage-based paid service.</p> <p>CloudTrail Insight is used to detect unusual activity in AWS account.</p>"},{"location":"monitoring/monitoring/#aws-config","title":"AWS Config","text":"<p>Record and evaluate our AWS resource configurations and changes over time against compliance rules. For example can be used to continuously monitor our EC2 instances to assess if they have a specific port exposed.</p> <p>It is a pre-region service. But data can be aggregated across regions and accounts.</p> <p>The following screen shots help to understand the type of configuration that needs to be done to use AWS Config:</p> <p></p> <p>Configuration snapshots and configuration history files are delivered to S3 bucket, and configuration change notifications are delivered through SNS. Can send events to EventBridge.</p> <p>AWS Config rules are used to evaluate compliance to best practices, and remediate using AWS Systems Manager Automation documents. This is done at the Action level of a config rule. There are a lot of pre-built rules to select from:</p> <p></p> <p>Custom rule has to be defined in Lambda function.</p> <p>We can leverage an AWS Config managed rule to check if any ACM certificates in our account are marked for expiration within the specified number of days.</p> <p>Rules are not here to prevent actions to be done on resources, but notifies in case of non-compliance.</p> <p>Combined with CloudTrail, we know who did the change to a configuration.</p> <p>See tutorialsdojo summary.</p>"},{"location":"monitoring/monitoring/#putting-them-together","title":"Putting them together","text":"<p>If we define an Elastic Load Balancer then, </p> <ul> <li>CloudWatch will help us to monitor incoming connections metric, visualize error codes as % over time, and supports dashbaord to get performance monitoring.</li> <li>Config will help us to track security group rules, configuration changes done on the load balancer, as well as defining compliance rules to ensure SSL certificates are always assigned to LB.</li> <li>CloudTrail tracks who made any changes to the configuration with API calls.</li> </ul>"},{"location":"playground/","title":"A set of personal labs to go deeper on some AWS services","text":""},{"location":"playground/#aws-cloud9","title":"AWS Cloud9","text":"<p>Try to use AWS Cloud9 to do most of the labs, as it avoids creating internet endpoints. Some helpful tricks:</p> <ul> <li>Verify the environment with <code>aws  sts get-caller-identity</code></li> <li>Get access to a web app</li> </ul>"},{"location":"playground/#iam-organization-security","title":"IAM - Organization - Security","text":"<ul> <li>User assuming a new role demonstration to access EC2 using trusted relationship, and IAM policies.</li> <li>IAM and AWS Organization for cross account access in separate git repository.</li> <li>An Attribute based access control tutorial, in labs/security/iam/abac folder with cdk to do the tutorial.</li> </ul>"},{"location":"playground/#cloudformation","title":"CloudFormation","text":"<ul> <li>A Redis Server in a VPC, public subnet. See labs/networking.</li> <li>EC2 to run docker and docker compose code based on articles like this one and video</li> </ul>"},{"location":"playground/#cdk-play","title":"CDK play","text":"<ul> <li>A complete app with Lambdas, API Gateway, DynamoDB table demonstrating how to chain lambda functions. It is part of a AWS cdk workshop. One Lambda is a basic hello world and the second one is counting each request to hello and increase a counter in DynamoDB. The hit count function is exposed as part of API Gateway <code>/</code> path. Once receive the event, it delegates to hello function to get the response, but update the dynamodb table before that. It also uses cdk-dynamo-table-view to view the content of a table. There are also access control to authorize the hello lambda to call the </li> <li> <p>The ec2-vpc folder supports the following definitions:</p> <p></p> </li> <li> <p>ec2-basic: use API to get reference to the default VPC then create <code>t2.micro</code> EC2 instance to host Apache httpd as defined in a user_data script.</p> </li> <li> <p>cdk/kinesis</p> </li> <li>cdk for a python app on EC2 using user-data to start it</li> <li>Prompt-engineering in llm-ref-arch-demo repo: A CDK with an ALB to ECS Fargate service and task running a Streamlit app</li> </ul>"},{"location":"playground/#analytics","title":"Analytics","text":"<ul> <li> <p>EMR serverless demo to do product recommendations emr-serverless-demo and then with EMR cluster and EMR Studio/ Notebook. emr-ec2-demo</p> </li> <li> <p>EMR getting started in labs/analytics/emr-starting. The goal is to process food establishment inspection data.</p> </li> <li>In emr-cdk-analytics EMR cluster in a dedicated VPC, with S3 bucket to get scripts from it. IAM role on emr principal with a custom policy to read from the s3 bucket, so our EMR can access S3. Then a IAM role for the emr job, as ec2 principal. An instance profile is also used in the EMR cluster definition. The cluster includes Core node on EC2 reserved instance and spots for master node. It includes the script to be executed in a job.</li> <li> <p>Analytics/emr-serverless includes aws cli scripts to define emr cluster, and submit job using python script to count word in a text, uploaded to a s3 bucket. It also include a cloud formation for a cloud watch dashboard. </p> </li> <li> <p>Analytics/kinesis-getting-started: Writing to an Amazon S3 Bucket from Kinesis data analytics using AWS CLI.</p> </li> </ul>"},{"location":"playground/#athena","title":"Athena","text":""},{"location":"playground/#lambda","title":"Lambda","text":"<ul> <li>S3 file processing with a Lambda. </li> <li>A java lambda with CDK deployment to process weather record.</li> <li>Car rides generator with python Lambda</li> <li>Repo to illustrate Getting GitHub events to API gtw, Lambda and Slack in Python (SAM based)</li> </ul>"},{"location":"playground/#active-mq-and-messaging","title":"Active MQ and messaging","text":"<ul> <li>Repo aws-messaging-study</li> </ul>"},{"location":"playground/ecs/","title":"ECS playgrounds","text":"<p>Different examples of how to deploy apps on ECS cluster.</p> <p>See also ECS service summary.</p>"},{"location":"playground/ecs/#deploy-nginx-docker-image-on-fargate","title":"Deploy NGinx docker image on Fargate","text":"<p>This is using the AWS Console.</p> <ol> <li> <p>Create an ECS cluster, using Fargate runtime, and CloudWatch container insights.</p> <p></p> <p>As an alternate approach, use aws CLI or CDK/cloudformation as illustrated below.</p> </li> <li> <p>Ensure the VPC in which the ECS cluster is deployed, has a route to access to the port number of the app to deploy (port 8080 or 80) </p> </li> <li> <p>Add a task definition to use NGInx demo hello docker image</p> <p></p> <p>There is no need to specify environment variables in this demonstration.</p> <p>Specify the environment like EC2 or Fargate and then the CPU requirements.</p> <p>There is not need to add a <code>Task role</code> as the container is not accessing any AWS service via API (if not, specify a IAM role which has the necessary policies). No Storage needed neither.</p> <p>Enable <code>log collection</code> to CloudWatch, but no need to send trace to AWS X-Ray and metrics to AWS Distro.</p> <p></p> </li> <li> <p>A web app may need to scale horizontally, and in this case we need to create a service to expose it: Select the ECS cluster as target environment:</p> <p></p> <p>Then specify that the service will use more than one replica:</p> <p></p> <p>The deployment parameters control the % allocated during deployment:</p> <p></p> <p>Be sure to select the VPC where the ECS cluster is defined then the subnets, and a specific security group granting access on port 80 coming from the ALB security group:</p> <p></p> <p>As we want to facade with an ALB, we do not want to setup a public IP address with a ENI. </p> <p></p> </li> <li> <p>Get DNS name of the ALB or the  IP address</p> <ul> <li>If we created a public IP address then an ENI was created, and going to the ENI definition will let access to the DNS name, something like: <code>ec2-34-216-228-13.us-west-2.compute.amazonaws.com</code> </li> </ul> </li> </ol>"},{"location":"playground/ecs/#deploy-a-python-flask-app-using-aws-console-on-ecs-fargate","title":"Deploy a Python Flask app using AWS Console on ECS Fargate","text":"<ol> <li>Create a ECS cluster if not done already.</li> <li> <p>Build the docker image for the Flask App. If build on MAC M1|M2 CPU, use the platform parameter.</p> <pre><code>docker build --platform linux/amd64 -t j9r_flaskapp . \n</code></pre> </li> <li> <p>Upload the image to ECR repository (see commands here): be sure to match repository name with the docker image name</p> <p></p> </li> <li> <p>Create a task definition, mostly the same as in previous example, the image reference is the one in the private registry in the same AWS Account. Be sure to select a Task execution role with the <code>AmazonECSTaskExecutionRolePolicy</code> managed policies attached to it:</p> <p></p> Task role vs execution task role <p>ECS task roles allow the containers in the task to assume an IAM role to call AWS APIs without having to use AWS Credentials inside the containers. The trusted principals in the role is the <code>ecs-tasks.amazonaws.com</code> service.  ECS task execution roles grant the ECS agents permission to make AWS API calls who are responsible for managing the tasks in the cluster.</p> </li> <li> <p>As the task should run as a web application, we need to create a service from the task definition detail panel. Choose Capacity provider strategy for the computation deployment configuration, then <code>Replica</code> and one desired task, select the Default VPC and one or 2 subnets. Select an existing Security Group is the matching port is already expose (port http 80). No need to create an ALB for this deployment. Defining a service, builds a cloud formation template. After few minutes the service and task are running. The URL is the public IP address for the task. </p> <p></p> </li> <li> <p>Update the security group if needed to get HTTP port 80 open. Use the URL.</p> </li> <li>Stop the service.</li> </ol>"},{"location":"playground/ecs/#how-to-connect-to-an-amazon-ecs-container-using-aws-ecs-execute-command","title":"How to connect to an Amazon ECS container using AWS ECS execute command","text":"<p>It is possible to use tool like ecs-exec-checker to remote connect to a container started as a task in ECS.</p>"},{"location":"playground/ecs/#deploy-a-python-flask-app-with-cdk-ecs_patterns","title":"Deploy a Python Flask app with CDK ecs_patterns","text":"<p>See the folder: labs/cdk/ecs-fargate with a ECS cluster in the Default VPC, a Python flask application build and pushed to ECR in private repository. Below is the extract from the stack definition:</p> <pre><code>    vpc = ec2.Vpc.from_lookup(self, \"lookup\", is_default=True)     # default is all AZs in region\n\n    cluster = ecs.Cluster(self, \"MyCluster\", vpc=vpc)\n\n    ecs_patterns.ApplicationLoadBalancedFargateService(self, \"MyFargateService\",\n        cluster=cluster,            # Required\n        cpu=256,                    # Default is 256\n        desired_count=1,            # Default is 1\n        task_image_options=ecs_patterns.ApplicationLoadBalancedTaskImageOptions(\n            image=ecs.ContainerImage.from_asset(\"./server\")),\n        memory_limit_mib=512,      # Default is 512\n        public_load_balancer=True)\n</code></pre> <ul> <li> <p>Cluster created with the name of the stack:</p> <p></p> </li> <li> <p>ALB: As the CDK code used the ecs.patterns with an ALB, we can see the ALB declaration in AWS console. The target group being the ECS service. There is also a DNS A record created, and using the DNS URL should bring the Flask app hello world message.</p> <p></p> </li> <li> <p>The target group</p> </li> <li> <p>ECS Service </p> <p></p> </li> <li> <p>ECS Task Definition illustrates the two IAM roles created: 1/ to get ECS principals being able to call ECR, and writing to logs, and 2/ an execution role to write logs</p> <p></p> </li> </ul> Networking topology change on Fargate 1.4.0+ <p>There is only one ENI now per task. All traffic stays inside of our VPC, even the Fargate platform traffic, like accessing ECR and Secret Manager. To be able for the task to access ECR private registry, Secret Manager... we need to add a privatelink to those services. </p>"},{"location":"playground/ecs/#deploying-quarkus-app-with-cdk","title":"Deploying Quarkus App with CDK","text":"<p>See this aws-cdk-project-templates repository for a project template.</p> <ol> <li> <p>Create a CDK app, define a VPC, public, private subnets and routing inbound rule on the same port as the exposed port of the app (80 or 8080) in its security group, and add ECS cluster construct to the stack: The code below use a config dict to share some configuration values.</p> <pre><code> self.ecs_cluster: ecs.Cluster = ecs.Cluster(\n        self, \"ECSCluster\", \n        vpc=self.vpc, \n        cluster_name=self.config.get(\"ecs_cluster_name\")\n    )\n</code></pre> </li> <li> <p>Define an IAM role to let the application accessing other AWS Service, with the secure policies attached to it</p> </li> <li>Add a task definition</li> <li>Add a load balancer</li> </ol>"},{"location":"playground/ecs/#some-how-tos","title":"Some how-tos","text":"Deactivate a task <p>Select the task definition name to open a second panel with <code>Deregister</code> action.</p> Delete task definition <p>A task definition deletion request will not complete if there are any Amazon ECS resources that depend on the task definition revision. Select the task definition name, and in the second panel the delete option.</p>"},{"location":"playground/ecs/#other-examples","title":"Other examples","text":"<ul> <li>Deploying Quarkus native app with Postgresql Aurora</li> </ul>"},{"location":"playground/gettingstarted/","title":"Getting started","text":""},{"location":"playground/gettingstarted/#create-a-ssh-keypair","title":"Create a SSH KeyPair","text":"<p>A key pair, consisting of a public key and a private key. The public key is saved on the EC2 instance, during the first boot (under <code>~/.ssh/authorized_keys</code>).  See the product documentation.</p> <p>Using the CLI:</p> <pre><code>aws ec2 create-key-pair \\\n    --key-name my-ec2-key-pair \\\n    --key-type rsa \\\n    --key-format pem \\\n    --query \"KeyMaterial\" \\\n    --output text &gt; my-ec2-key-pair.pem\nchmod 0400 my-ec2-key-pair.pem\n\n# Verify the key\naws ec2 describe-key-pairs --key-names my-ec2-key-pair\n</code></pre> <p>When we create a new key pair using AWS CloudFormation, the private key is saved to AWS Systems Manager Parameter Store.</p> <pre><code>Resources:\n  NewKeyPair:\n    Type: 'AWS::EC2::KeyPair'\n    Properties: \n      KeyName: new-key-pair\n</code></pre> <p>Then get the reference </p> <pre><code>aws ec2 describe-key-pairs --filters Name=key-name,Values=new-key-pair --query KeyPairs[*].KeyPairId --output text\n</code></pre> <p>And <code>.pem</code> file</p> <pre><code>aws ssm get-parameter --name /ec2/keypair/&lt;REPLACE_WITH_RESULT_PREVIOUS_CMD&gt; --with-decryption --query Parameter.Value --output text &gt; new-key-pair.pem\n</code></pre> <p>To get the public key from a private key:</p> <pre><code>ssh-keygen -y -f /path_to_key_pair/new-key-pair.pem\n</code></pre>"},{"location":"playground/gettingstarted/#deploying-apache-http-on-ec2","title":"Deploying Apache HTTP on EC2","text":"<p>Using AWS console, create a EC2 t2.micro instance with AWS Linux, a public IP address, a security group with SSH enabled from anywhere and HTTP on port 80 accessible from the internet (0.0.0.0/0). Associate the EC2 with a Key Pair so we can do SSH on the instance (and download the .pem file). The free tier is eligible to 30 GB of disk. Under the <code>Advanced details</code> section, add the following <code>bash</code> script in the <code>User Data</code> field:</p> <pre><code>#!/bin/bash\n# update OS\nyum update -y\n# Get Apache HTTPd\nyum install -y httpd\n# Start the service\nsystemctl start httpd\n# Enable it cross restart\nsystemctl enable httpd\n&gt; Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service\n# Get the availability zone\nEC2-AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone/)\n# Change the home page by changing /var/www/html/index.html\necho \"&lt;h1&gt;Hello from $(hostname -f)&lt;/h1&gt;\" &gt; /var/www/html/index.html\n# or use the following\necho \"&lt;h3&gt;Hello World from $(hostname -f) in AZ= $EC2_AZ &lt;/h3&gt;\" &gt; /var/www/html/index.html\n</code></pre> <p>Once launched, from the console, get the DNS name or the public IP address and try a curl or use your web browser to that HTTP address (not https).</p> <p>We can do the same with IaC, see the example labs/cdk/ec2-basic and use the output to get the URL of the web server.</p>"},{"location":"playground/gettingstarted/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Connection timeout: Any timeout (not just for SSH) is related to security groups or a firewall rule. This may also means a corporate firewall or a personal firewall is blocking the connection. Go to the security group and look at inbound rules.</li> <li><code>Permission denied (publickey,gssapi-keyex,gssapi-with-mic)</code>: You are using the wrong security key or not using a security key. </li> <li>\"Able to connect yesterday, but not today\": When you restart a EC2 instance, the public IP of your EC2 instance will change, so prefer to use the DNS name and not the IP @. </li> </ul>"},{"location":"playground/gettingstarted/#ssh-to-ec2","title":"SSH to EC2","text":"<ul> <li>Get public IP address of the EC2 instance</li> <li>Get pem certificate for the CA while you created the EC2 via the Key pair (e.g. <code>my-key-pair.pem</code>)</li> <li>Issue the following command where the certificate is to open a SSH session</li> </ul> <pre><code>ssh -i my-key-pair.pem ec2-user@35.91.239.193\n</code></pre> <p>you should get the prompt:</p> <pre><code>Last login: Wed Nov  2 17:24:30 2022 from ec2-18-237-140-165.us-west-2.compute.amazonaws.com\n\n       __|  __|_  )\n       _|  (     /   Amazon Linux 2 AMI\n      ___|\\___|___|\n\nhttps://aws.amazon.com/amazon-linux-2/\n</code></pre>"},{"location":"playground/gettingstarted/#ec2-instance-connect","title":"EC2 Instance Connect","text":"<p>Access the EC2 terminal inside the web browser using SSH. Select the instance and then <code>Connect</code> button at the top. </p> <p></p> <p>It comes with the <code>aws cli</code>. Never enter any account id inside <code>aws configure</code> inside an EC2 instance, use IAM role instead.</p> <p>For example to access another service (like IAM), we need an IAM Role added to the EC2 instance: go to the EC2 instance, <code>Action &gt; Security &gt; Modify IAM Roles</code> add <code>DemoEC2Role</code> for example. We should be able to do <code>aws iam list-users</code> command.</p>"},{"location":"playground/gettingstarted/#access-to-service-within-the-ec2","title":"Access to service within the EC2","text":"<p>To access to external AWS service we need to use IAM role. So define a Role in IAM </p> <p></p> <p>with the Permission Policy linked to the resource you try to access, for example select on S3 policies to be able to access S3 bucket. </p> <p></p> <p>On an existing EC2 we can use the menu <code>Actions &gt; Security &gt; Modify IAM Roles</code>.   </p>"},{"location":"playground/gettingstarted/#using-ec2-with-a-redis-server","title":"Using EC2 with a Redis Server","text":"<p>The user-data for Redis server looks like:</p> <pre><code>sudo yum -y install gcc make # install GCC compiler\ncd /usr/local/src \nsudo wget http://download.redis.io/redis-stable.tar.gz\nsudo tar xvzf redis-stable.tar.gz\nsudo rm -f redis-stable.tar.gz\ncd redis-stable\nsudo yum groupinstall \"Development Tools\"\nsudo make distclean\nsudo make\nsudo yum install -y tcl\nsudo cp src/redis-server /usr/local/bin/\nsudo cp src/redis-cli /usr/local/bin/\nredis-server --protected-mode no\n</code></pre> <p>See also the cloud formation template labs/networking</p>"},{"location":"playground/gettingstarted/#a-high-availability-webapp-deployment-summary","title":"A High availability WebApp deployment summary","text":"<p>Based on the AWS essential training, here is a quick summary of the things to do for a classical HA webapp deployment.</p> <ol> <li> <p>Create a VPC with private and public subnets, using at least 2 AZs. This is simplified with the new console which creates all those elements in one click:</p> <p></p> <p>The results, with all the networking objects created, look like below:</p> <p></p> </li> <li> <p>Verify routing table for public and private subnets. </p> </li> <li>Add security group to the VPC using HTTP and HTTPS to the internet gateway.</li> <li> <p>Start EC2 to one of the public subnet and define user data to start your app. Here is an example</p> <pre><code>#!/bin/bash -ex\nyum -y update\ncurl -sL https://rpm.nodesource.com/setup_15.x | bash -\nyum -y install nodejs\nmkdir -p /var/app\nwget https://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/ILT-TF-100-TECESS-5/app/app.zip\nunzip app.zip -d /var/app/\ncd /var/app\nnpm install\nnpm start\n</code></pre> </li> <li> <p>Get the security key with .pem file for the public certificate</p> </li> <li>Be sure the inbound rules include HTTP and HTTPS on all IPv4 addresses defined in the security group.  </li> </ol>"},{"location":"playground/gettingstarted/#create-a-ec2-instance-with-terraform","title":"Create a EC2 instance with Terraform","text":"<ul> <li> <p>Build a main.tf labs/terraform-vpc, which uses the aws provider to provision a micro EC2 instance:</p> <pre><code>terraform {\nrequired_providers {\n    aws = {\n    source  = \"hashicorp/aws\"\n    version = \"~&gt; 3.27\"\n    }\n}\n\nrequired_version = \"&gt;= 0.14.9\"\n}\n\nprovider \"aws\" {\n    profile = \"default\"\n    region  = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"app_server\" {\n    ami           = \"ami-830c94e3\"\n    instance_type = \"t2.micro\"\n\n    tags = {\n        Name = \"ExampleAppServerInstance\"\n    }\n}\n</code></pre> <p>Resource blocks contain arguments which you use to configure the resource.  Arguments can include things like machine sizes, disk image names, or VPC IDs.</p> </li> </ul> <pre><code>terraform apply\n# inspect state\nterraform show\n</code></pre>"},{"location":"playground/gettingstarted/#install-nginx-inside-a-ec2-t2micro","title":"Install nginx inside a EC2 t2.micro.","text":"<p>Be sure to have a policy to authorize HTTP inbound traffic on port 80 for 0.0.0.0/0.</p>"},{"location":"playground/gettingstarted/#asg-launch-template-and-load-balancer","title":"ASG, launch template and load balancer","text":"<ol> <li>Create a launch template</li> </ol>"},{"location":"playground/gettingstarted/#ecr-for-container-registry","title":"ECR for Container Registry","text":"<p>Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable.</p> <p>See Getting started guide which can be summarized as:</p> <ul> <li>Any client (CLIs) must authenticate to Amazon ECR registries as an AWS user before we can push and pull images.</li> <li>We define public or private repositories using the console or cli.</li> <li>Repository names can support namespaces and then image name.</li> <li>Create OCI with tag with the repository URI. When in private registry see the <code>Push commands</code> button to get the list of docker CLI commands to run. Something like:</li> </ul> <pre><code>export ACCOUNT_ID\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com\n\ndocker build  --platform linux/amd64 -t j9r_flaskapp .\ndocker tag j9r_flaskapp:latest $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/j9r_flaskapp:latest\ndocker push $ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com/j9r_flaskapp:latest\n</code></pre> <ul> <li>Images can be replicated to other repositories across Regions in our own registry and across accounts.</li> <li>We can control access to our repositories and the images within them with repository policies.</li> <li>We can get public images from docker hub or other public regisry and cache them to our private registry.</li> <li>For a ECS task to be able to pull image from private registry, we need to attach the <code>AmazonECSTaskExecutionRolePolicy</code> managed policies to the execution role of the task.</li> </ul> <p>Finally ECR supports sharing public image via the public gallery service.</p>"},{"location":"playground/gettingstarted/#deploy-a-web-app-on-aws-elastic-beanstalk","title":"Deploy a Web App on AWS Elastic Beanstalk","text":"<p>Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS</p> <p>Guide</p>"},{"location":"playground/networking/","title":"Networking hands on","text":""},{"location":"playground/networking/#vpc-hands-on-work","title":"VPC hands-on work","text":"<p>The CDK example in the ec2-vpc folder supports the following definitions:</p> <p></p> <ul> <li>The routes needs to define that traffic from internet goes to the IGW. The following route is associated to the public subnet-1:</li> </ul> <p></p> <ul> <li>Alternatively, to allow an instance in our VPC to initiate outbound connections to the internet but prevents unsolicited inbound connections from the internet, we can use a Network Address Translation (NAT) service for IPv4 traffic. NAT gateway maps multiple private IPv4 addresses to a single public IPv4 address, and does IP translation so host in the internet sees only public IP, original EC2's IP @ is in the header.</li> <li>In the private subnet, outbound to reach the internet has to go to the NAT Gateway, while traffic from machines in the subnets stays local, as illustrated in the routing table  below:</li> </ul> <p></p> <ul> <li>IPv6 uses Egress only Internet Gateway for outbound requests from a private Subnet. IPv4 oubtound internet traffic from a private subnet, uses a NAT instance or NAT Gateway. There is always one IP v4 address assigned to EC2 instance.</li> <li>NAT gateway is deployed inside a subnet and it can scale only inside that subnet. For fault tolerance, it is recommended that we deploy one NAT gateway per availability zone.</li> <li> <p>As we have set up a NAT Gateway in each public subnet, and the route in the private network routes all Internet traffic to the NAT gateway, we can ping from the EC2 running in the private subnet to the internet.</p> </li> <li> <p>Below are the two NAT gateway references, one in each subnet/AZ:</p> </li> </ul> <p></p>"},{"location":"playground/rds/","title":"Hands-on RDS","text":""},{"location":"playground/rds/#create-rds-postgresql-with-console","title":"Create RDS Postgresql with Console","text":"<ul> <li> <p>Be sure to define a security group in your VPC with a Inbound rules for the expected port number and MyIP address so you can run your app locally on your laptop and access RDS PostgreSQL DB.</p> <p></p> </li> <li> <p>Using PostgreSQL instance: </p> <p></p> </li> </ul> <p>See also the tutorial of creating Postgresql to get the different steps and default practices.</p> <p>Some remarks from the AWS tutorial:</p> <ul> <li> <p>In <code>Templates</code> changes the options in the form, for a production deployment with multi-AZ DB instance with primary and standby DB instances.</p> <p></p> </li> <li> <p>Be sure to keep the username and password</p> </li> <li> <p>Specify EC2 instance type and Storage class, and if you need storage autoscaling or not. </p> <p></p> </li> <li> <p>Assign to a VPC, choose network IP type and subnets, public access (not for production), and exposed port:</p> <p></p> </li> <li> <p>Be sure the DB instance is associated with a security group that provides access to it (port 5432). If your DB instance is publicly accessible, make sure its associated security group has inbound rules for the IP addresses that you want to access from. </p> </li> </ul> Info <p>Access to the created security group via the RDS instance panel: </p> <ul> <li>Verify the Monitoring panel to see the traffic you may have generated (see next sections)</li> <li>To delete the DB using the list of DB panel and <code>Actions &gt; Delete</code>. In case of modify the RDS database configuration to authorize deletion.</li> </ul>"},{"location":"playground/rds/#modify-an-existing-instance-to-be-public","title":"Modify an existing instance to be public","text":"<ul> <li>Choose Databases, and then select the Aurora DB instance in the Aurora Cluster that you want to modify.</li> <li>Choose Modify.</li> <li>From the Modify DB instance page, under Connectivity, expand the Additional Configuration section. Set Public access to Yes or No.</li> <li>Choose Continue, and check the summary of modifications.</li> <li> <p>To apply the changes immediately, select Apply immediately. Changing this setting on the existing DB instance in the cluster affects the network connectivity.</p> </li> <li> <p>See this note for more detail.</p> </li> </ul>"},{"location":"playground/rds/#create-an-aurora-serverless-instance","title":"Create an Aurora Serverless instance","text":"<p>The difference with previous step is the Aurora database, with a postgresql driver, </p> <p></p> <p>and then serverless option. </p> <p></p> <p>This will be a better solution for demonstration and to have access to an SQL editor inside AWS RDS console webapp, if not we need an external tool like pgadmin remotely connected.</p>"},{"location":"playground/rds/#quarkus-app-client","title":"Quarkus App Client","text":"<p>See the repository Autonomous Car Ride which uses Quarkus, Panache, and CarRide entity. The repository includes a docker file to start local postgresql and pgadmin containers.</p> <p>When changing to the URL properties to the one of the RDS service, then the local execution with <code>quarkus dev</code> connect to the RDS database.</p> <pre><code>quarkus.datasource.jdbc.url=jdbc:postgresql://&lt;&gt;.us-west-2.rds.amazonaws.com:5432/postgres\n</code></pre> <p>Adding a <code>new server</code> definition in the PGADMIN tool, with the RDS URL, user and password, we can see the DB and records from the RDS DB as illustrated below:</p> <p></p> <p>The URL for the quarkus app can be overwritten by environment variable: <code>QUARKUS_DATASOURCE_JDBC_URL</code></p>"},{"location":"playground/rds/#python-client","title":"Python Client","text":"<ul> <li>Need to install boto3 wiht pip. Bot3 RDS API documentation.</li> </ul> <pre><code>import boto3\n\nclient = boto3.client('rds')\n\nresponse = client.describe_db_instances(\n    DBInstanceIdentifier='customer-action-instance-1',\n)\nprint(response)\n</code></pre>"},{"location":"playground/rt-data-processing/","title":"Build a Serverless Real-Time Data Processing App","text":"<p>From this hands-on lab</p> <p>Some changes from the lab itself.</p>"},{"location":"playground/rt-data-processing/#another-demo","title":"Another Demo","text":""},{"location":"playground/rt-data-processing/#functional-architecture","title":"Functional architecture","text":"<ul> <li>We have 10000 refrigerator containers in the field, which send sensor telemetry messages using MQTT over HTTPS to a data ingestion and real-time event backbone as landing zone. This layer keeps data for 7 days, so on the left side we are moving records to Data Lake</li> </ul> <ul> <li>From Data lake data engineers and data scientists will query data at rest, encrypted, and even build dashboards</li> <li>On the right side we have the cold chain monitor component, and the best action processing in case there is a refrigeration container or reefer with problem.</li> <li>As part of the action processing we can notify a field engineer to work on container, persist data in a DB for triggering a downstream business process</li> </ul>"},{"location":"playground/rt-data-processing/#hands-on-architecture","title":"Hands-on architecture","text":"<ul> <li>Now on the scope of the demonstration we build for you, using AWS managed services, this is the architecture we are proposing</li> </ul> <ul> <li>We do not have refrigerator at our hand so we get the data structure from your engineers, and develop a small simulator to send records to the data ingestion layer.</li> <li>The AWS Kinesis Data Stream is the service to manage pub/sub processing from streams (like a  topic). we have telemetries and faulty-reefers </li> <li>The stateful logic processing  is done inside Kinesis analytics: The logic is to consume messages from data stream, process them with time window constraint and logic then write messages to the faulty-reefers data stream </li> <li>On the right side we have the act part, where you can have different consumers which are getting message asynchronously to process the faulty message</li> <li>For data lake, Firehose is the service to do data movement and transformation, like ETL. The target is distributed storage, called S3. The persistence looks like a file system folder, and is named bucker.</li> <li>For dashboard we use QuickSight</li> </ul> <p>Kinesis SDK for python</p>"},{"location":"playground/rt-data-processing/#create-kinesis-elements","title":"Create Kinesis Elements","text":""},{"location":"playground/rt-data-processing/#data-streams","title":"Data Streams","text":""},{"location":"playground/rt-data-processing/#streams-analytics","title":"Streams Analytics","text":"<pre><code>CREATE OR REPLACE PUMP \"STREAM_PUMP\" AS\n  INSERT INTO \"FAULTY_REEFERS\"\n  SELECT STREAM \"container_id\",\"measurement_time\",\"product_id\", \"temperature\",\"kilowatts\",\"oxygen_level\",\"carbon_dioxide_level\",\"fan_1\",\"latitude\",\"longitude\",\n TUMBLE_START(ts, INTERVAL '1' MINUTE) as window_start,\n TUMBLE_STOP(ts, INTERVAL '1' MINUTE) as window_stop,\nCOUNT(temperature)\nFROM \"SOURCE_SQL_STREAM_001\"\nGROUP BY TUMBLE(ts, INTERVAL '1' MINUTE), temperature\n    WHERE \"temperature\" &gt; 18; \n</code></pre>"},{"location":"playground/s3-org-billing/","title":"S3 storage aggregation with S3 Storage Lens and AWS Organization","text":"<p>Info</p> <p>Created 1/11/2023 - Updated 1/18/23</p> <p>The goal of this lab is to demonstrate the single view in S3 Storage Lens of S3 bucket usages created in multiple regions, in multiple accounts.</p>"},{"location":"playground/s3-org-billing/#requirements","title":"Requirements","text":"<ul> <li>Get one AWS Organization to centralize 2 child AWS accounts.</li> <li> <p>Get S3 buckets for the 2 accounts each in 2 regions.</p> <p></p> </li> <li> <p>Create a Storage Lens dashboard to see consolidated metrics.</p> </li> </ul>"},{"location":"playground/s3-org-billing/#aws-organizations","title":"AWS Organizations","text":"<p>AWS Organizations is an AWS managed service that helps us aggregate all of our AWS accounts under one organization hierarchy.</p> <ul> <li> <p>Invite or add an account from AWS Organization.</p> <p></p> </li> <li> <p>Enable trusted access from S3 Storage Lens to Organization, to aggregate storage metrics and usage data for all member accounts in our organization.</p> <p></p> </li> </ul>"},{"location":"playground/s3-org-billing/#s3-storage-lens","title":"S3 Storage Lens","text":"<p>A service that aggregates our usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format.</p> <ul> <li> <p>[Optional] Add an accountID to be S3 Storage Lens administrator:</p> <p></p> </li> <li> <p>Create a S3 Storage Lens dashboard, using an home region to keep dashboard's states:</p> <p></p> <p>Metrics are updated daily.</p> </li> <li> <p>Still in the same web entry form, we should see the organization linked to the service, and select include all accounts and all regions:</p> <p></p> </li> <li> <p>We can select metrics and define new metrics, here we use the free ones:</p> <p></p> <p>Metrics will help to find out how much storage we have across the entire organization. Which are the fastest-growing buckets and prefixes...   </p> <p>We can specify advanced metrics to get prefix-level aggregation. Free metrics are available for queries for a 14-day period, and advanced metrics are available for queries for a 15-month period.</p> </li> <li> <p>We export metrics generated every day, in CSV or Parquet format, and save result in the parent account's s3 bucket:</p> <p></p> </li> <li> <p>Finally the data should be encrypted at rest, so using S3 managed keys will be the default:</p> <p></p> </li> </ul> <p>The dashboard is created and will get data in the next 24h to 48h.</p> <p>Lens pricing can be found in the Management and Analytics tab in S3 pricing page.</p>"},{"location":"playground/s3-org-billing/#add-files-in-buckets","title":"Add files in buckets","text":"<ul> <li>Account 1: add buckets in regions us-west-2 and us-east-1</li> </ul> <pre><code>aws w3 cp ... s3://jb-data-set/\n</code></pre> <ul> <li> <p>Account 2: add buckets in regions us-west-2 </p> <p></p> </li> <li> <p>The results are reported in the Storage Lens dashboard, with a view per account (the aggregation is done during a time period we selected):</p> <p></p> <p>or per region:</p> <p></p> </li> </ul>"},{"location":"playground/s3-org-billing/#resources","title":"Resources","text":"<ul> <li>S3 Storage Lens user guide</li> </ul>"},{"location":"playground/s3-play/","title":"S3 play with security","text":"<p>This note groups a set of small lab to assess how to use security on top of S3.</p> <p>In an AWS account, create a user <code>bob</code> part of <code>dev</code> group, with the following group permissions:</p> <pre><code>aws iam create-group --group-name dev\naws iam create-user --user-name bob \naws iam add-user-to-group --user-name bob --group-name dev\naws iam create-login-profile --user-name bob --password &lt;putalongpassword&gt; --no-password-reset-required\naws iam create-access-key --user-name bob\n</code></pre> <ul> <li>Attach S3 Full access to the group</li> </ul> <pre><code>aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name dev\n</code></pre> <ul> <li>Verify access https://us-west-2.console.aws.amazon.com/console/: using the AWS Account id and <code>bob</code> user.</li> <li>Create an access key for use bob, using the console or CLI.</li> <li>Verify the CLI access with <code>aws configure</code> then use profile bob</li> </ul> <pre><code>aws sts get-caller-identity --profile bob\n</code></pre> <p>As a developer bob needs to be able to see cloudwatch metrics and logs, so we can add such policies to the dev group.</p> <pre><code>aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess --group-name dev\n</code></pre>"},{"location":"playground/s3-play/#create-s3-bucket","title":"Create S3 bucket","text":"<p>Be sure to get a user with S3 access. </p> <pre><code>export ACCOUNT_ID=\naws s3api create-bucket --bucket bob-data-$ACCOUNT_ID --region us-west-2  \\\n    --create-bucket-configuration LocationConstraint=us-west-2 --profile bob\n</code></pre> <p>The bucket is blocked for public access by default, but a <code>aws s3 cp</code> will work:</p> <pre><code>aws s3 cp ./car-price/Automobile_data.csv s3://bob-data-9...54 --profile bob \n</code></pre> <ul> <li>Verify</li> </ul> <pre><code>aws s3api list-objects --bucket bob-data-9..54 --output text --profile bob\n</code></pre> <p>See this git repo for aws s3 and s3api commands</p>"},{"location":"playground/s3-play/#create-s3-access-point","title":"Create S3 access point","text":"<ul> <li>Define an internet facing access point associate with a unique bucket:</li> </ul> <pre><code>aws s3control create-access-point --account-id $ACCOUNT_ID --name bob-data-ap \\\n    --bucket bob-data-$ACCOUNT_ID \\\n    --public-access-block-configuration BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=true,RestrictPublicBuckets=true \\\n    --profile bob\n# for a VPC specific add\n--vpc-configuration VpcI=vpc...\n</code></pre> <ul> <li>Add policy to get only bob being able to get object.</li> </ul>"},{"location":"playground/s3-play/#share-with-pre-signed-url","title":"Share with pre-signed URL","text":""},{"location":"playground/spark-emr/","title":"Data analytics with Spark and EMR","text":""},{"location":"playground/spark-emr/#emr-serverless-demo-product-recommendations","title":"EMR Serverless demo: product recommendations","text":"<p>Sources are in <code>labs/analytics/emr-serverless-demo</code>. </p> Summary of the demonstration steps <ol> <li>Create S3 bucket and upload <code>dataset_en_dev.json</code> and <code>reviews.py</code> files.</li> <li>Define IAM policy, named <code>EMRServerlessS3AndGlueAccessPolicy</code>, used by EMR using the content of <code>EMRServerlessS3AndGlueAccessPolicy.txt</code></li> <li>Create IAM role, named <code>emrserverlessrole</code>, using Custom trust policy, and the content of <code>EMR Serverless Trust policy.txt</code> which let this role to assume role on the <code>emr-serverless.amazonaws.com</code> principal</li> <li>Access EMR Studio as to create an Amazon EMR Serverless application, we need an EMR Studio.</li> <li>Create an application <code>ProductRecommendation</code>, use default spark settings.</li> <li>Submit a job:      </li> <li>After the run status is complete, go to S3 bucket output folder to review the results.</li> <li>Clean up: Delete the EMR application in EMR Studio, delete S3 bucket content </li> </ol>"},{"location":"playground/spark-emr/#product-recommendation-with-emr-ec2-cluster","title":"Product recommendation with EMR-EC2 cluster","text":"<p>Sources are in <code>labs/analytics/emr-ec2-demo</code> and the dataset in <code>labs/analytics/emr-serverless-demo</code>.</p> Summary of the demonstration steps <ol> <li>Create VPC with 1 public subnet with no  NAT gtw and no private subnet</li> <li>Create S3 bucket and input folder, upload <code>dataset_en_dev.json</code> file</li> <li>Create IAM role, named <code>EMRstudio</code> with AWS service as trusted entity, and in User cases for other service, select EMR.</li> <li>Add to the created role, a permission to read from the Amazon S3 bucket, select to attach S3FullAccess for this demo.</li> <li>Add permission to create EMRStudio and EMR clusters, using create inline policy and use <code>Studio_inlinepolicy.txt</code> which allow action on EC2, EMR...</li> <li>Modify the Trust relationships of the EMRstudio role with the content of <code>Trustpolicy.txt</code> file, modify the region and accountID.</li> <li> <p>Create EMR cluster with the VPC and subnet, role created before and with the custom packages as:</p> <p></p> </li> <li> <p>Create an instance of EMR Studio</p> <p> </p> </li> <li> <p>Create a Workspace in EMR Studio to link to the EMR cluster.</p> <p></p> </li> <li> <p>Upload the <code>labs/analytics/emr-ec2-demo/reviews.ipynb</code> using the Upload arrow icon on the left of EMR notebook UI. Update the paths to the S3 bucket, then wait to get the kernel ready and execute all cells. The process creates parquet files in the <code>output/analyze</code> folder in s3.</p> <p></p> </li> </ol>"},{"location":"playground/spark-emr/#gender-age-analytics","title":"Gender - Age Analytics","text":"<p>This is based on the tutorial from A Cloud Guru on Data Analytics with Spark and EMR, with some new adaptations.  </p> The problem statement <p>The marketing department has personal information in thousand of CSV files and wants to run analytics by counting the gender per age groups...</p> <ul> <li>Hundred of CSV files are in S3 bucket. Using <code>s3-dist-cp</code> to move them into the EMR cluster.</li> <li>Use EMR to run Spark job.</li> <li>Define Spark processing in python to read all CSV files from HDFS then group records by age and gender then count records and order the results in descending mode. Result in HDFS.</li> <li>Copy result back to S3.</li> </ul> <p>Error</p> <p>11/29 Still issue in S3 to HDFS step</p> Detailed steps <ol> <li> <p>Create a EMR cluster last version (6.9) with Spark runtime 3.3, Hadoop, Hue, Hive and Pig</p> <p></p> </li> <li> <p>Do not change the networking or cluster config...</p> <p></p> </li> <li> <p>Change the EC2 to <code>m4.large</code>, if it is available within the AWS Region you selected and use only one instance for the core node:</p> <p></p> </li> <li> <p>Set a cluster name:</p> <p></p> </li> <li> <p>Do not use key pair as we do not need to SSH on core node</p> <p></p> </li> <li> <p>Once the cluster is started, get the URL of the HDFS Name Node console and keep note of the associated port number</p> <p></p> </li> <li> <p>Update the <code>Security Groups for Master</code>: </p> <p></p> <p>to authorize access to the port number from anywhere 0.0.0.0/0. Be sure to use <code>Custom TCP</code> protocol for the Inbound rule:</p> <p></p> </li> <li> <p>Open the <code>HDFS Name Node</code> URL in a web browser to get visibility into the files in our Hadoop cluster.</p> <p></p> </li> <li> <p>Upload data files (folder upload) to S3 bucket. (The data files are in user-data-acg.zip)</p> </li> <li> <p>Use <code>s3-dist-cp</code> as a Step (type: Custom jar) to copy data from S3 to HDFS. (This command is defined in <code>command-runner.jar</code>).</p> <p></p> <p>Here is the arguments to use:</p> <pre><code>s3-dist-cp --src=s3://jb-data-set/user-data-acg/ --dest=hdfs:///\n</code></pre> <p>Running the task...</p> <p></p> </li> <li> <p>Create a Step to run the python code</p> </li> <li>Create a S3 bucket as target for the output: <code>s3://jb-data-set/gender-age-output</code></li> <li> <p>Define a new Step with s3-disp-cp from HDFS to S3. </p> <pre><code>s3-dist-cp --src=hdfs:///results --dest= s3://jb-data-set/gender-age-output\n</code></pre> </li> </ol> <p>spark-summit hdfs://pyspark-script/gender-age-count.py</p> <p>s3://emr-scripts-403993201276/scripts/gender-age-count.py </p>"},{"location":"playground/spark-emr/#aws-cli-based-deployment","title":"AWS CLI based deployment","text":"<p>Under the labs/analytics/ folder.</p> <ul> <li> <p>Create the EMR cluster with Spark, Hadoop and Hive (version 6.9)</p> <pre><code>./emr-starting/create-cluster.sh\n</code></pre> </li> <li> <p>Under <code>gender-age/data</code> folder unzip the <code>user-data-acg.zip</code> then upload the <code>user-data-acg</code> folder to a s3 bucket</p> </li> <li> <p>Upload data to s3 bucket</p> <pre><code>aws s3 cp gender-age/data/user-data-acg s3://jb-data-set/\n</code></pre> </li> <li> <p>Deploy the S3 to HDFS step</p> <pre><code>cd gender-age\n./s3-to-hdfs.sh\n</code></pre> </li> <li> <p>Start the python</p> </li> <li>Copy the HDFS result to S3</li> </ul>"},{"location":"playground/spark-emr/#clean-up","title":"Clean up","text":"<ul> <li>Delete cluster: <code>./emr-starting/delete-cluster.sh</code></li> <li>Delete S3 bucket content and buckets</li> <li>Remove <code>emr</code> roles in IAM</li> </ul>"},{"location":"playground/security/","title":"Security Demonstrations","text":"<p>The security demonstrations are aimed to get hands-on experience around:</p> <ul> <li>Basic IAM role and user authorization</li> <li>Evaluating cross-account access management</li> <li>Integrating with third-party identity providers</li> <li>Deploying encryption strategies for data at rest and data in transit</li> <li>Developing a strategy for centralized security event notifications and auditing</li> </ul>"},{"location":"playground/security/#basic-demonstrations","title":"Basic demonstrations","text":""},{"location":"playground/security/#define-users-and-groups-with-iam","title":"Define users and groups with IAM","text":"<p>See summary on IAM. Better to define user and do not use root user. User can be created using CloudFormation or CDK. See  cdk samples and the aws-remote-svc-access-from-k8s repository to demonstrate remote service access from k8s..</p>"},{"location":"playground/security/#iam-role-to-trust-a-user-to-access-aws-service-like-ec2","title":"IAM Role to trust a user to access AWS Service like EC2","text":"<p>A user has no access to any of AWS services, but can login to the AWS console. The demo aims to show how to add an IAM role, trusting an IAM user, and defining a policy to get full access to EC2 service.  When user switches to the new role in the AWS console, the security context changes.</p> Manual demonstration <ol> <li> <p>Create a user in IAM (named Julie): authorize access to the AWS console, with custom password, and no other permissions.</p> <p></p> </li> <li> <p>Add a role named <code>EC2FullAccessRole</code>, with a trusted relationship for the created user as principal: <code>iam:user...</code>:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::4....:user/Julie\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>Add the <code>permission</code> for the user to get EC2 full access via the managed policy: AmazonEC2FullAccess</p> <p></p> <p>So now the user will be able to assume the role.</p> </li> <li> <p>Login in a private web browser session, using IAM user option on the AWS account the user belongs to, and the newly created user.</p> <p></p> </li> <li> <p>Go to the EC2 service, we should get API errors like below:</p> <p></p> </li> <li> <p>Switch Role from the AWS console, user top right menu, and select the role:</p> <p></p> </li> <li> <p>Should now see the EC2 console without any error.</p> <p></p> </li> </ol>"},{"location":"playground/security/#using-cdk-to-create-users-and-policies","title":"Using CDK to create users and policies","text":"<p>The cdk in labs/cdk/iam-user creates the Julie user, role, the EC2 permission policy, then we can use the same step as above to login to the console and assume the role</p> How it was built <ul> <li>Jumpstart the cdk</li> </ul> <pre><code>cdk init sample-app --language python\n</code></pre> <ul> <li> <p>Modify the iam_user_stack.py by adding a user, a role, and attach amazon policy. <pre><code> user = iam.User(self,\n                    \"Julie\",\n                    user_name=\"Julie\",\n                    password=SecretValue.unsafe_plain_text(\"Passw0rd!\")\n                    )\n\n    role = iam.Role(self,\"EC2FullAccess\",\n                    role_name=\"EC2FullAccess\",\n                    description=\"A role to allow user to do EC2 work\",\n                    assumed_by=user,\n                    managed_policies=[iam.ManagedPolicy.from_aws_managed_policy_name(\"AmazonEC2FullAccess\")])\n</code></pre></p> </li> <li> <p><code>cdk synth</code> and <code>cdk deploy</code></p> </li> <li>Execute the same demo to login Julie user and access to EC2.</li> </ul>"},{"location":"playground/security/#advanced-demonstrations","title":"Advanced Demonstrations","text":"<p>See the product IAM tutorials and IAM workshops like Refining IAM Permissions Like A Pro.</p>"},{"location":"playground/security/#cross-accounts-authorization","title":"Cross Accounts authorization","text":"<p>The IAM tutorial, Delegate access across AWS accounts using IAM roles, demonstrates how to create a role to establish trust with another account and define what actions trusted entities can take:</p> <ol> <li>Create a S3 bucket in Production account</li> <li>Create a policy to control access to the S3 bucker</li> <li>Create a role in Production account with this policy attached and as a trusted relationships the Account A</li> <li>Create a policy in the Account A for user to assume the role defined in Production account</li> </ol> <p>See the SDK code to do the same thing but for a role so an application can act on S3 bucket on another account in labs/security/sts-demo</p> <p>Then, modify a user group policy to control which IAM users can access the role. As a result, developers from the Development account can make updates to the production bucket in the Production account by using temporary credentials.</p>"},{"location":"playground/security/#attribute-based-access-control","title":"Attribute-based access control","text":"<p>The attribute based access control uses tags to control access to resources for 2 projects, using different groups of user (dev, qas). See the labs/security/iam folder for cdk to do the same tutorial.</p>"},{"location":"playground/security/#cross-accounts-authorization_1","title":"Cross Accounts authorization","text":"<p>This is a common use case, where a main account A can read DynamoDB table and put message to SQS queue in a second account B, the owner of the queue.</p> <ul> <li>Create a second account.</li> <li> <p>Create a SQS and define an Amazon SQS policy that specifies both accounts as Principal</p> <pre><code>\"Effect\": \"Allow\",\n  \"Principal\": {\n    \"AWS\": [\n      \"arn:aws:iam::&lt;account_id_owner_queue&gt;:root\",\n      \"arn:aws:iam::&lt;account_id_client_queue&gt;:root\"\n    ]\n  },\n  \"Action\": \"SQS:*\",\n  \"Resource\": \"arn:aws:sqs:us-west-2:&lt;account_id_owner_queue&gt;:MyQueue\"\n}\n</code></pre> </li> <li> <p>Test to send and receive message from the owner account</p> </li> <li>Login to the client account (via STS token while using from a remote computer/laptop)</li> <li> <p>Specify the SQS client to use queue and id of the specified account</p> <pre><code>sqs = boto3.resource('sqs')\nqueue = sqs.get_queue_by_name(QueueName='MyQueue',QueueOwnerAWSAccountId=\"account_id_owner_queue\")\nresponse = queue.send_message(MessageBody='Hello from client app')\n</code></pre> </li> <li> <p>Use the AWS console, SQS console to look at the messages</p> </li> </ul>"},{"location":"playground/security/#other-samples","title":"Other samples","text":"<ul> <li>AWS Organizations playground: a repo to play with cross accounts access control using Organizations.</li> </ul>"},{"location":"sa/psa-role/","title":"Principal Solution Architect","text":"<ul> <li>Partner to navigate the 200+ services, deploy your solution to the cloud</li> <li>Technologist Evangelist and educator</li> </ul>"},{"location":"sa/psa-role/#role","title":"Role","text":"<ul> <li>technical leader and a strategic influencer like a CTO</li> <li>architect solutions to significantly complex problems, high ambiguity,</li> <li>leverage technical, industry, and business context expertise (e.g., outcome priorities, customer experience, shared goals, business case) to influence the direction of longer-term business and technology strategies</li> <li>identify both immediate and future risks and constraints</li> <li>advise customers on how to make the right trade-offs in solutions design (e.g., extensibility, flexibility, scalability, maintainability)</li> <li>able to dive deeply into technical details </li> <li>own the design and delivery of a program of customer solutions including the overall strategy and end-to-end architecture. Apply the design principles of security, reliability, cost optimization, operational excellence, and performance efficiency. </li> <li>proactively look for opportunities to scale the solution to benefit other customers with similar problems or requirements</li> <li>lead the curation of thought leadership content and ensure delivered content is relevant to customer needs.</li> </ul>"},{"location":"sa/psa-role/#attitude","title":"Attitude","text":"<ul> <li>relentlessly simplify and are able to deconstruct extraordinarily complex problems into their constituent building blocks, accelerating customer adoption and/or enabling teams across the organization to work in parallel on the problem.</li> <li>recognize problems both inside and outside your area, build consensus around a vision, and drive resolution leveraging the right resources</li> </ul>"},{"location":"sa/psa-role/#associate-certification-study","title":"Associate certification study","text":""},{"location":"sa/resiliency/","title":"Resiliency","text":"<p>Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues.  It addresses DR (restore service) and Availability (prevent loss of service).</p> <p></p> <p>Apply the sharing responsibility of resiliency: AWS for resiliency of the cloud: infrastructure. Customer responsible for resiliency in the cloud: adopting instance deployment across multiple locations, support self-healing, design for resilience.</p>"},{"location":"sa/resiliency/#assessing-resiliency-needs","title":"Assessing resiliency needs","text":"<ul> <li>Start by looking at business impact: assess cost of downtime? business metrics used to measure impact? and existing metrics and tools to compute outage impact. 70% of CIO wants to be resilient, 80% have nothing in place, and most has no real metrics.</li> <li>Assess what is the current method to detect downtime, and the definition of downtime. Moving to a standby region cost time and effort, getting a clear assessment of what downtime means is very important before triggering the failover. Is it latency? is it missing orders?</li> <li>Get an application inventory by category of criticality, including compliance requirements</li> <li>What is the process in place to alert human for downtime?</li> <li>What are the current observability practice?</li> <li>What is the recovery strategy?</li> </ul> <p>Operational Readiness Review (ORR) has questions and tools to help understand current state, assess risks, and presents best practices to create highly available, resilient systems. </p>"},{"location":"sa/resiliency/#disaster-recovery","title":"Disaster recovery","text":"<p>DR is about business continuity. Review the core principles of DR in this note and then the AWS options whitepaper. Recalls the important recovery metrics of RTO and RPO, while for availability we measure mean over time (99.99%), and the assessment of the acceptable recovery cost to select the best solution possible.</p> <p>DR in the cloud, AZ is the equivalent of data center for on-premises deployment, also users are responsible for backups and need to assess the Multi-region deployment. The DR site does not need to have the same capacity at first to run back the business. Most of the serverless platform supports HA and elastic scaling.</p>"},{"location":"sa/resiliency/#region-az","title":"Region, AZ","text":"<ul> <li>Region and AZs are the building blocks to support HA and DR in the cloud. With very low latency network between AZs we can do in-synch replicas.</li> <li>Non-AZ affinity: a write operation can cross AZs and with data replications, it can make a lot of hops.</li> <li>AZ-affinity helps to reduce the number of hops, but it enforces using NLB and not ALB.</li> </ul>"},{"location":"sa/resiliency/#strategies-for-dr","title":"Strategies for DR","text":"<ul> <li>Backup/restore is for lower priority workload, cost less, and provision cloud resources after the event. RTO/RPO in hours</li> <li>Pilot Light: Data is live but services are idle. Provision some resource and scale after event. RPO/RTO on 10s of minutes</li> <li>Warm standby: The DR site is always running but smaller scale. Resources will scale after the event. RPO/RTO at the minutes level.</li> <li>Multi-site active/active: Zero downtime, near zero data loss. This is really for mission critical services. </li> </ul>"},{"location":"sa/resiliency/#route-53-arc","title":"Route 53 ARC","text":"<p>See dedicated note on Route 53 ARC to support resiliency.</p>"},{"location":"sa/resiliency/#elastic-disaster-recovery","title":"Elastic Disaster Recovery","text":"<p>Helps to support application DR from on-premises to AWS or cloud based apps. It uses a Replication agent on source server to replicate data to AWS region and account.</p> <p>It supports testing non-disruptive recovery and fallback drills, replications monitoring.</p>"},{"location":"sa/resiliency/#high-availability","title":"High Availability","text":"<p>It is not DR. Here is a classical AWS deployment for HA, where Apps running on EC2, ELB, Database are deployed in 3 AZs.</p> <p></p> <p>S3 and DynamoDB are also deployed in AWS AZs outside of the customer's VPC.</p> <p>Availability is measured as:</p> <p></p> <p>It cannot be better than it's least available dependency.</p> <p></p> <p>Spare components improves availability. Multi-AZ deployment improve availability.</p>"},{"location":"sa/resiliency/#measuring-availability","title":"Measuring availability","text":"<p>Metrics to consider:</p> <ul> <li>Mean time between failure MTBF, try to increase it</li> <li>Mean time to detection MTTD</li> <li> <p>Mean time to repair MTTR</p> </li> <li> <p>Consider server-side and client-side request success rate. Define what unit of work to be used: HTTP request, message in queue, async job.</p> </li> <li>Define downtime, for example, drop below 95% availabiltiy for any API during a 5 minutes window.</li> </ul> <p>Use CW embedded metric format EMF to combine logs and metrics</p>"},{"location":"sa/resiliency/#latency","title":"Latency","text":"<p>Latency impacts availability. Analyze histograms for latency distribution trends. Use percentiles and trimmed man to measure latency.</p>"},{"location":"sa/resiliency/#resiliency-patterns","title":"Resiliency Patterns","text":"<p>Need to see it from the client or/and server side:</p> <ul> <li>Client side, use circuit braker, retries with jitter, multiplexing connection with new protocol like HTTP/3, gRPC</li> <li>Server side, apply caching strategy, cache-aside is more resilient, inline cache, like DynamoDB DAX, may be a single point of failure. </li> </ul>"},{"location":"sa/resiliency/#fault-isolation-concepts","title":"Fault isolation concepts","text":"<ul> <li>Control plane is a complex orchestration with many dependencies so is more difficult to make it resilient. But the lower level of APIs makes it less risky. Still they will fail more often. Latency is not a strong requirement, 100s ms.</li> <li>Do not rely on Auto Scaling Group for EC2 in case of AZ failure, but plan for overcapacity to support the load after AZ failure.</li> <li> <p>Examples of control plan and data plane:</p> Service Control plane Data plane Amazon S3 create bucket, put bucket policy GetObject, PutObject DynamoDB Create table, update table GetItem ... ELB CreateLoadBalancer, CreateTargetGroup The load balancer itself EKS CreateCluster, PodScheduling, scale worker nodes worker node, pod Route 53 CreateHostedZone ... DNS resolution, health checks IAM CreateRole Authn, Authz RDS Create DB instance The data base </li> <li> <p>Static stability: system still running even dependencies fail without the need to make changes. Use the following approaches: prevent circular dependencies, pre-provision capacity, maintain existing state, eliminate synchronous interaction.</p> </li> <li>One way AWS achieve static statbility is by removing control plane deplendencis from the data plane in AWS services. </li> <li>Relying on data plane operations for recovery can help make the system more staticall stable, this may include pre-provisioning resources, or relying on data plane operations.</li> </ul>"},{"location":"sa/resiliency/#aws-partitions","title":"AWS Partitions","text":"<p>AWS Partitions are isolated infrastructure and services. It includes commercial partition, China , and GovCloud. Within a partition there are regions. There is no cross IAM definition sharing.</p> Type Service Planes Zonal RDS, EC2, EBS Control plane is regional while data plane is zonal Regional S3, SQS, DynamoDB Control plane  and data plane are regional Global CloudFront, Global accelerator, Route 53, IAM Control plane is single region  and data plane is global <p>There are three categories of global services: partitional, Edge and globally-scoped operations. Global services have a single control plane and a distributed, highly available data plane. Avoid control plane dependencies in global services in the recovery path, implement static stability.</p>"},{"location":"sa/resiliency/#cell-based-architecture-and-shuffle-sharding","title":"Cell-based architecture and shuffle sharding","text":"<p>Goal is to reduce the blast-radius. Use the concept of <code>Cell</code> which is a construct to isolate compute, routing, and storage (workload). Cells are not AZs but cross AZs. Cell shares nothing with each other. Cell can scale-up and out. Cell have a maximum size.</p> <p>It is not fitting for all type of workloads.</p> <p>API Gateway uses this Cell-based architecture.</p> <p>Some e-commerce fullfilment delivery centers use cell-based architecture.</p> <p>Considerations to address:</p> <ul> <li>Cell size</li> <li>Router robustness</li> <li>Partitioning dimension</li> <li>Cross-cell use cases</li> <li>Cell migration</li> <li>Alignment to AZs and Regions</li> </ul> <p>Shards is another construct to isolate blast radius, but not at the level of resources.  Any serverless services is using the concept of shuffle sharding.</p>"},{"location":"sa/resiliency/#multi-az-patterns-az-gray-failure","title":"Multi-AZ patterns: AZ gray failure","text":"<ul> <li>System follows differential observability. </li> <li>Different perspective: system versus application</li> </ul> <p>Detection tools:</p> <ul> <li>cloudwatch insights. </li> </ul>"},{"location":"sa/resiliency/#sources","title":"Sources","text":"<ul> <li>DR of workloads on AWS video.</li> </ul>"},{"location":"sa/saas/","title":"Software as a Service","text":"<p>A licensing and delivery model whereby software is centrally managed and hosted by a provider and available to customers on a subsciption fee, or pay-per-user basis, or pay per usage basis.</p>"},{"location":"sa/saas/#concepts","title":"Concepts","text":"<p>Everything done in SaaS is about multi-tenancy, data isolation and sharing resources like compute, networking and storage as part of the infrastructure to optimize cost of running the business. </p> <p></p> <p>Figure 1</p> <p>As illustrated in the figure above an onboarding / shared service component is needed to manage the multi-tenant platform. Any software vendor is bringing their own solution and it is highlighted as application specific domain. As an example, if we take a big data platform provider such application domain may look like in the following architecture diagram, which supports a map/reduce job execution environment with data ingestion, transformation and persistence:</p> <p></p> <p>Figure 2</p> <p>A control plane manages cluster of worker nodes. Each worker node runs an executor that executes jobs. Jobs are the custom piece of code in Python, Scala, Java which performs the data ingestion, data transformation &amp; enrichment, MAP/Reduce logic and persists the results to distributed storage.</p> <p>Footprint scales dynamically based on aggregate load of all tenants. All tenants are managed via a single operational control plane or lens.</p> <p>The Analytic component is very important to help marketing team to understand how the customers are using the platform, and how to optimize the usage of the platform.</p> <p>For the application domain, new features are deployed to all tenants, more quickly via DevOps practices. </p>"},{"location":"sa/saas/#multi-tenancy-support-approaches","title":"Multi tenancy support approaches","text":"<p>There are multiple patterns for multi-tenancy, some linked to business requirements and sometime to technical reasons: </p> <p></p> <p>Figure 3</p> <ul> <li>Silo: Each tenant gets unique set of infrastructure resources. As environments are partitioned, there is no cross-tenant impacts. Agility is compromised. Needed for strong regulatory and compliance. We can tune the configuration per tenant and get specific SLAs. It costs more and operations are more complex. The analytics services need to aggregate from those silos. Tenant may be mapped to AWS Account and a linked account to aggregate billing. Or based in VPC.</li> <li>Pool: Shared resources, centralized management, simplified deployment. Compliance is a challenge and cross-tenant impacts with a all or nothing availability. The advantages are cost optimization and operation. </li> <li>Bridge: A mix of silo and pool. Can be applied to the different level of the solution architecture, for example, web and data tier can be pool, and app layer in silo.</li> </ul> <p>It is important to decide what approach the SaaS architecture needs to support.</p>"},{"location":"sa/saas/#needs","title":"Needs","text":"<p>Agility is the major requirements to go to SaaS, which means:</p> <ul> <li>On-boarding without friction: including environment provisioning.</li> <li>Frequent feature release: get to customer's hands as quick as possible. Release on daily frequency should be possible.</li> <li>Rapid market response, to pivot to a new direction.</li> <li>Instant customer feedback: with metrics on feature adoption.</li> </ul> <p>The important metrics to consider:</p> <ul> <li>Usage, consumption, system/tenant health.</li> <li>Survey and customer satisfaction.</li> <li>Engagement data.</li> </ul>"},{"location":"sa/saas/#architecture-landscape","title":"Architecture Landscape","text":"<p>There are a set of shared services that we will find in any SaaS solution which supports the high level view introduced in figure 1. Those services are:</p> <ul> <li>Admin Console: SaaS provider administrative application. It may include a landing web app to get tenant registering. The administration services are supporting the SaaS business. </li> <li>On-Boarding: Complex solution to provision all the environment.</li> <li>Identity: Identity is a very important element of SaaS. Connect users to tenant.</li> <li>Tenant management: As a multi-tenancy platform, <code>tenant</code> is a main business entity.</li> <li>User management: Each tenant has one to many users of the vendor software.</li> <li>Metering and billing: How to get tenant metrics and how to isolate billing.</li> <li>Analytics: Service responsible to gather usage metrics, and drive new use case of cost optimization, customer churn assessment...</li> <li>Monitoring and infrastructure management: For integrating into the cloud provider and manage compute, storage, networking resources.</li> </ul> <p>The figure below illustrates the integration of those services within the landscape:</p> <p></p> <p>Figure 4</p> <p>Amazon Cognito is used as an OpenID identity provider.</p> <p>The app domain is the specific ISV solution. DevSecOps is to support devOps practices and security validation. </p> <p>The shared services can be developed using a serverless / container approach or using AMI images for EC2 instances. We will adopt a container approach so we can more quickly adapt those service runtime execution according to the demand, and deploy them on Kubernetes cluster.</p> <p>The shared services deployment on EKS will look like in the following diagram within the VPC of the software vendor (not all services are presented):</p> <p></p> <p>Figure 5</p> <p>Some of those services will use AWS managed services like DynamoDB to persist tenant and user data, Cognito for authentication to their platform, EFS for sharing file, S3...</p> <p>The figure above demonstrates that the control-plane services are replicated into different availability zone, inside of the same EKS cluster.</p> <p>Bridge or Pool runs in the SaaS vendor VPC, and Web and App tiers are shared and persist data in same DB (may be different schema or a dedicated tenantID column in tables). </p> <p>Now on the SaaS provider's customer side we may use kubernetes namespace to deliver tenant isolation. The SaaS control plane runs and the customers own use of the solution runs in the same k8s cluster. Dedicated worker nodes can be used. In the figure 6, the ISV solution is instantiated as pods in different namespaces (gree, purple, red colors) for each tenant. The control plane, shared services run in the same cluster. All the solution is managed inside of this unique k8s cluster. </p> <p></p> <p>Figure 6</p> <p>Silo Isolation model may be achieve with VPC per tenant. Therefore the supporting approach will be to deploy the solution in dedicated EKS cluster in the customers VPC.</p> <p></p> <p>Figure 7</p> <p>Registering a tenant generates Terraform scripts to create the needed infrastructure and deploy the needed components to support one of the tenant isolation selected. The type of isolation can also being related to the tenant profile. Gold being on its own cluster for example. </p> <p>Coming back to the big-data processing platform example, we also need to consider data and storage partitioning, how to isolate data for each tenant. Long term persistence of object can be done in S3, now buckets can be defined in the VPC of the SaaS provider or in the customer's VPC. All the metadata about customer's interactions with the platform, the jaob execution can be ingected back to the ISV platform and then analytics can be used to do product and features usage assessment.</p> <p></p> <p>As mentioned before the solution could also being deployed on EC2s which may looks like:</p> <p></p> <p>Which maps to the following provisionned environment with classical HA deployment within a region / VPC, two AZs, private and public subnets and gateway &amp; application load balancer. </p> <p></p> <p>IAM roles and policies are used to support isolation. </p> <p>IAM policies should help isolating some resources in AWS services like S3 bucket or dynamoDB tables, while other like queues can be shared. </p>"},{"location":"sa/saas/#operations","title":"Operations","text":"<p>For SaaS, we need to focusing on monitoring the environments and applications health, and sometime at the tenant level (tenant experience). May be considering the following dashboard elements:</p> <ul> <li>Most active tenants.</li> <li>Tenant resources consumption.</li> <li> <p>Feature / services usage.</p> </li> <li> <p>consumption is for resource usages, more an internal metric. % of resource used by a tenant. </p> </li> <li>metering is a billing construct: # of users, product features usage,... </li> </ul> <p>Those numbers are very important to get how the revenue are coming from and the cost too. The figure below illustrates a classical e-commerce market place use case where basic tier customers are using a lot of resources and do not generate a lot of revenue, as they are trying product features and no more, while advanced tier better target their usage and own sell operation and therefore are generating bigger revenue. </p> <p></p> <p>What to measure when focusing on consumption? request count, execution time (latency of a function), CPU impact, Memory impact?</p>"},{"location":"sa/saas/#deeper-dive","title":"Deeper dive","text":"<p>The content on this note comes from:</p> <ul> <li>Saas at AWS</li> <li>Tod Golding form AWS SaaS factory: Deconstructing SaaS: A Deep Dive into Building Multi-tenant Solu has the diagrams above plus a deep discussion on how to secure and support tenants to access pooled resources like content in DynamoDB. (Need further study).</li> <li>Serverless SaaS documentation</li> <li>DevCon 2022: Building a customizable, multi-tenant serverless orchestration framework for bulk-data ingestion.</li> <li>SaaS youtube playlist.</li> <li>SaaS head start, using ready-made solutions to accelerate adoption.</li> <li>Open source project - SaaS Boost with the corresponding AWS SaaS boost git repo.</li> <li>AWS SaaS factory reference architecture git repo</li> <li>Aug 2021 blog: Building a Multi-Tenant SaaS Solution Using AWS Serverless Services</li> </ul>"},{"location":"sa/sol-design/","title":"Solution design fun with AWS services","text":"<p>This section presents examples of solution design using AWS services, with links to external demonstrations when available.</p>"},{"location":"sa/sol-design/#migration","title":"Migration","text":""},{"location":"sa/sol-design/#migrate-an-old-not-maintained-app","title":"Migrate an old not maintained app","text":"<ul> <li>When application uses custom OS configurations, use Application Migration Service.</li> <li>Replatform existing database server to Amazon RDS. AWS Database Migration Service gives the ability to convert DB to a supported RDS engine such as MySQL. AWS Schema Conversion Tool supports the schema conversion to Amazon RDS.</li> </ul>"},{"location":"sa/sol-design/#networking","title":"Networking","text":""},{"location":"sa/sol-design/#hybrid-integration-with-2-aws-regions","title":"Hybrid integration with 2 AWS regions","text":""},{"location":"sa/sol-design/#security","title":"Security","text":""},{"location":"sa/sol-design/#use-identity-center-user-to-perform-aws-cli","title":"Use Identity Center user to perform AWS CLI","text":"<ul> <li>Create an identity center in one region, and add users</li> <li>Create IAM service role, for CodeDeploy, for each target compute environment: for lambda uses <code>AWSCodeDeployRoleForLambda</code>. (See this note for detail).</li> </ul>"},{"location":"sa/sol-design/#blocking-an-ip-address","title":"Blocking an IP address","text":"<p>There are multiple solutions to support blocking an IP@ or a range of IP@ depending of the deployment.</p> Simplest using NACL <p>Define network inbound rule to deny a specific IP@ on all traffic types.</p>"},{"location":"sa/sol-design/#using-identity-federation","title":"Using identity federation","text":"<ul> <li>Users are identified with corporate directory, or 3nd party identity provider.</li> <li>Users need to work on AWS resources, or with applications that access AWS resources</li> <li>Cognito is a first solution for mobile and web apps, as it supports 3nd party IdP compatible with OpenID Connect (OIDC). The AWS SDK is used to create an identities for the users so they can be authenticated. It support anonymous sign-ins.</li> <li>When existing on-premises uses SAML 2.0, we can create a trust between the AWS as service provider and the org as IdP. Use SAML to provide the users with federated single-sign on (SSO) to the AWS Management Console or federated access to call AWS API operations.</li> <li>Alternatively we can build a custom identity broker app, calling assumeRole or GetFederationToken to AWS STS to get temporary credentials (access key ID, a secret access key, and a session token). Apps can then use the temporary credentials to make calls to AWS directly.</li> <li>In a SAML federation scenario, the IdP is responsible for the authentication and coarse grained authorization of all AWS users. AWS is then able to leverage and consume these identity assertions according to a preestablished cryptographic trust.</li> </ul> <p>For study or demonstration purposes, the identity store could be OpenLDAP the identity provider, Shibboleth. See this workshop: A Self-Directed Journey to AWS Identity Federation Mastery - Open Source variant. Once both are running on a server (like EC2), we can configure Shibboleth to offer authentication services for AWS using the <code>relying party</code> configuration.</p> <ul> <li>In a SAML federation, the Identity Provider can pass various attributes about the user, the authentication method, or other points of context to the service provider (in this case AWS) in the form of SAML attributes. In order for SAML federation for AWS to function properly, several attributes are required.</li> <li>The IdP is entirely responsible for authorizing which roles within AWS a given user can assume. The groups in LDAP are the source of truth. </li> <li>The first attribute is the <code>Roles</code> attribute, which specifies combinations of AWS IAM Roles and authorizing AWS IAM Identity Providers that the user is authorized to assume. The mapping can be done from a <code>memberOf</code> attribute from LDAP which includes groups the user belong to. This information is mapped to a IAM Role ARN. The group name in LDAP needs to follow a naming convention, like: <code>AWS-Account Number-Role Name</code>.</li> <li>Apply separation of duties: users will have different roles in different accounts.</li> <li>The second AWS specific attribute is <code>RoleSessionName</code>. This attribute is recorded as part of the identity within CloudTrail to ensure that we can properly account for the federated user's actions. It is mapped from the LDAP <code>uid</code> of the user.</li> <li> <p>Once IdP configuration is done, we need to configure AWS to trust the authentication and authorization information the IdP provides. We need the SAML metadata which details all of the encryption and signing certificates that IdP will use.</p> <p></p> </li> <li> <p>Add IAM Roles in each AWS Account to match the groups define in LDAP, using SAML as trusted entity</p> <p></p> </li> <li> <p>Add any security policy the role needs to be able to perform on AWS service.</p> </li> <li>User authenticates using the IdP login page, once done, the user is redirected to AWS role chooser page (as a user may have multiple AWS IAM roles). Also for CLI, and <code>aws configure</code> will use the keys and token from the STS response which was authenticated by the SAML assertion returned by the IdP.</li> <li>Resource policy can auhorize federated users too. For example S3 resource policy may look like: Deny all s3 actions to the bucket, but only when the aws:userid is not the following list:</li> </ul> <pre><code>    \"Statement\": [\n      {\n        \"Sid\": \"AllowSpecificUsersAndInstances\",\n        \"Effect\": \"Deny\",\n        \"Principal\": \"*\",\n        \"Action\": \"s3:*\",\n        \"Resource\": [\n          \"arn:aws:s3:::&lt;YOUR_BUCKET_HERE&gt;\",\n          \"arn:aws:s3:::&lt;YOUR_BUCKET_HERE&gt;/*\"\n        ],\n        \"Condition\": {\n           \"StringNotLike\": {\n              \"aws:userid\": [\n                \"&lt;ROOT_ACCOUNT&gt;\",\n                \"&lt;EC2RoleId:RoleSessionName&gt;\",\n                \"&lt;FederationRoleId:RoleSessionName&gt;\"\n              ]\n    ...\n</code></pre>"},{"location":"sa/sol-design/#stateless-with-persistence-app","title":"Stateless with persistence app","text":""},{"location":"sa/sol-design/#how-to-change-an-ec2-instance-type-while-the-host-runs-a-webapp-without-impacting-client-apps","title":"How to change an EC2 instance type, while the host runs a webapp, without impacting client apps?","text":"<p>The simplest example is one EC2 with a Java based webapp deployed on it. Upgrading the EC2 instance type, brings the webapp down.</p> <p></p> The basic solution <p>Use Elastic IP address (public or private) so clients keep the IP @. (EC2&gt; Elastic IP addresses&gt; Allocate Elastic IP address). </p> <p>Recall that we need to be able to SSH to the EC2 instance, to install language runtime, copy the tar of the app to the linux EC2 /home/users or somewhere else. Do not forget to use the .pem file downloaded when creating the EC2. <code>scp -i ec2host.pem ...</code></p> Cleaner solution <p>We should add a DNS domain and subdomain with Amazon Route 53 records, two ALBs, one per AZ, with Health Checks to Route 53, restricted security groups and public and private subnets, auto scaling group, and may be, use one reserved EC2 instance per AZ to reduce long term cost.</p> <p></p> <p>For the backend/database solution use a HA service like Aurora or DynamoDB.</p>"},{"location":"sa/sol-design/#stateful-app","title":"Stateful app","text":"<p>Keep data (shopping cart) into session. We can use the stickiness on ELB and session affinity. Or use a user cookie, with the app verifying the cookie content. </p> <p>When we need to keep more data than what the cookie can save, then we keep the sessionID in the cookie and deploy an ElasticCache to store data with the key of sessionID.</p> <p>Final transactional data can be persisted in RDS, and with RDS replication we can route specific traffic (\"/products\") which is read/only to the RDS Read replicas. Which can also being supported by having the product in the ElasticCache.</p> <p></p>"},{"location":"sa/sol-design/#sharing-images","title":"Sharing images","text":"<p>There are multiple solutions for this problem depending of the requirements:</p> <ol> <li>S3 with CloudFront for static content </li> <li> <p>With EC2 and EFS:</p> <p></p> </li> </ol>"},{"location":"sa/sol-design/#disaster-recovery-solution","title":"Disaster Recovery Solution","text":"<p>We try to address disaster and recovery for different needs, knowing that not all applications need active/active deployment.</p>"},{"location":"sa/sol-design/#backup-multi-regions","title":"Backup multi-regions","text":"<p>The simplest resilience solution is to use backup and restore mechanism. Data and configuration can be moved to S3 in the second region. For even longer time we can use Glacier. Use database service to ensure HA at the zone level, and replicate data within AZ.</p> <p></p> <p>RPO will be average time between snapshots - and RTO at the day level.</p>"},{"location":"sa/sol-design/#warm-region","title":"Warm region","text":"<p>For applications, where we want to limit out of services time, the approach is to replicate AMI images so app servers, in DR region, can be restarted quickly. And Database are replicated and warm on the second region. Web servers are also warm but not receiving traffic.</p> <p></p> <p>If something go down in region 1, the internet facing router (53) will route to local balancers in second region.</p> <p>RTO is now in minutes, and RPO average time between DB snapshots. </p>"},{"location":"sa/sol-design/#active-active-between-multi-regions","title":"Active - Active between multi regions","text":""},{"location":"sa/sol-design/#write-global-read-local-pattern","title":"Write global - read local pattern","text":"<p>Users close to one region will read from this region and all write operations go to a global service / region.</p> <p></p> <p>Database replications and snapshot replications are done to keep data eventually consistent between regions. Those synchronisations are in sub second.</p>"},{"location":"sa/sol-design/#write-to-origin-read-local-pattern","title":"Write to origin - read local pattern","text":"<p>To increase in complexity, R/W can go the local region. So when a user writes new records, he/she is associated to a region, so the application is sharding the data. When the user moved to another region, write operation will still go to the first region, while read could happened on the region close to him.</p> <p>This applies to applications with write to read ratio around 50%.</p>"},{"location":"sa/sol-design/#write-read-local-anti-pattern","title":"Write / read local (anti) pattern","text":"<p>This pattern uses two master DB, one in each region so user can write and read locally. Dual writes, in each region, at the same time may generate the same key but record will have different data.  You have inconsistency, and it is difficult to figure out, and rollback. So use this pattern only if you cannot do the two previous patterns.</p>"},{"location":"sa/sol-design/#aws-services-supporting-ha-and-dr-multi-regions","title":"AWS Services supporting HA and DR multi-regions","text":"<ul> <li>S3</li> <li>EBS</li> <li>DynamoDB</li> </ul>"},{"location":"sa/sol-design/#saas-deployment-for-an-isv","title":"SaaS deployment for an ISV","text":"<p>See also SaaS considerations.</p> <p>The ISV provides a data platform to manage data with performance and feature set compare to data warehouse but at the scale and low cost of data lake.  On top of the data their platform helps to develop ML model, visualize data and build integrate view of distributed, structured and unstructured data.</p> <p></p> <p>They developed a multi-tenancy SaaS platform to manage their customer deployments. Customers will create instance of this managed service in their own VPC and integrate with their own data source and data pipeline, to organize this data processing into data ingestion, data engineering, serving layer and analytics. Part of their platform is also to provide easy integration into AWS data services like Kinesis, S3, RDS, Redshift\u2026 This ISV is gathering data from the customer usage of their platform, get real time metrics, and manage all this in the AWS cloud using existing managed services as S3, Glacier for long persistence, streaming, redshift,\u2026</p>"},{"location":"sa/sol-design/#a-question-of-caching","title":"A question of caching","text":"<p>We have different level of information caching with different throughput. </p> <p></p> <ul> <li>Routes and DNS are cached in Route 53</li> <li>CloudFront can cache static data, with 100k requests/s</li> <li>ALB can have some caching of API response</li> <li>or API Gateway can do caching too, with 10k r/s</li> <li>Computer layer is slower but can scale horizontally, using ASG, Lambda or container on Fargate</li> <li>Database layer can scale with read replicas, or local read accelerator like DAX for Dynamodb.</li> <li>ElasticCaches can be used with Redis or Memcached.</li> <li>EBS volumes or instance store can be used to cache data in the EC2 machine.</li> <li>When decoupling components using queueing, caching can be done too.</li> <li>CloudFront Edge can be used to cache content from S3, which has baseline performance of 5500 GET per prefix/s. </li> <li>Encryption with KMS will impact those number as KMS has 10k API calls per region</li> </ul>"},{"location":"sa/sol-design/#amazon-codedeploy-demonstration","title":"Amazon CodeDeploy Demonstration","text":"<ul> <li> <p>Deploy an application from GitHub. See this repo AWS-CodeDeployGitHubDemo. The git repository has a <code>appspec</code> file which define scripts to run in different stages of the deployment. The app uses HTTPD and a basic HTML page.</p> <p></p> </li> <li> <p>Create a role with EC2 and codedeploy as trusted principals</p> <pre><code>\"Statement\": [\n    {\n        \"Sid\": \"\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Service\": [\n                \"codedeploy.amazonaws.com\",\n                \"ec2.amazonaws.com\"\n            ]\n        },\n        \"Action\": \"sts:AssumeRole\"\n    }\n]\n</code></pre> <p>With the inline policy like</p> <pre><code>\"Statement\": [\n    {\n        \"Action\": [\n            \"s3:Get*\",\n            \"s3:List*\"\n        ],\n        \"Effect\": \"Allow\",\n        \"Resource\": [\n            \"arn:aws:s3:::jb-data-set/*\",\n             \"arn:aws:s3:::aws-codedeploy-us-west-2/*\"\n             ]\n    }\n]\n</code></pre> </li> <li> <p>Add an instance profile and add the role to the instance profile</p> <pre><code>aws iam create-instance-profile --instance-profile-name CodeDeployDemo-EC2-Instance-Profile\naws iam add-role-to-instance-profile --instance-profile-name CodeDeployDemo-EC2-Instance-Profile --role-name CodeDeployDemo-EC2-Instance-Profile\n</code></pre> </li> <li> <p>Create an EC2 instance with CodeDeploy Agent: use the instance profile created above and the following user data:</p> <pre><code>sudo yum update\nsudo yum install -y ruby wget\nwget https://aws-codedeploy-us-west-2.s3.us-west-2.amazonaws.com/latest/install\nchmod +x ./install\nsudo ./install auto\nsudo service codedeploy-agent status\n</code></pre> </li> <li> <p>Add an application in CodeDeploy:</p> <pre><code>aws deploy create-application --application-name CodeDeployGitHubDemo-App\naws deploy create-deployment-group --application-name CodeDeployGitHubDemo-App --ec2-tag-filters Key=Name,Type=name,Value=CodeDeployDemo --deployment-group-name CodeDeployGitHubDemo-DepGrp --service-role-arn CodeDeployDemo-EC2-Instance-Profile\n</code></pre> </li> <li> <p>Create a deployment</p> <pre><code>aws deploy create-deployment \\\n        --application-name CodeDeployGitHubDemo-App \\\n        --deployment-config-name CodeDeployDefault.OneAtATime \\\n        --deployment-group-name CodeDeployGitHubDemo-DepGrp \\\n        --description \"My GitHub deployment demo\" \\\n        --github-location repository=jbcodeforce/,commitId=a40chars-string-from-commit-id\n</code></pre> </li> </ul>"},{"location":"sa/well-architectured/","title":"AWS Well Architectured","text":"Updates <p>Created Oct 2023 - update 10/10/2023 - Reliability</p> <p>The AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. </p> <p>By using the Framework, customers will learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud.</p> <p>It was started as a white paper from Werner Vogels.</p> <p>Those are the questions to ask for designing a cloud native solution by understanding the potential impact . All hardware are becoming software.</p> <p>Workload represents interrelated applications, infrastructure, policies, governance and operations. </p>"},{"location":"sa/well-architectured/#traditional-environment-challenges","title":"Traditional environment challenges","text":"<ul> <li>Had to guess infrastructure needs.</li> <li>Could not afford to test at scale.</li> <li>Had a fear of change.</li> <li>Could not justify experiments.</li> <li>Face architecture that was frozen in time.</li> </ul>"},{"location":"sa/well-architectured/#six-pilards","title":"Six pilards","text":"<p>When architecting technology solutions, never neglect the six pillars of:</p> <ul> <li>Operational Excellence</li> <li>Security</li> <li>Reliability</li> <li>Performance Efficiency</li> <li>Cost Optimization</li> <li>Sustainability</li> </ul> <p>Each pillar has questions, context and best practices.</p>"},{"location":"sa/well-architectured/#design-principles","title":"Design Principles","text":"<ul> <li>Stop guessing your capacity needs: use as much or as little capacity as you need, and scale up and down automatically.</li> <li>Test systems at production scale, then decommission the development resources.</li> <li>Automate to make architectural experimentation easier, using infrastructure as code.</li> <li>Allow for evolutionary architectures: the capability to automate and test on demand lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.</li> <li>Drive architectures using data: In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload.</li> <li>Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.</li> </ul>"},{"location":"sa/well-architectured/#jumpstart-an-assessment","title":"Jumpstart an assessment","text":"<p>In AWS console, search for <code>well architected</code>.</p> <p></p> <p>With Lenses to define a set of question to ask.</p> <p></p>"},{"location":"sa/well-architectured/#operational-excellence","title":"Operational Excellence","text":"<p>Support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures. </p> <p>Four area of focus:</p> <ul> <li>Organization: define clear responsabilities, roles, and success interdependencies.</li> <li>Prepare: design architecture for operations like telemetry (logs, metrics...), improve flow to deliver to production, mitigate deployment risks and react quickly to issue, understand operational readiness.</li> <li>Operate: understand workload health, operation health (like # features deploy per time unit), achievement of business outcome. Runbooks and playbooks should define escalation process and consistent response to events, and define ownership for each action. Assess applications at risk.</li> <li>Evolve: learn from experience, get feedback loops to identify areas of improvement, make improvements, share with teams.</li> </ul>"},{"location":"sa/well-architectured/#oe-design-principles","title":"OE- Design principles","text":"<ul> <li>Perform operations as code. Apply same methodology for infrastructure as we do for application code. Use configuration management application.</li> <li>Use separate environments for development, testing and production activities.</li> <li>Make frequent, small, reversible changes.</li> <li>Refine operations procedures frequently. Set up regular game days to review and validate that all procedures are effectives.</li> <li>Anticipate failure: Perform \u201cpre-mortem\u201d exercises to identify potential sources of failure so that they can be removed or mitigated. </li> <li>Learn from all operational failures.</li> </ul> <p>Use established runbooks for well-understood events, and use playbooks to aid in investigation and resolution of issues. </p> <p>AWS CloudWatch is used to aggregate and present business, workload, and operations level views of operations activities.</p>"},{"location":"sa/well-architectured/#oe-assessment","title":"OE- Assessment","text":"Questions to assess Note How do you determine what your priorities are? How do you structure your organization to support your business outcomes? How does your organizational culture support your business outcomes? How do you design your workload so that you can understand its state? How do you reduce defects, ease remediation, and improve flow into production? How do you mitigate deployment risks? How do you know that you are ready to support a workload? How do you understand the health of your workload? How do you understand the health of your operations? How do you manage workload and operations events? How do you evolve operations?"},{"location":"sa/well-architectured/#oe-deeper-dive","title":"OE- Deeper dive","text":"<ul> <li>100 and 200 labs</li> </ul>"},{"location":"sa/well-architectured/#security","title":"Security","text":"<p>Encompass the ability to protect data, systems and assets by controlling access and get visibility on who does what.</p> <p>It encompasses 5 items:</p> <ul> <li>Identity and access management</li> <li>Detection</li> <li>Infrastructure protection</li> <li>Data protection: involves identifying and classifying the data, and using controls and patterns to keep data confidential while preserving its integrity and availability at rest and in transit.</li> <li>Incident response: defines the processes to be in place to respond to and mitigate the potential impact of security incidents.</li> </ul>"},{"location":"sa/well-architectured/#security-design-principles","title":"Security Design principles","text":"<ul> <li>Apply security at all layers.</li> <li>Use fine-grained access controls.</li> <li>Automate security best practices.</li> <li>Protect data in transit and at rest.</li> <li>Define strong sign-in mechanisms.</li> <li>Centralized identity provider.</li> <li>Grant least privilege.</li> <li>Reduce permissions continuously.</li> </ul>"},{"location":"sa/well-architectured/#security-assessment","title":"Security- Assessment","text":"Questions to assess Answer How do you manage identities for people and machines? How do you manage permissions for people and machines? How do you detect and investigate security events? How do you protect your network resources? How do you protect your compute resources? How do you classify your data? How do you protect your data at rest? How do you protect your data in transit? How do you anticipate, respond to, and recover from incidents?"},{"location":"sa/well-architectured/#security-tools","title":"Security- Tools","text":"<p>Organize workload using AWS accounts, and use AWS Organizations and Amazon Control Tower to manage accounts and define common guard rails to apply security best practices.</p> <p>IAM helps control access for users and machines to workloads and AWS services.</p> <p>CloudTrail logs, AWS API calls, and CloudWatch provides monitoring of metrics with alerting.</p> <p>Ensure that you have a way to quickly grant access for your security team, and automate the isolation of instances as well as the capturing of data and state for forensics.</p> <p>Amazon Cognito is an identity provider and supports integration to multiple login providers.</p> <p>AWS Config records resource configuration changes and evaluate them against rules we configure.</p>"},{"location":"sa/well-architectured/#security-deeper-dive","title":"Security- deeper dive","text":"<ul> <li>300 labs</li> </ul>"},{"location":"sa/well-architectured/#reliability","title":"Reliability","text":"<p>The ability of a workload to perform its intended function correctly and consistently when it is expected to. Reliability requires that the workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues.</p> <p>Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. It addresses DR (restore service) and Availability (prevent loss of service).</p> <p>Shared responsibility model for resiliency: AWS for resiliency of the cloud: infrastructure. Customer is responsible for resiliency in the cloud: adopting instance deployment across multiple locations, support self-healing, design for resiliency. Be sure to have continuous testing of infrastructure failure, have good observability and failure management, manage service quotas and other constraints.</p>"},{"location":"sa/well-architectured/#reliability-design-principles","title":"Reliability- Design principles","text":"<ul> <li>Use highly available network connectivity for the workload with public endpoints: These endpoints and the routing to them must be highly available. Use Elastic Load Balancing which provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases.</li> <li>Implement loosely coupled dependencies.</li> <li>Deploy the workload to multiple locations: Distribute workload data and resources across multiple Availability Zone.</li> <li>Automatically recover from failure.</li> <li>Design to mitigate failures: fails fast and limit queues, asynchronous communication, throttling requests, make all requests idempotent.</li> <li>Test recovery procedures: use automation to simulate different failures or to recreate previous failure scenarios.</li> <li>Scale horizontally to increase aggregate workload availability. Understand physical limits and resource constraints: use service quotas and limits.</li> <li>Stop guessing capacity: monitor demand and workload utilization, and automate the addition or removal of resources to maintain the optimal level to satisfy demand without over- or under-provisioning.</li> <li>Manage change in automation.</li> <li>Use fault isolation: adopt multiple availability zones, use multiple regions where needed, and apply bulkhead architecture</li> </ul> <p>bulkhead architecture</p> <p>Elements of the architecture are isolated into pools, so one failure is isolated by a subset of requests. See building fault tolerant with bulkhead pattern.</p>"},{"location":"sa/well-architectured/#reliability-assessment","title":"Reliability- Assessment","text":"<p>Before architecting any system, define foundational requirements that influence reliability:</p> Reliability Questions Notes How do you manage service quotas and constraints? How do you plan your network topology? How do you design your workload service architecture? How do you design interactions in a distributed system to prevent failures? Search to improve mean time between failures (MTBF) How do you design interactions in a distributed system to mitigate or withstand failures? Look to improve mean time to recovery (MTTR) How do you monitor workload resources? Monitor Logs and metrics How do you design your workload to adapt to changes in demand? Add or remove resources automatically to adapt to the demand How do you implement change? Controlled changes to deploy new feature, patched or replaced in a predictable manner How do you back up data? Helps to address RTO and RPO How do you use fault isolation to protect your workload? Components outside of the boundary should not be affected by the failure. How do you design your workload to withstand component failures? How do you test reliability? testing is the only way to ensure that it will operate as designed. Use practice Failure Mode Engineering Analysis (FMEA) How do you plan for disaster recovery (DR)? Regularly back up your data and test your backup files to ensure that you can recover from both logical and physical errors <p>Use AZ, regions and bulkhead (elements of an application are isolated into pools so that if one fails, the others will continue to function).</p> <p>Use chaos engineering, playbooks and test function and performance.</p>"},{"location":"sa/well-architectured/#reliability-deeper-dive","title":"Reliability- deeper dive","text":"<p>See also files in labs/reliability folder.</p> <ul> <li>Disaster recovery labs from AWS</li> <li>Reliability workshop.</li> </ul>"},{"location":"sa/well-architectured/#performance-efficiency","title":"Performance efficiency","text":"<p>Use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve. The focus is on:</p> <ul> <li>Selection of the resource types and features, for compute, network, storage and databases. Use benchmarking and load testing. </li> <li>Review updated features and new services.</li> <li>Monitoring existing deployment. Use performance alarms while monitoring, using CloudWatch. </li> <li>Trade-offs to tune for efficiency. Trade-off where space is used to reduce processing time. Caching is also a level we can use for trade offs.</li> </ul>"},{"location":"sa/well-architectured/#performance-design-principles","title":"Performance- Design principles","text":"<ul> <li>Democratize advanced technologies: delegate to your cloud vendor.</li> <li>Go global in minutes.</li> <li>Use serverless architectures.</li> <li>Experiment more often.</li> <li>Consider mechanical sympathy: always use the technology approach that aligns best with your workload goal</li> </ul> <p>In AWS, compute is available in three forms: instances, containers, and functions. Storage is available in three forms: object, block, and file. Databases include relational, key-value, document, in-memory, graph, time series, and ledger databases.</p>"},{"location":"sa/well-architectured/#performance-assessment","title":"Performance- Assessment","text":"Performance Questions Comments How do you select the best performing architecture? Use a data-driven approach to select the patterns and implementation for your architecture and achieve a cost effective solution. How do you select your compute solution? Varies based on application design, usage patterns, and configuration settings How do you select your storage solution? The optimal storage solution for a system varies based on the kind of access method (block, file, or object), patterns of access (random or sequential), required throughput, frequency of access (online, offline, archival), frequency of update (WORM, dynamic), and availability and durability constraints How do you select your database solution? Consider requirements for availability, consistency, partition tolerance, latency, durability, scalability, and query capability How do you configure your networking solution? varies based on latency, throughput requirements, jitter, and bandwidth How do you evolve your workload to take advantage of new releases? How do you monitor your resources to ensure they are performing? How do you use tradeoffs to improve performance? improve performance by trading consistency, durability, and space for time and latency. <p>Amazon CloudWatch is a monitoring and observability service that provides you with data and actionable insights to monitor your workload, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health</p> <p>AWS cloudformation to define infrastructure as code.</p>"},{"location":"sa/well-architectured/#cost-optimization","title":"Cost optimization","text":"<p>Run systems to deliver business value at the lowest price point possible.</p>"},{"location":"sa/well-architectured/#cost-design-principles","title":"Cost- Design principles","text":"<ul> <li>Implement Cloud Financial Management practices / team</li> <li>Be aware of expenditure and usage awareness. On-demand, spot instances...</li> <li>Adopt a consumption model use approapriate instances and resources for our system.</li> <li>Measure overall efficiency: Measure the business output of the workload and the costs associated with delivering it.</li> <li>Stop spending money on undifferentiated heavy lifting. Match supply to demand.</li> <li>Optimize over time.</li> </ul>"},{"location":"sa/well-architectured/#cost-assessment","title":"Cost- Assessment","text":"Questions to assess Comments How do you govern usage? How do you monitor usage and cost? How do you decommission resources? How do you evaluate cost when you select services? Trade off between low level service like EC2, S3, EBS versus higher level like DynamoDB How do you meet cost targets when you select resource type, size and number? How do you plan for data transfer charges? <p>As your requirements change, be aggressive in decommissioning resources, entire services, and systems that you no longer require. Stay up to date on enw services and features.</p>"},{"location":"sa/well-architectured/#cost-tools","title":"Cost- Tools","text":"<ul> <li>Cost Explorer</li> <li>Cloudwatch</li> </ul>"},{"location":"sa/well-architectured/#sustainability","title":"Sustainability","text":"<p>Focuses on environmental impacts, especially energy consumption and efficiency.</p> <ul> <li>Scale infrastructure to continually match user load and ensure that only the minimum resources required to support users are deployed. </li> <li>Identify redundancy, underutilization, and potential decommission targets.</li> <li>Adapt EC2 instance type to the processing needs, and decomission when no more needed.</li> <li>Implement patterns for performing load smoothing and maintaining consistent high utilization of deployed resources to minimize the resources consumed. </li> <li>Monitor workload activity to identify application components that consume the most resources.</li> <li>Understand how data is used within your workload, consumed by your users, transferred, and stored.</li> <li>Lifecycle data to more efficient, less performant storage when requirements decrease, and delete data that\u2019s no longer required.</li> <li>Adopt shared storage and single sources of truth to avoid data duplication and reduce the total storage requirements of your workload</li> <li>Back up data only when difficult to recreate</li> <li>Minimize the amount of hardware needed to provision and deploy</li> <li>Use automation and infrastructure as code to bring pre-production environments up when needed and take them down when not used.</li> <li>Distribute workload among multiple AZs help to improve utilization so consume less.</li> </ul> <p>When selecting a region, use one near Amazon renewable energy projects. Some regions has over 95% renewable energy.</p>"},{"location":"sa/well-architectured/#well-architected-review","title":"Well-architected review","text":"<p>Provides customers and partners with consistent to reviewing their workloads against current AWS best practices, and gives advice on how to architect workload for cloud. The workload is the focus of the review.</p> <p>The benefits:</p> <ul> <li>Build and deploy faster</li> <li>Lower or mitigate risks</li> <li>Make informed decisions</li> <li>Learn AWS best practices</li> </ul> <p>Tool is available in AWS console.</p>"},{"location":"sa/well-architectured/#more-readings","title":"More readings","text":"<ul> <li>Well-architected website</li> <li>System design exercices using AWS services</li> <li>Disaster Recovery AWS</li> <li>WAS Well-Architected training</li> </ul>"},{"location":"serverless/","title":"Serverless","text":"<p>Info</p> <p>Created 04/2023 - Updated 1/19/2024</p> <p>Serverless helps organization to get to the market quickly, removing heavy lifting work like servers management. Serverless should not be lambda / function programming. </p>"},{"location":"serverless/#characteristics-of-serverless-computing","title":"Characteristics of serverless computing","text":"<ul> <li>No server infrastructure to provision or to manage.</li> <li>Flexible scaling: scale automatically in response to the load, and capacity defined in term of unit of work. Scale to zero too.</li> <li>Automated HA and fault tolerance.</li> <li>No idle capacity. Only charged for the total invocations of our function, and the amount of time that our function is actually running. </li> </ul> <p>Serverless on AWS is supported by a lot of services:</p> <ul> <li>AWS Lambda: Function programming model. Limited by time - short executions, runs on-demand, and automated scaling. Pay per call, duration and memory used.</li> <li>DynamoDB: no sql db, with HA supported by replication across three AZs. Millions req/s, trillions rows, 100s TB storage. Low latency on read. Support event-driven programming with streams: lambda function can read the stream (24h retention). Table oriented, with dynamic attribute but primary key. 400KB max size for one document. It uses the concept of Read Capacity Unit and Write CU. It supports auto-scaling and on-demand throughput. A burst credit is authorized, when empty we get ProvisionedThroughputException. Finally it use the DynamoDB Accelerator to cache data to authorize micro second latency for cached reads. Supports transactions and bulk tx with up to 10 items. </li> <li>AWS Cognito: gives users an identity to interact with the applications.</li> <li> <p>AWS API Gateway: API versioning, websocket support, different environment, support authentication and authorization. Handle request throttling. Cache API response. SDK. Support different security approaches:</p> <ul> <li>IAM:<ul> <li>Great for users / roles already within your AWS account</li> <li>Handle authentication + authorization</li> <li>Leverages Sig v4</li> </ul> </li> <li>Custom Authorizer:<ul> <li>Great for 3<sup>rd</sup> party tokens</li> <li>Very flexible in terms of what IAM policy is returned</li> <li>Handle Authentication + Authorization</li> <li>Pay per Lambda invocation</li> </ul> </li> <li>Cognito User Pool:<ul> <li>You manage your own user pool (can be backed by Facebook, Google login etc\u2026)</li> <li>No need to write any custom code</li> <li>Must implement authorization in the backend</li> </ul> </li> </ul> </li> <li> <p>Amazon S3</p> </li> <li>Amazon SNS &amp; SQS: for asynchronous communication between components.</li> <li>AWS Kinesis Data Firehose</li> <li>Aurora Serverless</li> <li>AWS Step Functions</li> <li>Fargate and ECS</li> <li>Managed Service for Apache Kafka: MSK.</li> <li>Kinesis Data Stream</li> </ul> <p>Lambda@Edge is used to deploy Lambda functions alongside a CloudFront CDN, it is for building more responsive applications, closer to the end user. Lambda is deployed globally. Here are some use cases: Website security and privacy, dynamic webapp at the edge, search engine optimization (SEO), intelligent route across origins and data centers, bot mitigation at the edge, real-time image transformation, A/B testing, user authentication and authorization, user prioritization, user tracking and analytics.</p> <p>Getting started with serverless computing on AWS.</p>"},{"location":"serverless/#container","title":"Container","text":"<p>A container is a standardized unit that packages the code and its dependencies together. This package is designed to run reliably on any platform, as container creates its own independent environment.</p> <p>The difference between containers and virtual machines (VMs) can be illustrated by the following figure:</p> <p></p> <p>In AWS, containers run on EC2 instances. For example, you might have a large instance and run a few containers on that instance. While running one instance is easy to manage, it lacks high availability and scalability. Most companies and organizations run many containers on many EC2 instances across several Availability Zones. Container orchestration can be done in EKS and/or ECS.</p>"},{"location":"serverless/#event-driven-architecture","title":"Event-driven architecture","text":"<p>Design by thinking: \"what are the events that should trigger something in my system\". API requests are becoming events, data updates in the DynamoDB are also events, event design allows to build very loosely coupled systems that integrate with other systems.</p> <ul> <li>Event-driven architectures use events to communicate between services and invoke decoupled services.</li> <li>AWS Lambda is a serverless compute service that is well suited to event-driven architectures.</li> <li>Lambda has native support for events produced by message and streaming services like Amazon Simple Queue Service (SQS), Amazon Simple Notification Service (SNS), and Amazon Kinesis.</li> <li>Adding SQS queue in front of Lambda to create asynch connection between API request and the function processing. API Gateway sends request to Amazon SQS. Lambda service uses long polling to poll the SQS queue.</li> <li>By default, Lambda reads up to five batches at a time and sends each batch within one event to our function.</li> <li>If a Lambda function returns errors when processing messages, Lambda decreases the number of processes polling the queue.</li> <li>When the Lambda function successfully processes a batch, the Lambda service deletes that batch of messages from the queue.</li> <li>If any of the messages in the batch fail, all items in the batch become visible on the queue again. Which will generate duplicates.</li> <li>When a message continues to fail, send it to a dead-letter queue, another SQS queue.</li> <li> <p>AWS Step Functions can coordinate the distributed components of the application and keep the need for orchestration out of the code.</p> <p></p> </li> </ul>"},{"location":"serverless/#communicate-status-updates-back-to-client","title":"Communicate status updates back to client","text":"<ul> <li>At the client side, the client app may poll on a specific  <code>/status/{mid}</code> API with a messageID returned by the SQS queue response, and may use another end point (<code>/order/{id}</code>) to get the transaction result. This approach adds additional latency to the data consistency within the client. </li> <li>Client may use Webhooks as user-defined HTTP callbacks. The callback is called from a SNS topic subscriber. Using SNS with HTTP subscribers can use retries and exponential backoff until the webhook succeeds. To handle webhook failures, we need a defined process with an agreed-upon expiration time window and number of retries to communicate to the webhook.</li> <li>As a 3nd solution is to use WebSocket with GraphQL integrated with <code>AppSync</code>. This is a bidirectional communication between the API and the client. With AWS AppSync, clients can automatically subscribe and receive status updates as they occur.</li> <li>Amazon Simple Notification Service (Amazon SNS) uses a publication/subscription. It is possible to define filter policies to topic subscription. The filter policy contains attributes that define which messages that subscriber receives, and Amazon SNS compares message attributes to the filter policy for each subscription. </li> <li>Amazon SNS supports a very high number of subscribers per topic, which means we can kick off a large number of processes with one SNS message.</li> </ul>"},{"location":"serverless/#data-processing-patterns","title":"Data processing patterns","text":"<ul> <li>Data streams is supported with Kinesis Data Streams, and clients use KCL (consumer), KPL (producer) APIs and AWS SDK to interact with stream. Lambda can be a consumer or producer.  Recall that Write is 1000 records/s or 1MB/s and read is 5 tx/s up to 10k records with 2MB/s</li> <li>Data processing never happens in isolation. The correct architecture depends on how the data will be used, transformed, and stored.</li> <li>Kinesis Data Firehose does not support exactly once delivery and record ordering. When we associate a Lambda function to a Data Firehose stream to transform data on the stream, Data Firehose tries the invocation three times and then skips that batch of records. Records that failed to process are delivered to an Amazon S3 bucket in a <code>processing_failed</code> folder.</li> <li> <p>Kinesis Data Streams guarantees order delivery of data records in the shard as well as exactly-once delivery of records. If Lambda fails on one record in a batch, the entire batch (and thus the shard) is blocked until either the message processes successfully or the retention period for the data records in the batch expires.</p> </li> <li> <p>Data Streams Dev guide.</p> </li> <li>Data Firehose Dev guide.</li> <li>Kinesis Data Analytics: Preprocessing Data Using a Lambda Function.</li> </ul>"},{"location":"serverless/#error-handling-by-execution-model","title":"Error Handling by execution model","text":"API Gateway - synch <ul> <li>Timeout \u2013 API Gateway has a 30-second timeout. If the Lambda function hasn't responded to the request within 30 seconds, an error is returned. </li> <li>Retries \u2013 There are no built-in retries if a function fails to execute. </li> <li>Error handling \u2013 Generate the SDK from the API stage, and use the backoff and retry mechanisms it provides.</li> </ul> SNS - asynch <ul> <li>Timeout \u2013 Asynchronous event sources do not wait for a response from the function's execution. Requests are handed off to Lambda, where they are queued and invoked by Lambda. </li> <li>Retries \u2013 Asynchronous event sources have built-in retries. If a failure is returned from a function's execution, Lambda will attempt that invocation two more times for a total of three attempts to execute the function with its event payload. We can use the Retry Attempts configuration to set the retries to 0 or 1 instead. If Lambda is unable to invoke the function (for example, if there is not enough concurrency available and requests are getting throttled), Lambda will continue to try to run the function again for up to 6 hours by default. We can modify this duration with Maximum Event Age.</li> <li>Error handling - Use the Lambda destinations OnFailure option to send failures to another destination for processing. Alternatively, move failed messages to a dead-letter queue on the function. </li> </ul> Kinesis Data Streams <ul> <li>Timeout \u2013 When the retention period for a record expires, the record is no longer available to any consumer. (24h). As an event source for Lambda, we can configure Maximum Record Age to tell Lambda to skip processing a data record.</li> <li>Retries \u2013  By default, Lambda retries a failing batch until the retention period for a record expires. We can configure Maximum Retry Attempts so that our Lambda function will skip retrying a batch of records.</li> <li>Error handling - Configure an OnFailure destination on our Lambda function so that when a data record reaches the Maximum Retry Attempts or Maximum Record Age, we can send its metadata, such as shard ID and stream Amazon Resource Name (ARN), to an SQS queue or SNS topic.</li> </ul> SQS Queue <ul> <li>Timeout \u2013 When the visibility timeout expires, messages become visible to other consumers on the queue. Set our visibility timeout to 6 times the timeout we configure for our function.</li> <li>Retries \u2013 Use the <code>maxReceiveCount</code> on the queue's policy to limit the number of times Lambda will retry to process a failed execution.</li> <li>Error handling - Write our functions to delete each message as it is successfully processed. Move failed messages to a dead-letter queue configured on the source SQS queue.</li> </ul> Dead-letter <p>We may turn on dead-letter queues and create dedicated dead-letter queue resources using Amazon SNS or Amazon SQS for individual Lambda functions that are invoked by asynchronous event source.</p>"},{"location":"serverless/#distributed-tracing-with-x-ray","title":"Distributed tracing with X-Ray","text":"<p>Another important best practice for event-driven, decoupled applications is the use of distributed tracing to give us insight into issues, latency spikes or bottlenecks across our distributed architecture. </p> <p>X-Ray SDK can be used in any application. Can be enabled in Lambda, API Gateway, SQS, SNS, and used to do lineage and application flow with a  visual representation of what\u2019s happening at each service integration point, highlighting successful and failed service calls, and time to execute a service.</p> <p>When X-Ray is enabled, it gets data from services as segments, and groups them by request into traces.</p> <p>Instead of sending trace data directly to X-Ray, each client SDK sends JSON segment documents to a daemon process listening for UDP traffic.</p> <p>We may add annotations to segments in order to group traces to help us identify performance stats on application-specific operations. Annotations are indexed within X-Ray.</p>"},{"location":"serverless/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Create serverless workflow</li> <li>10 Things Serverless Architects Should Know</li> <li>Create a State Machine that Implements the Saga Pattern</li> <li>Error handling and automatic retries in AWS Lambda</li> <li>Handle Lambda Errors in API Gateway</li> <li>Step function example retry and catch</li> <li>AWS Event Fork Pipelines are prebuilt applications, available in the AWS Serverless Application Repository, that we can use in our serverless applications. Each application is packaged with an AWS Serverless Application Model (SAM) template that defines the AWS resources used.</li> </ul>"},{"location":"serverless/#serverless-architecture-patterns","title":"Serverless architecture patterns","text":""},{"location":"serverless/#few-write-lot-of-reads-app-to-do","title":"Few write / Lot of reads app (To Do)","text":"<p>The mobile application access application via REST HTTPS through API gateway. This use serverless and users should be able to directly interact with s3 buckets.  They first get JWT token to authenticate and the API gateway validates such token. The Gateway delegates to a Lambda function which goes to Dynamo DB.</p> <p></p> <p>Each of the component supports auto scaling. To improve read throughput cache is used with DAX. Also some of the REST request could be cached in the API gateway. As the application needs to access S3 directly, Cognito generates temporary credentials with STS so the application can authenticate to S3. User's credentials are not saved on the client app. Restricted policy is set to control access to S3 too.</p> <p>To improve throughput we can add DAX as a caching layer in front of DynamoDB: this will also reduce the sizing for DynamoDB. Some of the responses can also be cached at the API gateway level.</p>"},{"location":"serverless/#serverless-hosted-web-site-blog","title":"Serverless hosted web site (Blog)","text":"<p>The public web site should scale globally, focus to scale on read, pure static files with some writes. To secure access to S3 content, we use Origin Access Identity and Bucket policy to authorize read only from OAI.</p> <p></p> <p>To get a welcome message sent when a user register to the app, we can add dynamoDB streams to get changes to the dynamoDB and then calls a lambda that will send an email with the Simple Email Service.</p> <p>DynamoDB Global Table can be used to expose data in different regions by using DynamoDB streams.</p>"},{"location":"serverless/#microservice","title":"Microservice","text":"<p>Services use REST api to communicate. The service can be dockerized and run with ECS. Integration via REST is done via API gateway and load balancer.</p> <p></p>"},{"location":"serverless/#paid-content","title":"Paid content","text":"<p>User has to pay to get content (video). We have a DB for users. This is a Serverless solution. Videos are saved in S3. To serve the video, we use Signed URL. So a lambda will build those URLs.</p> <p></p> <p>CloudFront is used to access videos globally. OAI for security so users cannot bypass it. We can't use S3 signed URL as they are not efficient for global access.</p>"},{"location":"serverless/#big-data-pipeline","title":"Big Data pipeline","text":"<p>The solution applies the traditional collect, inject, transform and query pattern.</p> <p></p> <p>IoT Core allows to collect data from IoT devices. Kinesis is used to get data as streams, and then FireHose upload every minute to S3. A Lambda can already do transformation from FireHose. As new files are added to S3 bucket, it trigger a Lambda to call queries defined in Athena. Athena pull the data and build a report published to another S3 bucket that will be used by QuickSight to visualize the data.</p>"},{"location":"serverless/#designing-for-serverless","title":"Designing for Serverless","text":"<p>Some of the common design patterns to consider when migrating to serverless:</p> <ul> <li>Leapfrog: bypass interim steps and go straight from an on-premises legacy architecture to a serverless cloud architecture.</li> <li>Organic: lift and shift. Experiment with Lambda in low-risk internal scenarios such as log processing or cron jobs.</li> <li>Strangler: incrementally and systematically decomposes monolithic applications by creating APIs and building event-driven components that gradually replace components of the legacy application. New feature branches can be serverless first, and legacy components can be decommissioned as they are replaced.</li> </ul>"},{"location":"serverless/#good-questions-to-assess","title":"Good questions to assess","text":"Question Comment What does the application do and how are its components organized? Establish a bounded context for each microservice. Assess entity and data store. How can you break your data needs based on the command query responsibility segregation (CQRS) pattern? Strangle data by needs. Decouple write transaction from queries at scale. How does the application scale and what components drive the capacity you need? Look at independent scaling, and high scaling demand. Do you have schedule-based tasks? cron jobs are good targets to replace with Lambda functions Do you have workers listening to a queue? Decoupling may already being adopted, EDA discussions may be started. Where can you refactor or enhance functionality without impacting the current implementation? Look at API needs, ALB needs. <p>See Serverless app Lens in AWS Well Architected.</p>"},{"location":"serverless/#scaling","title":"Scaling","text":"<p>While considering ways to scale a serverless architecture, there are some best practices to consider:</p> <ul> <li>Separate the application and database.</li> <li>Take advantage of the AWS Global Cloud Infrastructure.</li> <li>Identify and avoid heavy lifting.</li> <li>Monitor for percentile.</li> <li>Refactor as we go.</li> </ul> <p>We need to know the capabilities and service limits of the services that we\u2019re integrating. Considering assessing timeouts, retry behaviors, throughput, payload size.</p> <p>For example with API Gateway, there are configuration options for each API, that can help to improve access like with edge-optimized endpoint, using CloudFront distribution. Or use cache to avoid reaching backend. </p> <p>Consider looking after the following questions:</p> <ul> <li>Who will be using each API? </li> <li>Where will they be accessing it from, and how frequently? </li> <li>What are the downstream systems that could impact performance? </li> <li>Do you need to throttle requests at the \"front door\" to prevent overloading a downstream component?</li> </ul> <p>Info</p> <p>For Lambda, lambda authorizer lets us implement a custom authentication scheme, that uses either request parameters to determine the caller's identity, or bearer token authentication. We need to anticipate the volume of authorization requests to ensure we don\u2019t exceed our concurrency limits. We can use auth caching to mitigate this.</p> <ul> <li>API Gateway quotas</li> </ul> <p>When Amazon SQS is used as a Lambda event source, the Lambda service manages polling the queue. We can set the batch size, concurrency limit, and timeout. Lambda defaults to using five parallel processes to get messages off a queue. If the Lambda service detects an increase in queue size, it will automatically increase how many batches it gets from the queue, each time. The maximum concurrency is 1000. </p> <p>SQS will continue to try a failed message up to the maximum receive count specified in the re-drive policy, at which point, if a dead-letter queue is configured, the failed message will be put into the dead-letter queue and deleted from your SQS queue.</p> <p>If the visibility timeout expires, before your Lambda function has processed the messages in that batch, any message in that batch that hasn\u2019t been deleted by your function will become visible again. This may generate duplicate processing on the same message. The best practice, is to set your visibility timeout to six times the function timeout.</p> <p>A concurrency limit that is set too low could definitely cause an increase in messages hitting the dead-letter queue. Anything less than five will generate errors out of the gate because Lambda defaults to five processes pulling batches off the queue and invoking functions.</p> <ul> <li>See using lambda with SQS</li> <li>Configure SQS queue to trigger lambda</li> <li>Tutorial SQS / Lambda, KMS key to encrypt message, using Dead Letter Queue.</li> </ul>"},{"location":"serverless/#security","title":"Security","text":"<p>One benefit of serverless architectures is that when we rely on AWS managed services, we shift more parts of the AWS shared responsibility model toward AWS. Our responsibility includes: Follow the principle of least privilege; protect data at rest and in transit; and audit the system for changes, unexpected access, unusual patterns, or errors. </p> <ul> <li>Best Practices for Security, Identity, &amp; Compliance</li> <li>OWASP Secure Coding Practices Quick Reference Guide</li> </ul> <p>Here is a classical story of a customer use case where security context is in each components and networking between those elements:</p> <p></p> <p>Amazon API Gateway is the front door to your application, so start preventing unauthorized access to our APIs. There are three options for authorization*:</p> <ul> <li>AWS Identity and Access Management: for client in AWS</li> <li>Amazon Cognito: supports sign-in/sign-up capabilities or acts as an identity provider (IdP) in a federated identity scenario.</li> <li>Lambda authorizers: An API Gateway Lambda authorizer invokes a Lambda function to authenticate/validate a user against your existing IdP.</li> </ul> <p></p> <p>WAF offers capabilities to avoid SQL injection, OWASP top 10, XSS rules.</p> <p>Using IAM policies and roles to define who can invoke the Lambda function, and what services the function can reach. Don\u2019t share IAM roles between functions.</p> <p>To pass data to a Lambda function we can use environment variable, AWS Systems Manager Parameter Store, AWS secret manager. Secrets Manager has the added benefit of secrets rotation and cross-account access. </p> <p>Think about security end to end at each integration point in the distributed architecture. Here is a deeper dive from re:inforce 2019 about Securing Serverless app with security patterns to consider, like authentication, defense from external threat, control application access with IAM policies, lambda execution role...</p>"},{"location":"serverless/#observability","title":"Observability","text":"<p>Defintion: A measure of how well internal states of a system can be inferred from knowledge of that systems's external outputs.</p> <p>Understand what is happening in our systems and applications to detect, investigate, and remediate problems.</p> <p>The goal is to be able to answer:</p> <ul> <li>Which decoupled service is up or down?</li> <li>Which service is causing a performance bottleneck?</li> <li>Slow application, service?</li> <li>How KPI or SLA are still being met?</li> </ul> <p>To achieve insight, use the three different pillars of observability: Metrics/ monitoring, trace and logs. (See CloudWatch metrics, Amazon CloudWatch Logs, and AWX X-Ray for end to end tracing).</p> <p>Lambda built-in metrics include total invocations, errors, duration, throttles, dead-letter queue errors, and iterator age, which helps troubleshoot stream-based invocations.</p> <ul> <li>Amazon API Gateway dimensions and metrics</li> <li>Working with Lambda function metrics</li> <li>Available CloudWatch metrics for Amazon SQS</li> <li>Monitoring Step Functions Using CloudWatch</li> <li>Monitoring Amazon SNS topics using CloudWatch</li> <li>Monitoring the Amazon Kinesis Data Streams Service with Amazon CloudWatch</li> </ul>"},{"location":"serverless/#auditing","title":"Auditing","text":"<p>Audit the configuration history, configuration changes, and all other API calls to facilitate security and governance in the serverless applications.</p> <p>Use Cloud Trail to record API usage, so we can assess who modify the resources. Automatically enabled at account creation. A trail is a configuration that enables the delivery of CloudTrail events to an Amazon Simple Storage Service (Amazon S3) bucket, Amazon CloudWatch Logs, and Amazon CloudWatch Events. If you need to maintain a longer history of events, you can create your own trai</p> <p>Use AWS Config to track changes to the application, that can track activities on some AWS services. We can define rules to enforce compliance. AWS Config flags the resource and the rule as noncompliant and notifies you through Amazon Simple Notification Service (Amazon SNS).</p>"},{"location":"serverless/#graphql-vs-restful-apis","title":"GraphQL vs RESTful APIs","text":"<ul> <li>GraphQL Foundation 2018: Support single static path and POST request.</li> <li> <p>The query is in the body as a json document.</p> <pre><code>query getAllCars(zip) {\n    getAllCars(zip){\n        carid\n        model\n        capacity\n        location\n    }\n}\n</code></pre> </li> <li> <p>AppSync supports GraphQL and uses resolver to retrieve fields from data sources and mapping to schema.</p> </li> <li>Consider development experience: RESTful is supported by all browsers, a lof of HTTP client code, widely accepted standard. GraphSQL uses SDK, and is less common adopted.</li> <li>With a lot of datasources, which may change often, GraphQL may be a good fit. </li> <li>When we do not control the client then REST is a better choice.</li> </ul> Benefits REST GraphQL Data Size Defined by server and schema Can change based on data requested by client Payload shape Static schema defined by server Defined by what client ask for Client support Works with browsers Requires JS libraries"},{"location":"serverless/#fargate","title":"Fargate","text":"<p>When running ECS and EKS on EC2, we are still responsible for maintaining the underlying EC2 instances. With Fargate, we do not have to manage EC2 instances.</p> <p>AWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure, it removes the need to provision and manage servers, let you specify and pay for resources per application, and improves security through application isolation by design.</p> <p>It natively integrates with AWS Identity and Access Management (IAM) and Amazon Virtual Private Cloud (VPC).</p> <p>It uses the following constructs:</p> <ul> <li>Task Definition: to define application containers, image URL, CPU and memory needed... It is immutable, so any update creates new version. It can define 1 to 10 container definitions. The container definition part includes memory reservation and cpu allocation. </li> <li>Task: A running instance of a Task Definition. A task can have multiple containers running in parallel. Each task has its own Elastic Network Interface with a private IP @ from the subnet.</li> <li>Cluster: infrastructure isolation boundary. Tasks run in the cluster. </li> <li>Service: Endpoint to support ELB integration and do mapping to tasks</li> </ul> <p>Pricing is based on what we provision, then the task level CPU and memory and the per-second billing with one minute minimum.</p> <p>To share data between containers, Fargate provides 4 GB volumes space per task, and the volume is mounted in the container. By default, Fargate tasks are given a minimum of 20 GiB of free ephemeral storage.</p> <p>For IAM security, the same policies can be set as we do for EC2. We still need to add an <code>execution role</code> to define access to ECR to download images and CloudWatch for monitoring.</p>"},{"location":"serverless/#examples","title":"Examples","text":"<ul> <li>ECS nginx demo in ECS playground</li> <li>Deploy Flask App from private ECR repository with AWS Console</li> <li>Deploy Flask App with CDK</li> </ul>"},{"location":"serverless/#step-function","title":"Step Function","text":"<p>See separate note.</p>"},{"location":"serverless/#aws-app-runner","title":"AWS App runner","text":"<p>AWS App Runner, a fully managed container application service that makes it easy for customers without any prior containers or infrastructure experience to build, deploy, and run containerized web applications and APIs </p>"},{"location":"serverless/#amazon-lightsail","title":"Amazon Lightsail","text":"<p>Lightsail is a VPS provider and is a useful way to get started with AWS for users who need a solution to build and host their applications on AWS Cloud. Lightsail provides developers with compute, storage, and networking capacity and capabilities to deploy and manage websites and web applications in the cloud. Lightsail includes VMs, containers, databases, content delivery network (CDN), load balancers, Domain Name System (DNS) management,</p>"},{"location":"serverless/#read-more","title":"Read more","text":"<ul> <li>10 Things Serverless Architects Should Know</li> <li>Serverless Land - AWS portal on Serverless</li> <li>Build on serverless - season 2 - 2020</li> <li>2021 re:Invent playlist for serverless breakout sessions</li> <li>Technical talks</li> <li>AWS serverless getting started - official page</li> <li>Getting started with serverless - For developer. Create a lambda, API gateway to send message from GitHub to Slack.  https://hooks.slack.com/services/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 </li> <li>Live coding well-architected solution - 2020</li> <li>Serverless patterns collection: a portal to access SAM template and code or AWS CDK.</li> <li>Implementing Safe Lambda Deployments with CodeDeploy</li> <li>API patterns Restful vs GraphQL APIs - George Mao 2020.</li> </ul>"},{"location":"serverless/#workshops","title":"Workshops","text":"<ul> <li> <p> Serverless patterns - immersion days done in cloud 9 Development machine and serverless-immersion-day folder. </p> <ul> <li> Module 1: API Gateway - Lambda - DynamoDB</li> <li> Module 2: Cognito, Lambda authorizer, API Gateway, Lambda (CRUD), DynamoDB + SAM. Sam template in labs/serverless/ws-serverless-patterns folder</li> <li> Module 3: CI/CD</li> <li>Assessment: 5 :octicons-star-fill-24:{ .star } but too much of mixing lambda = serverless. </li> </ul> </li> <li> <p> Build, Secure, Manage Serverless Applications at Scale on AWS</p> </li> <li> <p> Decoupled microservice workshop</p> <ul> <li> Lab 1 - Fan-out &amp; message filtering: use SNS to get multiple services (lambda fcts) to subscribe to the topic, API Gtw exposes API to a Lambda which persist data in DynamoDB and send to SNS. The function is not idempotent, and not transactional. See labs/serverless/asynchronous-messaging/fanout-sns</li> <li> Lab 2 - Topic-queue chaining &amp; load balancing: in previous example, it is possible subscriber service may miss topic messages. See labs/serverless/asynchronous-messaging/topic-queue-chaining</li> <li> Lab 3 - scatter pattern by sending messages through multiple channels. The application comprises of API Gateway endpoints and lambda functions which provide the ability to send requests for quote and to query the responses. Service providers have subscribed to a SNS topic which is used to publish the request for quote (RFQ). On receiving the RFQ message, service providers send their response for quotes in a queue. The queue triggers a lambda function which loads the responses in a DynamoDB table. The response for quotes service queries all the responses based on request id. See labs/serverless/asynchronous-messaging/scatter-gather</li> <li> Lab 4 - Orchestration and coordination: to demonstrate how to support long lived transaction across distributed components using Step function to ensure data consistency across microservices. See labs/serverless/asynchronous-messaging/orchestration</li> </ul> </li> <li> <p> Serverless security workshop</p> </li> <li> Serverless Observability Workshop</li> <li> <p> Building event-driven architectures on AWS: it uses nested cloud formation templates reproduced in the <code>labs/eda</code> folder. it uses Amazon EventBridge, Cognito, Amazon SNS, Amazon SQS, AWS Lambda, Step functions...</p> <p></p> </li> </ul>"},{"location":"serverless/apigtw/","title":"Amazon API Gateway","text":"<p>Info</p> <p>Created 11/2023 - Updated 01/29/2024</p>"},{"location":"serverless/apigtw/#introduction","title":"Introduction","text":"<p>Fully managed service to define, deploy, and monitor APIs within a AWS region. It supports HTTP, REST, or WebSocket APIs. As illustrated in the diagram below, it also covers other API endpoints for different backends, even on-premises backends.</p> <p></p> <p>Figure 1: API Gateway to serve multiple end points</p> <p>It supports hundred of thousands of concurrent API calls. It can even cache backend response (at the stage level) to reduce calls to backend to improve request latency. Only GET methods are cached.</p> <p>APIs are deployed to \"Stages\" (e.g. dev, prod...). Stage can be rolled back as there is an history of each deployment.</p> <p>API Gateway is not for authentication, and it uses standard capabilities (Mutual TLS, JWT) or different services for authorization: Cognito user pools, resources policies, WAF, lambda authorizer...</p> <p>CloudWatch is used to trace access and execution loggings. CloudTrail helps logging and monitoring of API usage and API changes.</p> <p>API Gateway supports data validation of the required request parameters in the URI, the HTTP headers, and even the payload to adhere to JSON schema.</p> <p>One of the common integration is with Lambda. There are two types of integration, the proxy one passes the client request to the lambda function with header, query parameters, etc, while for the non-proxy the developer has to define the integration logic (data transformation) to match the Lambda parameters.</p> <p>For data transformation, mapping templates (Velocity Template Language) can be added to the integration request to transform the incoming request to the format required by the backend of the application or to transform the backend response payload to the format required by the method response.</p> <p>See the tutorial: \"Build an API Gateway REST API with Lambda non-proxy integration\" to go over all those capabilities.</p> <p>We can define a custom domain name to get our own URL endpoint and use base path (<code>/myservice</code>) mapping to go to the different URLs/APIs served by the domain. We can use API Gateway Version 2 to create and manage Regional custom domain names for REST APIs. The domain name needs to be registered to an internet domain registrar like Route 53.</p>"},{"location":"serverless/apigtw/#designing-considerations","title":"Designing considerations","text":"<p>Amazon API Gateway will automatically scale to handle the amount of traffic the API receives. It has a max timeout set to 29 seconds, and a max payload to 10MB per request. To protect against traffic spikes define throttling and caching settings.</p> <p>With API Gateway, we can set throttle and quota limits on our REST API consumers. This can be useful for things such as preventing one consumer from using all of the backend system\u2019s capacity or to ensure that the downstream systems can manage the number of requests sent through. The Throttle can be set per client and method, and has a quota per AWS account.</p> <p>API Gateway throttles requests (Clients may receive HTTP 429 Too Many Requests) to the API using the token bucket algorithm, where token counts requests. Specifically, API Gateway examines the rate and a burst of request submissions against all APIs in AWS account, per Region.</p> <p>To identify client we need to use API keys and set the throttle rules per key.</p> <p>It does not limit or throttle invocations to the backend operations.</p>"},{"location":"serverless/apigtw/#rest-api","title":"REST API","text":"<p>REST APIs and HTTP APIs are both RESTful API products. REST APIs support more features than HTTP APIs.</p> <p>API Gateway REST APIs use a synchronous request-response model. Always assess the type of endpoints:</p> <ul> <li>Edge-optimized API Gateway will automatically configure a fully managed CloudFront distribution to provide lower latency access to the API. It also reduces TLS connection overhead, and it is designed for globally distributed clients.</li> <li>Regional, where client apps run in the same region as the API gateway, and most likely as the backends (Also for HTTP api).</li> <li>VPC or Private to expose API from applications running within VPC.</li> </ul> <p>We can turn on API caching in API Gateway to cache the endpoint's responses, with TTL for 300s (max 3600s). Caches are defined per stage, and it is possible to override cache settings per method. The cache can be between 0.5GB to 237GB.</p> <p>Client can invalidate cache by using a HTTP header Cache-Control: max-age=0. It needs proper IAM permission.</p> <p>We can get client code generated from the REST api definition.</p>"},{"location":"serverless/apigtw/#http-api","title":"HTTP API","text":"<p>HTTP APIs are optimized for building APIs that proxy to Lambda functions or HTTP backends, making them ideal for serverless workloads. But they do not currently support API management functionality</p>"},{"location":"serverless/apigtw/#open-api-standard","title":"Open API standard","text":"<p>It is possible to upload an OpenAPI definition of an existing API in a API Gateway.</p>"},{"location":"serverless/apigtw/#websocket-api","title":"WebSocket API","text":"<p>With a WebSocket API, the client and server can send messages to each other at any time. The API Gateway manages the persistence and state needed to connect it to the client. When a client sends a message over its WebSocket connection, this results in a route request to the WebSocket API. The request will be matched to the route with the corresponding route key in API Gateway. </p> <p>The client apps connect to your WebSocket API by sending a WebSocket upgrade request. If the request succeeds, the $connect route is invoked while the connection is being established. Until the invocation of the integration you associated with the $connect route is completed, the upgrade request is pending and the actual connection will not be established. If the $connect request fails, the connection will not be made.</p> <p>A backend endpoint is also referred to as an integration endpoint and can be a Lambda function, an HTTP endpoint, or an AWS service action.</p> <p>API Gateway uses selection expressions as a way to evaluate the request and response context and produce a key.</p> <p>After the connection is established, the client's JSON messages can be routed to invoke a specific backend service based on message content. When a client sends a message over its WebSocket connection, this results in a route request to the WebSocket API. The request will be matched to the route with the corresponding route key in API Gateway. </p>"},{"location":"serverless/apigtw/#api-access","title":"API access","text":"<p>When it comes to granting access to our APIs, we need to think about two types of permissions:</p> <ol> <li>Who can invoke the API: To call a deployed API, or refresh the API caching, the caller needs the execute-api permission.  Create IAM policies that permit a specified API caller to invoke the desired API method.</li> <li>Who can manage the API: To create, deploy, and manage an API in API Gateway, the API developer needs the apigateway permission.</li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"execute-api:Invoke\",\n                \"execute-api:ManageConnections\"\n            ],\n            \"Resource\": \"arn:aws:execute-api:*:*:*\"\n        }\n    ]\n}\n</code></pre> <p>See API Gateway identity-based policy examples for detail on how to do these.</p>"},{"location":"serverless/apigtw/#parameter-tampering","title":"Parameter tampering","text":"<p>We can configure API Gateway to do basic validation to verify:</p> <ul> <li>The required request parameters in the URI, query string, and headers of an incoming request are included and non-blank.</li> <li>The applicable request payload adheres to the configured JSON schema request model of the method.</li> </ul> <p>We can also do a data mapping based on the JSON schema.</p>"},{"location":"serverless/apigtw/#an-api-frontend-to-s3","title":"An API frontend to S3","text":"<p>It is possible to define an API to upload doc to S3, but there is a risk that the file is bigger than 10GB, so the better solution is to use a Lambda function to serve the API, to get a S3 signed URL, and the client will use this URL to upload the file to S3. See this blog and this git repo, using SAM.</p>"},{"location":"serverless/apigtw/#scaling-and-quotas","title":"Scaling and quotas","text":"<ul> <li>Quota is account and region based.</li> <li>Support 10k rps, with burst absorption via a bucket of 5000 requests</li> <li>Limit in the URL size</li> <li>Limit on the number of domains</li> </ul>"},{"location":"serverless/apigtw/#pricing","title":"Pricing","text":"<ul> <li>Pay when APIs are in use at a set cost per million requests. Cost occurs for Data Transfer out of AWS.</li> <li>HTTP APIs are designed with minimal features so that they can be offered at a lower price. </li> <li>WebSocket APIs maintain persistent connections with clients for full-duplex communication. WebSocket APIs for API Gateway charge for the messages you send and receive. Also charged for the total number of connection minutes. the <code>$connect</code> route is to initiate the connection, <code>$disconnect</code> route to close the communication, and <code>$default</code> route when route selection cannot be assessed. </li> <li>Caching is billed by the hour.</li> <li>APIs can be sell as SaaS on Marketplace.</li> </ul>"},{"location":"serverless/apigtw/#monitoring","title":"Monitoring","text":"<ul> <li>Two level of execution logs, Error and Info. Turn CloudWatch logs for troubleshooting the API gtw.</li> <li>For REST APIs consider looking at the <code>CacheHitCount</code> and <code>CacheMissCount</code> metrics. Other metrics are Calls count, Latency, 4XX, 5XX, Integration latency</li> <li>For HTTP metrics are calls count, Latency, 4XX, 5XX, Integration latency, data processed</li> <li>For WebSocket, connect count, message count, integration error, client error, execution error, integration latency.</li> </ul>"},{"location":"serverless/apigtw/#lamba-authorizer","title":"Lamba Authorizer","text":"<p>It is an API Gateway feature that uses a Lambda function to control access to our API. Used for a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity.</p> <p>The Lambda takes the caller's identity as input and returns an IAM policy as output.</p> <p>There are two types of Lambda authorizers:</p> <ul> <li>A token-based Lambda authorizer </li> <li>A request parameter-based </li> </ul> <p>The lambda can:</p> <ul> <li>Calling out to an OAuth provider to get an OAuth access token.</li> <li>Calling out to a SAML provider to get a SAML assertion.</li> <li>Generating an IAM policy based on the request parameter values.</li> <li>Retrieving credentials from a database</li> </ul> <p>Here is a boilerplate for decode JWT</p>"},{"location":"serverless/apigtw/#hands-on","title":"Hands-on","text":""},{"location":"serverless/apigtw/#tutorials","title":"Tutorials","text":"<ul> <li>Basic API to front end a Lambda function.</li> <li>API Gateway tutorials</li> <li>Autonomous car manager with Lambda, API Gateway</li> <li>A lot of serverlessland examples use APIGTW.</li> <li>Serverless examples in this repo.</li> <li>Load an OpenAPI definition in API Gateway.</li> </ul>"},{"location":"serverless/apigtw/#an-end-to-end-quarkus-based-solution","title":"An end-to-end Quarkus based solution","text":"<p>See the CarRideManager microservice which exposes a Quarkus app running on ECS Fargate with Open API deployed on Amazon API gateway.</p>"},{"location":"serverless/apigtw/#deeper-dive","title":"Deeper dive","text":"<ul> <li>FAQs</li> <li>Using AWS Lambda with Amazon API Gateway.</li> </ul>"},{"location":"serverless/ecs/","title":"Amazon Elastic Container Service (Amazon ECS)","text":"<p>Amazon ECS is an end-to-end container orchestration service that helps to spin up new containers and manages them across a cluster of EC2 (or Fargate) instances, without managing a control plane. Created in 2014 with Linux containers.</p> <p>It maintains application availability and allows us to scale our containers up or down to meet the application's capacity requirements.</p> <p>Integrated with familiar features like Elastic Load Balancing, EBS volumes, VPC, and IAM. Simple APIs let us integrate and use our own schedulers or connect Amazon ECS into our existing software delivery process.</p> <p>It is possible to run container into two modes: EC2 or Fargate. For EC2, we can create the instances upfront or use an auto scaling group and start instances within it. We need to install the Amazon ECS container agent on each EC2 instances, the docker engine, and manage the EC2 ourselves...</p> <p></p> <p>Figure 1: ECS with EC2 instances</p> <p>Since 2018, with Fargate, as a serverless approach, we only need to specify the container configuration, services to expose the app and configuration:</p> <p></p> <p>Figure 2: ECS cluster with Fargate</p> <p>Fargate has various deployment options using on-demand, compute saving plans and spot instances. It uses the concept of control and data planes. The control plane runs on its own VPC owned by AWS, while the data plane is able to access customers' VPCs via ENI.</p> <p></p> <p>Figure 3: ECS cluster with Fargate</p> <p>The deployment steps are 1/ provision resources from a pool of EC2s, 2/ download the image from a registry like ECR, 3/ activate the workload with or withour load balancer. The task is now running.</p> <p>It is possible to combine Fargate and EC2 auto scaling group inside the same ECS cluster.</p> <p>ECS with EC2 launch type is charged based on EC2 instances and EBS volumes used. ECS with Fargate is charged based on vCPU and memory resources that the containerized application requests.</p> <p>The container is executed by defining Task Definition which describes cpu, memory, networking, volume mount, and IAM needs. Tasks are instances of Task Definition.</p> <pre><code>{\n    \"taskDefinitionArn\": \"arn:aws:ecs:....\",\n    \"containerDefinitions\": [\n        {\n            \"name\": \"simple-app\",\n            \"image\": \"....dkr.ecr.us-west-2.amazonaws.com/demo-customer-api:latest\",\n            \"cpu\": 10,\n            \"memory\": 300,\n            \"portMappings\": [\n                {\n                    \"containerPort\": 8080,\n                    \"hostPort\": 8080,\n                    \"protocol\": \"tcp\"\n                }\n            ],\n            \"essential\": true,\n        }],\n        \"requiresCompatibilities\": [ \"FARGATE\"],\n        \"cpu\": 128,\n        \"memory\": 512\n}\n</code></pre> <p>Task definition also includes placement strategies, like: random, binpack, spread, and placement constraints.</p> <p>Service groups identical tasks and exposes them for other applications via load balancer. Here is a task definition example, which includes CPU and memory capacities, with the docker image reference:</p> <p></p> <p>Running the task:</p> <p></p> <p>Task IAM role needs to be defined for the task to access specific AWS services such as S3, RDS, ... and it is referenced in the Task Definition, and EC2 Instance Profile roles is also defined so EC2 can call ECS control plane.</p> <p>Here is a classical network deployment. ALB can do dynamic port mapping to support routing to multiple instances of the application running within the same EC2:</p> <p></p> <p>ECS Tasks can be launched with different networking support:</p> <ul> <li>None: no network connection (? strange)</li> <li>Bridge: uses the docker virtual network</li> <li>Host: bypass docker and use host network interface</li> <li>awsvpc: each task get its own ENI and a private IP address. It is a simplified networking, and let access to security groups, VPC flow logs. This is the default for Fargate.</li> </ul> <p>See a demo for NGInx deployment on ECS.</p>"},{"location":"serverless/ecs/#value-propositions","title":"Value propositions","text":"<ul> <li>Run more than 2.5 billion tasks per week.</li> <li>70% of all containers from customer, run on ECS.</li> <li>The main reasons for ECS: faster time to market, lower cost, massive scale, reliable, and secure and compliant.</li> <li>Fargate is a serverless offering, no EC2 provisioning and lower level management.</li> <li>Fargate supports autoscaling without EC2 capacity management.</li> <li>Fargate pricing is based on on-demand based on vCPU and memory usage.</li> <li>Support OCI image standard and docker images.</li> <li>Support Firelens as a side car in each task to integrate with other open source logging products like Prometheus.</li> </ul>"},{"location":"serverless/ecs/#fargate-basic-deployment-steps","title":"Fargate basic deployment steps","text":"<ol> <li>Be sure to have a VPC, a security group for container with SSH from your development laptop, and allow inbound and outbound HTTP and HTTPS access from anywhere. Be sure to set port for the container (8080?)</li> <li>Define an execution IAM role taking examples from the ECS first run wizard permission documentation.. Define a role for the task to access other services like secrets, MQ...</li> <li>Build the docker image and push it to a ECR repository</li> <li>Create a <code>task definition</code>: Specify the container images, environment variables and any resources configurations... Add a Application Load Balancer. The task definition is a text file, in JSON format, that describes one or more containers.</li> <li> <p>Then define the cluster: it is a regional grouping of container instances. Cluster may have one to many EC2 instances. Try to use Fargate as runtime engine for a serverless deployment model.</p> <p></p> <p>With all the resources created automatically:</p> <p></p> <p>It automatically creates a VPC with two public subnets.</p> <p>Task definitions can be defined outside of a cluster, but services are associating task to cluster, subnets, and security groups.</p> </li> </ol>"},{"location":"serverless/ecs/#service-auto-scaling","title":"Service Auto Scaling","text":"<p>ECS service auto scaling helps to automatically increase/decrease the number of ECS task. It uses AWS Application Auto Scaling which specifies the service usage in term of CPU, memory and request count per target.</p> <p>There are 3 ways to define scaling rules:</p> <ul> <li>Target Tracking, based on value for a specific CloudWatch metric.</li> <li>Step Scaling, based on CloudWatch Alarm.</li> <li>Scheduled Scaling, based on a specified date/time.</li> </ul> <p>This is different than EC2 auto scaling which works at the EC2 instances, as application auto scaler works at the task level.</p>"},{"location":"serverless/ecs/#ecs-service-connect","title":"ECS Service Connect","text":"<p>ECS Service Connect is a Network capability to simplify service discovery, connectivity and traffic observability for ECS. It delivers service mesh capability without developer knowing it. It is intended for interconnecting multiple applications on Amazon ECS.</p> <p>ECS Service Connect is fully supported in AWS CloudFormation, AWS CDK, AWS Copilot, and AWS Proton for infrastructure provisioning, code deployments, and monitoring of our services.</p> <p>Tutorial: using service connect in Fargate with AWS CLI.</p>"},{"location":"serverless/ecs/#aws-copilot","title":"AWS Copilot","text":"<p>AWS Copilot provides an open-source CLI for developers to build, release, and operate production ready containerized applications on Amazon ECS. See this git repo to deploy nginx with Copilot.</p> <p>Here is a quick summary of common command:</p> <pre><code>brew install copilot-cli\ncopilot init\n</code></pre>"},{"location":"serverless/ecs/#others","title":"Others","text":"<ul> <li>IAM Roles are defined for each container. Others roles can be added to access ECS, ECR, S3, CloudWatch, ...</li> <li><code>EC2 Instance Profile</code> is the IAM Role used by the ECS Agent on the EC2 instance to execute ECS-specific actions such as pulling Docker images from ECR and storing the container logs into CloudWatch Logs.</li> <li>ECS Task Role is the IAM Role used by the ECS task itself. Used when our container wants to call other AWS services like S3, SQS, etc.</li> <li>EventBridge can define a rule to run a ECS task.</li> <li>See also ECS anywhere</li> </ul>"},{"location":"serverless/ecs/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Under the hood: AWS Fargate data plane</li> <li>Tutorials</li> <li>Some personal ECS playground with AWS console, CLI or CDK, and the messaging studies with CDK to deploy Fargate apps.</li> <li>CI/CD ECS Fargate workshop with github, Code deploy and blue/green deployment.</li> <li>Container from the Couch.</li> </ul>"},{"location":"serverless/eks/","title":"Elastic Kubernetes Service","text":"<p>Info</p> <p>Updated 9/12/2023</p> <p>Amazon EKS is a fully managed service to run Kubernetes. It is highly available, AWS manages the control plane, and user defines the worker nodes and then the workloads deployed:</p> <p></p> <ul> <li>There are 3 public subnets and 3 private subnets to be created across 3 availability zones and we provision the EKS node group in the private subnets and it communicates with the EKS control plane through the private cluster mode by using the private endpoint.</li> <li>Because the Pods inside the private EKS cluster might have to access the internet resources so we shall setup the NAT gateway in the public subnets.</li> <li>Application Load Balancer is used as the ingress to route the traffic for the deployed application in private subnets, and only ALB can access EKS cluster by referencing the ALB security group from EKS data plane security group. It is integrated with VPC for isolation, IAM for authentication, ELB for load distribution, and ECR for container image registry.</li> <li>Worker nodes can be EC2 instances in a <code>Managed Node Group</code> (EKS managed) or Fargate worker nodes as part of <code>Fargate profile</code>.</li> </ul>"},{"location":"serverless/eks/#major-characteristics","title":"Major characteristics","text":"<ul> <li> <p>Scale K8s control plane across multiple AZs.</p> <p></p> </li> <li> <p>No need to install, operate and maintain k8s cluster.</p> </li> <li>Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instance.</li> <li>It supports EC2 as worker nodes or Fargate to deploy serverless containers or nodes in AWS Outposts.</li> <li>Fully compatible with other CNSF kubernetes.</li> <li>Can be deployed on-premises with Amazon EKS Distro (EKS-D) distribution.</li> </ul> <p>The EKS node types are:</p> <ul> <li> <p>Managed node groups: to automate the provisioning and lifecycle management of EC2 (could be On-demand or spot instances). Nodes run using the latest Amazon EKS optimized AMIs. This is the recommended way to allocate capacity. EC2 are managed by AWS and assigned to a ASG managed by EKS. Nodes are tagged for auto-discovery by k8s cluster autoscaler. Can configure <code>Launch template</code> to be used with custom AMI or with custom user data. </p> <p>The following figure illustrates a classical multi-AZ, one ASG/MMG deployment for stateless applications.</p> <p></p> <p>While for stateful apps, we need to consider one MMG/ASG per AZ.</p> <p></p> </li> <li> <p>Self-managed nodes: nodes are managed by the user and attached to EKS cluster by using an ASG. User uses his own AMI-based EC2 instances that are part of the auto-scaling group and will serve as worker nodes for the cluster. Spot instances can be used to reduce cost.</p> </li> <li>AWS Fargate which represents a cost optimized deployment for EKS worker nodes. Each time a pod is created it is assigned to an existing EC2 instance and runs within a microVM. It works with ALB.</li> </ul> <p>Data volumes (EBS, EFS, FSx) are defined with StorageClass and they need to have Container Storage Interface compliant driver.</p> <p>See Pricing calculator: pay for cluster control plane, EC2 instances, Fargate or AWS outposts.</p>"},{"location":"serverless/eks/#cluster-management","title":"Cluster management","text":"<ul> <li>EKS runs a single tenant Kubernetes control plane for each cluster. 3 <code>etcd</code> instances in 3 AZs within one region.</li> <li>A set of add-ons are defined by default: CoreDNS, kube-proxy, VPC CNI. Others can be added like metrics server, cluster autoscaler, EFS CSI driver, EBS CSI driver, fluentbit (logging and metrics processor and forwarder).</li> <li>EKS uses IAM to provide authentication to the Kubernetes cluster, and k8s RBAC for authorization.</li> <li>EKS  hosts a public OIDC discovery endpoint per cluster containing the signing keys for the <code>ProjectedServiceAccountToken</code> JSON web tokens so external systems, like IAM, can validate and accept the Kubernetes-issued OIDC tokens. OIDC federation access allows user or app to assume IAM roles via the Secure Token Service (STS), enabling authentication with an OIDC provider, receiving a JSON Web Token (JWT), which in turn can be used to assume an IAM role.</li> </ul>"},{"location":"serverless/eks/#iam-role-for-service-account","title":"IAM role for service account","text":"<p>IAM role for service account helps to define role for pods and not the EC2 running the pods. This is a fine grained access control.</p> <p>To use IAM roles for service accounts in the cluster, we must create an IAM OIDC Identity Provider. </p>"},{"location":"serverless/eks/#fargate-profile","title":"Fargate profile","text":"<p>A fargate profile specifies which pods use Fargate. EKS uses special controller to schedule pods to Fargate. Pods must match a Fargate profile at the time that they're scheduled to run on Fargate. They run only on private subnets. Each pod is isolated within a microVM (Firecrakers).</p>"},{"location":"serverless/eks/#ecs-comparisons","title":"ECS comparisons","text":"<ul> <li>An EC2 instance with the ECS agent installed and configured is called a container instance. In Amazon EKS, it is called a worker node.</li> <li>An ECS container is called a task. In Amazon EKS, it is called a pod.</li> <li>While Amazon ECS runs on AWS native technology, Amazon EKS runs Kubernetes.</li> <li>ECS scale at thousand of tasks, EKS is still limited in number of nodes and total pods.</li> </ul>"},{"location":"serverless/eks/#what-to-do-the-first-time","title":"What to do the first time","text":"<ol> <li> <p>Install kubernetes tools like <code>kubectl</code> and AWS CLI:</p> <pre><code>sudo curl --silent --location -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.23.7/2022-06-29/bin/linux/amd64/kubectl\n\nsudo chmod +x /usr/local/bin/kubectl\n\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre> </li> <li> <p>Install jq, gettext...</p> <pre><code>sudo yum -y install jq gettext bash-completion moreutils\n# Verify the path\nfor command in kubectl jq envsubst aws\ndo\n    which $command &amp;&gt;/dev/null &amp;&amp; echo \"$command in path\" || echo \"$command NOT FOUND\"\ndone\n\n# Enable kubectl bash_completion\nkubectl completion bash &gt;&gt;  ~/.bash_completion\n. /etc/profile.d/bash_completion.sh\n. ~/.bash_completion\n</code></pre> </li> <li> <p>Download <code>eksctl</code> (eksctl.io). (It also installs <code>kubectl</code>)</p> <pre><code>brew tap weaveworks/tap\nbrew install weaveworks/tap/eksctl\n# Verify it\neksctl version\n</code></pre> </li> <li> <p>Create IAM Role and attach the required Amazon EKS IAM managed policy to it. Kubernetes clusters managed by Amazon EKS make calls to other AWS services on our behalf to manage the resources that we use with the service.</p> <pre><code># under the  labs/eks folder\naws iam create-role \\\n    --role-name myAmazonEKSClusterRole \\\n    --assume-role-policy-document file://\"eks-cluster-role-trust-policy.json\"\n# Attach the required Amazon EKS managed IAM policy to the role.\naws iam attach-role-policy \\\n    --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \\\n    --role-name myAmazonEKSClusterRole\n</code></pre> </li> </ol>"},{"location":"serverless/eks/#working-with-cluster","title":"Working with cluster","text":"<p>To create an EKS cluster with EC2 or Fargate, within its own VPC, in private subnets, with security groups, we can use different approaches:</p> <ul> <li>AWS Console.</li> <li>Using <code>eksctl</code> command line and a yaml config file defining the cluster.</li> <li>CDK to create the infrastructure and then EKS cluster itself. See examples in the product doc, see also a CDK example in <code>labs/cdk/eks-single</code> folder or the <code>labs/eks/eks-cdk</code> folder for a python based deployment.</li> <li>Using CloudFormation for the VPC infrastructure with a predefined stack.</li> </ul>"},{"location":"serverless/eks/#eksctl-approach","title":"eksctl approach","text":"<p>Based on the EKS workshop), using Cloud9, quick summary of steps:</p> <ol> <li>Create workspace in Cloud9 and install dependencies (See previous section).</li> <li>Create IAM role named <code>eks-admin</code> with AdministratorAccess, and modify the Cloud9, EC2 instance IAM role using: <code>Actions &gt; Security &gt; Modify IAM Role.</code></li> <li>Update Cloud9 workspace to disable Cloud9 to manage IAM credentials dynamically (This is not compatible with the EKS IAM authentication). <code>Gear &gt; AWS Settings &gt;</code> . </li> <li> <p>Save region and account as env variable and configure aws CLI:</p> <pre><code>echo \"export ACCOUNT_ID=${ACCOUNT_ID}\" | tee -a ~/.bash_profile\necho \"export AWS_REGION=${AWS_REGION}\" | tee -a ~/.bash_profile\naws configure set default.region ${AWS_REGION}\naws configure get default.region\n\n# validate that the Cloud9 IDE is using the correct IAM role\naws sts get-caller-identity --query Arn | grep eks-admin -q &amp;&amp; echo \"IAM role valid\" || echo \"IAM role NOT valid\"\n</code></pre> </li> <li> <p>Use <code>eksctl create cluster</code> which creates CloudFormation to deploy a EKS cluster with managed nodes:</p> <pre><code>eksctl create cluster -f eks-cluster.yaml\n</code></pre> <p>See the schema definition here.</p> <p>Find the cluster credentials added to <code>~/.kube/config</code></p> </li> <li> <p>Update kubeconfig to interact with the cluster</p> <pre><code>aws eks update-kubeconfig --name MyEKS --region ${AWS_REGION}\n</code></pre> </li> <li> <p>Verify kubectl works with the cluster</p> <pre><code>kubectl get nodes\n</code></pre> </li> <li> <p>Export worker role name</p> <pre><code>STACK_NAME=$(eksctl get nodegroup --cluster myeks -o json | jq -r '.[].StackName')\nROLE_NAME=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME | jq -r '.StackResources[] | select(.ResourceType==\"AWS::IAM::Role\") | .PhysicalResourceId')\necho \"export ROLE_NAME=${ROLE_NAME}\" | tee -a ~/.bash_profile\n</code></pre> </li> <li> <p>Create an eks-admin service account and cluster role binding</p> <pre><code>kubectl apply -f eks-admin-service-account.yaml\n</code></pre> </li> <li> <p>Deploy Prometheus (be sure to have helm cli)</p> <pre><code>kubectl create namespace prometheus\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm upgrade -i prometheus prometheus-community/prometheus \\\n    --namespace prometheus \\\n    --set alertmanager.persistentVolume.storageClass=\"gp2\",server.persistentVolume.storageClass=\"gp2\"\nkubectl get pods -n prometheus\n</code></pre> <p>To access Prometheus web app: <code>kubectl --namespace=prometheus port-forward deploy/prometheus-server 8080:9090</code> </p> </li> <li> <p>For administration, deploy Kubernetes dashboard:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n# Connect to the dashboard by first getting the admin user secret\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')\n# the following could also be used\naws eks get-token --cluster-name myeks | jq -r '.status.token'\n# Then local proxy\nkubectl proxy --port=8080 --disable-filter=true\n# append the api to the exported URL:  api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/?#!/login\n</code></pre> </li> <li> <p>For monitoring, it is interesting to deploy AWS Distro for OpenTelemetry to collect metric logs to AWS CloudWatch, and <code>Container Insight</code> feature of CW to see application performance. Deploy the AWS Distro for OpenTelemetry collector as a DaemonSet by entering the following command.</p> <pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/deployment-template/eks/otel-container-insights-infra.yaml |\nkubectl apply -f - \n# Verify the running pods\nkubectl get pods -l name=aws-otel-eks-ci -n aws-otel-eks\n</code></pre> <p>After a few mins, the CloudWatch Logs Group should have 3 logs groups created by the OTEL DaemonSet.</p> </li> <li> <p>Verify nodes and pods</p> <pre><code>kubectl get nodes -o wide\n# across namespaces\nkubectl get pods --all-namespaces -o wide\n</code></pre> </li> <li> <p>Deploy an application using deployment, services and a service account. It is important to use the concept of iam role to service account mapping to control what AWS services the pod is able to access instead of using iam role and policies set at the EC2 level, as all pods within the EC2 will have the same policies. Applying the least permission.</p> </li> <li> <p>Expose the application to the internet.</p> <p>To do so we need to deploy the AWS Load Balancer controller inside the EKS cluster. Using Helm we can do:</p> <pre><code>helm repo add eks https://aws.github.io/eks-charts\n# Install the TargetGroupBinding CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\"\n# install controller\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=web-host-on-eks --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\n</code></pre> </li> </ol>"},{"location":"serverless/eks/#delete-cluster","title":"Delete cluster","text":"<ul> <li>List all services</li> </ul> <pre><code>kubectl get svc --all-namespaces\n</code></pre> <ul> <li>Delete any services that have an associated EXTERNAL-IP value. These services are fronted by an ELB load balancer, and we must delete them in Kubernetes to allow the load balancer and associated resources to be properly released.</li> </ul> <pre><code>kubectl delete svc &lt;service-name&gt;\n</code></pre> <ul> <li>Delete the cluster</li> </ul> <pre><code>eksctl delete cluster --name &lt;cluster name&gt;\n</code></pre>"},{"location":"serverless/eks/#eks-with-fargate-node-using-cdk","title":"EKS with Fargate node using CDK","text":"<p>AWS Fargate is a technology that provides on-demand, right-sized compute capacity for containers. We control which pods start on Fargate and how they run with Fargate Profiles.</p> <ul> <li>See instructions EKS fargate getting started.</li> <li>Fargate profiles are associated to namespaces.</li> <li>Only private subnets are supported for pods that are running on Fargate.</li> <li>Pods that match a selector are scheduled on Fargate.</li> <li>Kubernetes affinity/anti-affinity rules do not apply and aren't necessary with Amazon EKS Fargate pod.</li> </ul> <p>Here is an example of CDK declaration for a Fargate Cluster. See the local CDK file in <code>labs/eks/cdk-fargate</code>:</p> <pre><code># EKS cluster that only uses Fargate capacity\n aws_eks.FargateCluster(self,\"MyEKS\",\n            version=aws_eks.KubernetesVersion.V1_24)\n</code></pre> <p>To add FargateProfile to an existing cluster:</p> <pre><code>cluster = aws_eks.Cluster(self, 'demo-cluster',\n                                  masters_role=self.eks_admin_role,\n                                  vpc=self.vpc,\n                                  default_capacity=0,\n                                  vpc_subnets=[aws_ec2.SubnetSelection(subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS)],\n                                  version=aws_eks.KubernetesVersion.V1_24,\n                                  output_cluster_name=True\n                                  )\ncluster.add_fargate_profile(\"MyProfile\",\n    selectors=[eks.Selector(namespace=\"default\")]\n)\n</code></pre> <p>See Python cdk eks documentation.</p> <p>If we do not specify a VPC while defining the cluster, one will be created on our behalf, which we can then access via <code>cluster.vpc</code>.</p> <p>The fargate cluster declaration for example will create a CloudFormation template with:</p> <ul> <li>VPC, two public subnets, route table for each public subnet, with default route to internet gateway</li> <li>An internet gateway attached to the VPC</li> <li>Elastic Interface per public subnet</li> <li>NAT gateway for each public subnet</li> <li>Two private subnets, with route table and route with outbound to NAT gateway</li> <li>An IAM role to allow access to EKS principal with cluster access</li> <li>An IAM role to be able to create EKS cluster</li> <li>An IAM role for the EKS master admin user</li> <li>An IAM role for fargate pod execution</li> <li>Security group for the control plane</li> <li>A custom kubernetes resources for Authentication</li> </ul>"},{"location":"serverless/eks/#eks-blueprint","title":"EKS Blueprint","text":"<p>The EKS Blueprints is an open-source development framework that abstracts the complexities of cloud infrastructure from developers.</p>"},{"location":"serverless/eks/#concepts","title":"Concepts","text":"<ul> <li>A blueprint combines clusters, add-ons, and teams into a cohesive object that can be deployed as a whole.</li> <li>Team is a logical grouping of IAM identities that have access to a Kubernetes namespace(s), or cluster administrative access depending upon the team type.</li> <li>Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions.</li> <li>Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding.</li> </ul>"},{"location":"serverless/eks/#eks-with-cdk-hands-on","title":"EKS with CDK Hands-on","text":"<p>This is a summary of the steps to get a running demonstration of creating EKS and Day 2 add-on.</p>"},{"location":"serverless/eks/#single-cluster","title":"Single Cluster","text":"<ul> <li>Using CDK typescript here are the commands:</li> </ul> <pre><code>mkdir my-eks-blueprints\ncd my-eks-blueprints\ncdk init app --language typescript\nnpm i typescript@~4.8.4\nnpm i @aws-quickstart/eks-blueprints\n</code></pre> <ul> <li> <p>If not done before, bootstrap CDK (the following command is to bootstrap CDK in 3 regions)</p> <pre><code>cdk bootstrap --trust=$ACCOUNT_ID \\\n  --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://$ACCOUNT_ID/$AWS_REGION aws://$ACCOUNT_ID/us-east-2 aws://$ACCOUNT_ID/us-east-1\n</code></pre> </li> </ul> <p>See the code in labs/cdk/eks-single</p> <ul> <li>Create a Cluster using the eks-blueprints package, which is published as a npm module.</li> </ul> <pre><code>import * as cdk from 'aws-cdk-lib';\nimport { Construct } from 'constructs';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nexport default class ClusterConstruct extends Construct {\n  constructor(scope: Construct, id: string, props?: cdk.StackProps) {\n    super(scope, id);\n\n    const account = props?.env?.account!;\n    const region = props?.env?.region!;\n\n    const blueprint = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .addOns()\n    .teams()\n    .build(scope, id+'-stack');\n  }\n}\n</code></pre> <p>And in the app instantiate the cluster construct:</p> <pre><code>#!/usr/bin/env node\nimport 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport ClusterConstruct from '../lib/my-eks-blueprints-stack';\n\nconst app = new cdk.App();\nconst account = process.env.CDK_DEFAULT_ACCOUNT!;\nconst region = process.env.CDK_DEFAULT_REGION;\nconst env = { account, region }\n\nnew ClusterConstruct(app, 'cluster', { env });\n</code></pre> <ul> <li>Deploy the cluster: <code>cdk deploy cluster-stack</code>, then config kubectl</li> </ul> <pre><code>export KUBE_CONFIG=$(aws cloudformation describe-stacks --stack-name cluster-stack | jq -r '.Stacks[0].Outputs[] | select(.OutputKey|match(\"ConfigCommand\"))| .OutputValue')\n$KUBE_CONFIG\nkubectl get svc\n</code></pre> <ul> <li>EKS Blueprints Patterns</li> </ul>"},{"location":"serverless/eks/#onboard-teams","title":"Onboard teams","text":"<p>We want two teams: platform and application teams.</p> <pre><code>mkdir teams &amp;&amp; cd teams &amp;&amp; mkdir platform-team &amp;&amp; mkdir application-team\naws iam create-user --user-name platform\naws iam create-user --user-name application\n</code></pre> <p>Under <code>platform-team</code> create a <code>init.ts</code>, Add a IAM Principal to add users to the platform using their IAM credentials</p> <pre><code>import { ArnPrincipal } from \"aws-cdk-lib/aws-iam\";\nimport { PlatformTeam } from '@aws-quickstart/eks-blueprints';\n\nexport class TeamPlatform extends PlatformTeam {\n    constructor(accountID: string) {\n        super({\n            name: \"platform\",\n            users: [new ArnPrincipal(`arn:aws:iam::${accountID}:user/platform`)]\n        })\n    }\n}\n</code></pre> <p>And do the same for application team. Then modify the cluster definition to add team instances: The <code>cdk deploy cluster-stack</code> will create a new namespace for the team application.</p> <pre><code>import { TeamPlatform, TeamApplication } from '../teams'; \n...\n.teams(new TeamPlatform(account), new TeamApplication('burnham',account))\n</code></pre> <p>A command like <code>kubectl describe role -n team-burnham</code> gives information on the role and actions that member can do.</p> <p>Using Kubernetes constructs such as namespaces, quotas, and network policies, we can prevent applications deployed in different namespaces from communicating with each other.</p> <ul> <li> <p>To see the application user access limitation, login to the console in incognito mode, use the account ID, application as user and be sure to have setup a password in IAM for the <code>application</code> user. Once logged assume the role of cluster-stack-teamburnhamAccessRole3.... Then go to the EKS console. We should see an error message that the Team Burnham user is NOT allowed to list deployments in all the namespaces. But selecting the <code>team-burnham</code> namespace we should see pods and other elements.</p> </li> <li> <p>The user platform with the role <code>cluster-stack-templatformAccessRole5...</code> can access the cluster as admin and see all namespaces.</p> </li> </ul>"},{"location":"serverless/eks/#adding-add-ons","title":"Adding add-ons","text":"<p>See the list of supported add-ons. To add them use the addOn() function in the blueprint:</p> <pre><code>  const blueprint = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .addOns(new blueprints.ClusterAutoScalerAddOn)\n    .teams(new TeamPlatform(account), new TeamApplication('burnham',account))\n    .build(scope, id+'-stack');\n  }\n</code></pre>"},{"location":"serverless/eks/#autoscaling-the-cluster","title":"Autoscaling the cluster","text":"<ul> <li> <p>Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).</p> <pre><code># Example of setting HPA on a pod/deployment\nkubectl autoscale deployment php-apache `#The target average CPU utilization` \\\n--cpu-percent=50 \\\n--min=1 `#The lower limit for the number of pods that can be set by the autoscaler` \\\n--max=10 \nkubectl get hpa\n</code></pre> </li> <li> <p>Cluster Autoscaler (CAS) is a component that automatically adjusts the size of a Kubernetes Cluster so that all pods have a place to run and there are no unneeded nodes. Cluster Autoscaler typically runs as a Deployment in our cluster.  It delegates to the Auto Scaling Group for EC2s to manage node groups. So be sure to configure the ASG with min, max and desired EC2 instance counts. With Fargate we do not need the cluster autoscaler. When creating a managed node group, we can choose either the On-Demand or Spot capacity type. The allocation strategy to provision Spot capacity is set to capacity-optimized to ensure that the Spot nodes are provisioned in the optimal Spot capacity pools.</p> </li> <li> <p>Karpenter observes the aggregate resource requests of unscheduled pods and makes decisions to launch and terminate nodes to minimize scheduling latencies and infrastructure cost. It is installed to the EKS cluster using Helm, and configured with CRD. Karpenter scales up nodes in a group-less approach. Karpenter selects which nodes to scale , based on the number of pending pods and the Provisioner configuration. It selects how the best instances for the workload should look like, and then provisions those instances. Karpenter uses cordon and drain best practices to terminate nodes. The configuration of when a node is terminated can be controlled with <code>ttlSecondsAfterEmpty</code>.</p> </li> </ul> <p>See support note: How do I install Karpenter in my Amazon EKS cluster.</p>"},{"location":"serverless/eks/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>Product documentation - Elastic Kubernetes Service</li> <li>EKS workshops.</li> <li>Web Application Hosts on EKS Workshop uses terraform to create the VPC, and EKS cluster + deploy a web app as an example.</li> <li>EKS Blueprints for CDK Workshop.</li> <li>Getting started with Amazon EKS \u2013 eksctl.</li> <li>EKS Best Practices Guides in github.</li> <li>EKS Blueprint bootstrap.</li> <li>Amazon EKS Blueprints for Terraform.</li> <li>EKS SaaS workshop.</li> <li>kOps open source project for running k8s cluster on any cloud provider.</li> <li>Cluster Autoscaler CAS FAQs.</li> <li>EKS improves control plane scaling and update speed by up to 4x.</li> <li>Eliminate Kubernetes node scaling lag with pod priority and over-provisioning.</li> <li>How H2O.ai optimized and secured their AI/ML infrastructure with Karpenter and Bottlerocket.</li> </ul>"},{"location":"serverless/eventbridge/","title":"EventBridge","text":"<p>Info</p> <p>Created 11/2023 - Updated 02/13/2024</p>"},{"location":"serverless/eventbridge/#introduction","title":"Introduction","text":"<p>Amazon EventBridge is a serverless event bus service (Formerly CloudWatch Event), which is a router to send messages to Lambda functions, SQS, SNS, or some SaaS providers, using event routing rules. This is not a queueing, topic, or streaming product.</p> <p>EventBridge is a service for building scalable event-driven applications, enabling highly agile software development via fully managed and secure integrations, cutting costs and reducing time to production.</p> <p>The following figure illustrates the typical source of events and how EventBridge can process them and sends JSON documents to different sinks. Filtering logic may be applied.</p> <p></p> <p>In each AWS account there is an EventBridge default event bus already created, but we can add our own, via CLI, CloudFormation, CDK,... An event bus is a pipeline that receives events. To receive events from SaaS partners, we need to create a partner event source connected to a partner event bus.</p> <p>EventBridge supports 28+ AWS service as targets. We can also integrate to Partners SaaS services like Datadog, Zendeck, git, Spotify, another EventBridge event bus... </p> <p>Target can be an API destination which is made up of an HTTP endpoint, HTTP method (such as GET, POST, PUT), and a connection, which defines the type of authorization and credentials used to call the endpoint. </p> <p>EventBridge event bus as target can be use for cross region replication, for events aggregation in a same region. This could be mono or cross AWS accounts. Routing events between event buses in different accounts allows to support workloads grow in size and complexity (the receiver account must grant permission on the target event bus to allow the sending account to send events).</p>"},{"location":"serverless/eventbridge/#value-propositions","title":"Value Propositions","text":"<ul> <li>No infrastructure to manage.</li> <li>Fully managed event backbone to pass events from sources to sinks using routing logic.</li> <li>A lot of native, easy, integration with AWS Services.</li> <li>Combined with DynamoDB streams, API Gateway, SQS, SNS and Lambda, it is relatively easy to develop quick applications with no server and complex environment to configure. </li> <li>No need to write custom integration code, as could be done in classical ESB, but still support filtering and transformations.</li> <li>Enabled archive to support replayability.</li> <li>End point to end-point integration, with pipe to do advanced filtering and enrichments. </li> <li>Support scheduled event creation.</li> <li>EventBridge supports three methods for authorization: basic auth, OAuth, and API key</li> </ul> <p>Read features presentation.</p>"},{"location":"serverless/eventbridge/#concepts","title":"Concepts","text":"<ul> <li>Event represents a change that happened in the past. Event, in EventBridge has the following structure with metadata and payload in the <code>detail</code> element. The combination of the \"source\" and \"detail-type\" fields serves to identify the fields and values found in the \"detail\" field.</li> </ul> <pre><code>{\n  \"version\": \"0\",\n  \"id\": \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\",\n  \"detail-type\": \"EC2 Instance State-change Notification\",\n  \"source\": \"aws.ec2\",\n  \"account\": \"111111111111\",\n  \"time\": \"2017-12-22T18:43:48Z\",\n  \"region\": \"us-west-1\",\n  \"resources\": [\n    \"arn:aws:ec2:us-west-1:123456789012:instance/ i-1234567890abcdef0\"\n  ],\n  \"detail\": {\n    \"instance-id\": \" i-1234567890abcdef0\",\n    \"state\": \"terminated\"\n  }\n}\n</code></pre> <ul> <li>Event buses - An event bus receives events.  In term of design, it should include all the events related to the same business entity. It can be seen as a queue or a topic, or a channel.</li> <li> <p>Event Pattern are JSON objects with the same structure of the event that it is matching. The field names must also appear in the event with the same nesting structure. Any additional fields in the event that aren't included in the event pattern are ignored. Event patterns are defined in the context of event rule.</p> <pre><code>{\n  \"source\": [ \"aws.ec2\" ],\n  \"detail\": {\n    \"state\": [ \"terminated\" ]\n  }\n}\n</code></pre> </li> <li> <p>Routing rule matches incoming events and routes them to targets for processing. When we create a rule, we associate it with a specific event bus, and the rule is matched only to events received by that event bus. <code>Source</code> field is a meta-data attribute that is set by the producer application to uniquely identify itself, and can be used to filter and route to different consumer. It is useful to avoid looping on events coming in the event bus from different sources.</p> <ul> <li>Rule matching operates in constant time. The rule engine is in this event-rules repository.</li> <li>There are limits on the number of rules per bus (2000).</li> <li>We can only use Scheduled rules on the <code>default</code> event bus.</li> <li>EventBridge rules help to filter the events before they are sent to the targets, making sure that each target receives only the events they are interested in. A routing rule can customize the JSON sent to the target, by passing only certain parts of the message or by overwriting message attributes with constant values.</li> <li>For each target of a rule, we can define a retry policy that defines how many times and how long EventBridge will try to send the event again. SQS DLQ can be used for undelivered events.</li> </ul> </li> <li> <p>Target is a resource, EventBridge sends events when the rule pattern matches. We can create multiple targets for one rule.</p> </li> <li>EventBridge Pipes: is used for point to point connection between producer and consumer with filtering, enrichment and transformation capabilities. It may keep message order from source to one destination. The sources can be DynamoDB stream, Kinesis stream, MQ broker, MSK stream, Kafka topic, SQS queue.</li> <li> <p>Global endpoints is used to facilitate failing over a secondary region, in case of primary disruption. It is linked to CloudWatch Alarm and Route 51 health checks.</p> </li> <li> <p>With Schema Discovery enabled, we can automatically get a schema definition from the event sent to the bus, saved in a schema registry. Schema Discovery also detects changes to event schema and automatically generates new schema version. From the schema in the registry, we can get code binding for Java, TypeScript, and Python, to jump start any consumer apps.</p> </li> <li>With the archive and replay feature, EventBridge can record any events processed by any type of event bus. Replay stores these recorded events in archives. We can choose to record all events, or filter events to be archived by using the same event pattern matching logic used in rules.</li> </ul>"},{"location":"serverless/eventbridge/#pattern-examples","title":"Pattern examples","text":"<ul> <li> <p>A or statement in a rule:</p> <pre><code>{\n  \"detail\": {\n    \"location\": [\"eu-west\", \"eu-east\"]\n  }\n}\n</code></pre> <p>Or between attribute</p> <pre><code>{\n  \"detail\": {\n    \"$or\": [\n      {\"location\": [\"eu-west\"]},\n      {\"nb_passengers\": [ {\"numeric\": [ \"&gt;\", 10 ]}]}\n    ]\n  }\n}\n</code></pre> </li> <li> <p>A prefix based statement:</p> </li> </ul> <p></p>"},{"location":"serverless/eventbridge/#security","title":"Security","text":"<p>EventBridge uses AWS Identity and Access Management, to manage both identity-based and resource-based policies. To make API calls against the resources we own, EventBridge needs the appropriate permissions. We can attach resource-based policies to the target resource with permission for the EventBridge service to call the required API. </p> <p>To invoke an API Gateway endpoint using an EventBridge rule, you could add an allow statement on the \"execute-api:Invoke\" API action.</p> <p>Identity-based policies are attached to an IAM user, group, or role that is assumed by the EventBridge service when the rule is invoked. See fine-grained access control documentation.. Amazon EventBridge has two AWS managed policies you can attach to users or groups:  AmazonEventBridgeFullAccess or AmazonEventBridgeReadOnlyAccess.</p> <p>We can authorize other account to PutEvents, PutRule or PutTargets API calls to an existing Event Bus via resource-based policy. It can be done for cross-region too.</p> <p>See Example policy: Send events to a custom bus in a different account.</p> <p>It provides encryption at rest and in transit by default.</p>"},{"location":"serverless/eventbridge/#cli-sam","title":"CLI - SAM","text":"<p>Develop can use AWS CLI events to create an event bus:</p> <pre><code>aws events create-event-bus --name account_events\n</code></pre> <p>SAM can be used with the following CLI options to select an existing schema: <code>Infrastructure event management &gt; ... &gt; EventBridge App from scratch (100+ Event Schemas) &gt;... Schema registry &gt; EventSchema...</code> </p> <p>The same can be done with AWS CLI:</p> <pre><code>aws schemas put-code-binding --registry-name discovered-schemas --schema-name com.aws.orders@OrderNotification --language Python36\n</code></pre> <p>When code bindings are published, we can download them using get-code-binding-source command. </p> <pre><code>aws schemas get-code-binding-source --registry-name discovered-schemas --schema-name com.aws.orders@OrderNotificatio\n</code></pre>"},{"location":"serverless/eventbridge/#deployment-patterns","title":"Deployment patterns","text":"<p>A a managed service, it is running under AWS VPC, scale horizontally and is highly available. Custom buses are defined per region. We can enable event replication to send from primary event bus to a secondary region. Events are processed in the secondary Region asynchronously.</p> <p>EventBridge provides at-least-once event delivery to targets, and is designed to retry sending events that are not delivered. Therefore we need to implement idempotency. The default retry policy is 24 hours up to 185 times. It uses exponential back-off and jitter. If a retry policy is unsuccessful in delivering the event, the event is dropped. To avoid losing events after they failed to be delivered to a target, configure a DLQ.</p> <p>AWS services send event to EventBridge using best effort delivery (in some rare cases an event might not be delivered) or with durable delivery (at least once).</p> <p>It is possible for application within a VPC to produce events to event bus, using Interface VPC endpoints, and to be an API destination (ECS task) also using Interface VPC endpoints.</p> <p></p> <p>See EventBridge product documentation and access an AWS service using an interface VPC endpoint.</p> <p>EventBridge with API Destination as target can also use private HTTP endpoint within a VPC. (See this blog) and the git repository aws-sample eventbridge-events-to-vpc.</p> <p>When using different regions for a disaster recovery architecture, RTO includes time period for triggering CloudWatch alarms and updating statuses for Route 53 health checks.  For RPO, the time includes events that are not replicated to the secondary Region and are stuck in the primary Region until the service or Region recovers. (RTO and RPO can be max of 420s).</p>"},{"location":"serverless/eventbridge/#solution-patterns","title":"Solution patterns","text":"<p>Below is a standard end to end solution using common patterns:</p> <p></p> <ul> <li>EventBridge can be used to Fan-out to different sinks or delivery targets but with limited way, see next bullet.</li> <li>With high transaction per second throughput, developers need to consider the number of delivery targets. For 1 target and 1 rule, it is common to be over 500k tps. Growing the number of delivery targets adds latency.</li> <li>So when we need to fan-out more, we can add SNS as delivery target after EventBridge. EB is then used for filtering events.</li> <li>We can define a central event hub to aggregate all the events from AWS Organizations into a single AWS account or region. Apps in different accounts can be authorized to send events to this central hub via resource-based policy.</li> <li>As events in archive aren't necessarily replayed in the same order that they were added to the archive, it is important to add a timestamp on the event message.</li> <li>Add a sequence_id or idempotency_id in the produced event for idempotency supports by consumers.</li> </ul>"},{"location":"serverless/eventbridge/#solution-development-approach-with-eventbridge","title":"Solution development approach with EventBridge","text":"<ul> <li>Assess who own the rules, and try to limit the number of rules, and focus on trying to get one rule to one delivery target.</li> <li>Define the schema of the event with JSONSchema, including the definition of the AWSEvent envelop.</li> <li>Upload the schema to the schema registry</li> <li>Create EventBridge bus per business entity (DDD aggregate). For example an <code>OrderEventBus</code> will support multiple event types: OrderCreated, OrderUpdated... </li> <li>Create routing rule with the event patterns that match certain event attribute and route to a target.</li> <li>Generate the Java, Nodejs or Python bindings</li> <li> <p>EventBridge is defined at the region level. So producer and consumer apps need to get region information in environment variables.</p> </li> <li> <p>The AWS CLI support EventBridge via aws events. </p> </li> <li>Cloud Formation EventBridge template</li> <li> <p>EventBridge CDK Construct library in Python</p> </li> <li> <p>Classical steps to develop a Lambda function from an Event definition in the schema registry:</p> </li> <li> <p>Create a sam project</p> </li> <li>By getting the code binding from the event model</li> <li>In the Lambda function, use <code>boto3</code> to access the backend and return response back to EventBridge event bus.</li> <li>Implement business logic in the Lambda handler function.</li> </ul>"},{"location":"serverless/eventbridge/#how-tos-and-best-practices","title":"How tos and best practices","text":"<p>ServerlessLand has a lot of CloudFormation examples to deploy different EDA patterns.</p> How to do a quick validation a routing rule works <p>A simple way to validate the rules execution, is to use Amazon CloudWatch as a catch all target. See the SAM below:</p> <pre><code>CarRideEventsLogGroup:\n    Type: AWS::Logs::LogGroup\n    Properties:\n      RetentionInDays: 1\n      LogGroupName: \"/aws/events/acr/carrides\"\nCarRideEventsToLogRule:\n  Type: AWS::Events::Rule\n  Properties:\n    Description: Catch all rule for development purposes- go to cloudwatch\n    EventBusName: !Ref EventBusName\n    EventPattern:\n      source:\n        - acr.com.car-ride-simulator\n    Name: CarRideEventsDevRule\n    State: ENABLED\n    Targets:\n      - Id: Ide60ca354-ebb9-47d2-ad81-6a52867eb9e6\n        Arn: !Sub \"arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${CarRideEventsLogGroup}\"\n</code></pre> <ul> <li>Create a rule for the event bus that will act as a \"catch-all\" for every event passed to the bus, irrespective of source.</li> </ul> The AWS Building EDA workshop solution <p>The solution looks like in the following figure:</p> <p></p> <p>It is also refactorized in labs/eda, (see readme).</p> <ol> <li>The event bridge to API gateway communication, illustrates that events can be pushed to HTTP endpoint, using Basic authentication. The demo also includes a lambda function to act as an authorizer by looking at the header token inside the HTTP request.</li> <li>Orders event bus has a routing rule to send to Step function when the detail of the event includes a <code>location</code> attribute with a prefix starting by \"eu\". The Step function sends back an event to the same source, <code>com.aws.orders</code> with a new order_id. </li> <li>Order events can also be propagated to a SNS to do a fan-out pattern.</li> </ol> Basic example to process EC2 event <ul> <li>Example of creating routing rules to assess when an EC2 is stopped:</li> </ul> <p></p> <ul> <li>We can use a sandbox feature to test the event type we want to work on (use the event template and then the <code>detail</code> element for the payload):</li> </ul> <p></p> <ul> <li>then define the rule using event pattern as json document, and test the rule:</li> </ul> <p></p> <ul> <li>specify the target to the routing rule, for example a SNS topic.</li> </ul> <p></p> <ul> <li>EventBridge can infer the data schema from the event, and use a SchemaRegistry. The SchemaRegitry will help generate code for our applications. </li> </ul> <p></p> <p>From this schema definition, in OpenAPI 2.0 format, we can get code sample to get the definition of the events and the marshalizer. </p> How to replay events from archive AWS Step task to publish event <p>Declaring a task in a Step function to publish an <code>OrderProcessed</code> event back to the <code>Orders</code> event bus, using Cloud Formation template syntax:</p> <pre><code>    \"PublishOrderProcessedEvent\": {\n        \"Type\": \"Task\",\n        \"Resource\": \"arn:aws:states:::events:putEvents\",\n        \"Parameters\": {\n        \"Entries\": [\n            {\n            \"Detail\": {\n                \"OrderId\": \"new_id\",\n                \"OrderDetails.$\": \"$.detail\"\n            },\n            \"DetailType\": \"Order Processed\",\n            \"EventBusName\": \"Orders\",\n            \"Source\": \"com.aws.orders\"\n            }\n        ]\n    },\n</code></pre> How to send an event via CLI <pre><code>aws events put-events --entries file://OrderNotification_v1.json\n</code></pre> Define EventBridge, pattern, rule and target with CDK <p>See CDK doc.</p>"},{"location":"serverless/eventbridge/#faqs","title":"FAQs","text":"<ul> <li>Product FAQs</li> </ul> Benefits of event-driven architectures in context of EventBridge <p>EventBridge offers decoupling protocol to share messages between producers and consumers. It offers schema registry to get some data contract between consumers and producer. It helps to adopt an evolutionary architecture by adding consumer over time and routing rules to those consumers. </p> Example of filtering and routing rules What is input transformer? <p>Input transformers are templating mechanisms to transform input JSON document to another format. The transformation defines variables that use the JSON path to reference values in the original matched event. To access the customer workload use <code>$.detail</code>.</p> What are the differences between default and custom event buses <p>Default is created for each account and used by internal AWS services to propagate events. It is also used by the Scheduler. Custom buses are mandatory to develop real-life solution. An event bus is mapped to a business entity and support the different event types for this entity.</p> Integrations with Lambda and other destinations <ul> <li>EventBus to Lambda</li> <li>Lambda to EventBus: use SDK to put message to EventBridge.</li> </ul> Explain archive and replay capabilities and best practices How to implement API Destination and explain benefits <p>API Destination supports integration to any HTTP endpoint. See this video, and the Python CDK to define API destination (authentication can be saved in AWS secrets). </p> How to assist customer in defining an event-driven architecture approach How to articulate benefits of EventBridge and assist customer in selecting a messaging platform Understanding of limits, throughput, and latencies <p>100 event buses per region. Each event bus can have up to 300 rules as the default limit (up to 2000 rules). Each rule can be configured for up to five targets. The event payload (with all metadata) must be less than 256 KB. There is a transactions-per-second quotas for the PutEvents API per AWS Region. Batch events in a single request, if needed. If there is throttling or delays in rule processing, it might come from exceeded the invocation quota limit. EventBridge will re-attempt invocations for up to 24 hours. See quotas documentation.</p>"},{"location":"serverless/eventbridge/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Product documentation Tutorials</li> <li>Building event-driven architectures on AWS</li> <li>Event Driven Architecture with Amazon API Gateway, Amazon EventBridge and AWS Lambda</li> <li>Amazon EventBridge CDK Construct Library Event </li> <li>Webinars and videos</li> <li>Integration with SaaS Partners</li> <li>Working with events and the Amazon EventBridge schema registry</li> <li>Amazon EventBridge Scheduler User Guide</li> </ul>"},{"location":"serverless/lambda/","title":"AWS Lambda","text":"<p>This document is a quick summary of the AWS Lambda technology, links to interesting content, labs, and sharing some best practices.</p> <p>Info</p> <p>Created 11/2023 - Updated 01/29/2024</p>"},{"location":"serverless/lambda/#introduction","title":"Introduction","text":"<p>With AWS Lambda, we can run code without provisioning or managing servers or containers.</p> <p>Upload the source code, and Lambda takes care of everything required to run and scale the code with high availability.</p> <ul> <li>Getting started tutorial with free tier</li> <li> <p>Pay only for what we use: # of requests and CPU time, and the amount of memory allocated.</p> </li> <li> <p>A  Lambda function has three primary components \u2013 trigger, code, and configuration.</p> <p></p> <ul> <li>Triggers describe when a Lambda function should run. A trigger integrates the Lambda function with other AWS services, enabling to run the Lambda function in response to certain API calls that occur in the AWS account.</li> <li>An execution environment manages the processes and resources that are required to run the function. </li> <li>Configuration includes compute resources, execution timeout, IAM roles (lambda_basic_execution)...</li> </ul> </li> </ul>"},{"location":"serverless/lambda/#lambda-execution-environment","title":"Lambda Execution environment","text":"<ul> <li> <p>The execution environment follows the life cycle as defined below (time goes from left to right):</p> <p></p> <ul> <li>In the Init phase Lambda creates or unfreezes an execution environment with the configured resources, downloads the code for the function and all the needed layers, initializes any extensions, initializes the runtime, and then runs the function\u2019s initialization code. After init, the environment is 'Warm'. The extension and runtime inits is part of the <code>cold start</code> (&lt;1s). </li> <li>In the Invoke phase Lambda invokes the function handler. After the function runs to completion, Lambda prepares to handle another function invocation.</li> <li>During Shutdown phase: Lambda shuts down the runtime, alerts the extensions to let them stop cleanly, and then removes the environment.</li> </ul> </li> </ul> Lambda Extension <p>Lambda supports external and internal extensions. An external extension runs as an independent process in the execution environment and continues to run after the function invocation is fully processed. Can be used for logging, monitoring, integration...</p> <ul> <li>The Lambda service is split into the control plane and the data plane. The control plane provides the management APIs (for example, <code>CreateFunction</code>, <code>UpdateFunctionCode</code>). The data plane is where Lambda's API resides to invoke the Lambda functions. It is HA over multi AZs in same region.</li> <li> <p>Lambda Workers are bare metal Amazon EC2 Nitro instances which are launched and managed by Lambda in a separate isolated AWS account which is not visible to customers. Each workers has one to many Firecraker microVMs. There is no container engine. Container image is just for packaging the lambda code as zip does.</p> <p></p> <ul> <li>Synchronous calls is used for immediate function response, with potential errors returned to the caller. It may return throttles when we hit the concurrency limit.</li> <li>Asynchronous calls return an acknowledgement message. Event payloads are always queued for processing before invocation. Internal SQS queue persists messages for up to 6 hours. Queued events are retrieved in batches by Lambda\u2019s poller fleet. The poller fleet is a group of Amazon EC2 instances whose purpose is to process queued event invocations which have not yet been processed. When an event fails all processing attempts, it is discarded by Lambda. The dead letter queue (DLQ) feature allows sending unprocessed events from asynchronous invocations to an Amazon SQS queue or an Amazon SNS topic defined by the customer. Asynchronous processing should be more scalable.</li> <li>Event source mapping is used to pull messages from different streaming sources and then synchronously calls the Lambda function. It reads using batching and send all the events as argument to the function. If the function returns an error for any of the messages in a batch, Lambda retries the whole batch of messages until processing succeeds or the messages expire. It supports error handling. </li> <li>If the service is not available. Callers may queue the payload on client-side to retry. If the invoke service receives the payload, the service attempts to identify an available execution environment for the request and passes the payload to that execution environment to complete the invocation. It may lead to create this execution environment.</li> </ul> <p></p> </li> <li> <p>There is at least one runtime which matches the programming language (Java, Node.js, C#, Go, or Python).</p> </li> <li>To reuse code in more than one function, consider creating a Layer and deploying it. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies.</li> <li> <p>Lambda supports versioning and developer can maintain one or more versions of the lambda function. We can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. Below  is an example of creating one Alias to version 1 and a routing config with Weight at 30% to version 2. Alias enables promoting new lambda function version to production and if we need to rollback a function, we can simply update the alias to point to the desired version. Event source needs to use Alias ARN for invoking the lambda function.</p> <pre><code>aws lambda create-alias --name routing-alias --function-name my-function --function-version 1  --routing-config AdditionalVersionWeights={\"2\"=0.03}\n</code></pre> </li> <li> <p>Each lambda function has a unique ARN.</p> </li> <li>Deployment package is a zip or container image.</li> </ul>"},{"location":"serverless/lambda/#scaling","title":"Scaling","text":"<p>Lambda invokes the code in a secure and isolated execution environment, which needs to be initialized and then executes the function code for the unique request it handles. Second request will not have the initialization step. When requests arrive, Lambda reuses available execution environments, and creates new ones if necessary.</p> <p>The number of execution environments determines the concurrency. Limited to 1000 by default.</p> <p>Concurrency (# of in-flight requests the function is currently handling) is subject to quotas at the AWS Account/Region level. See quotas documentation.</p> <p>When the number of requests decreases, Lambda stops unused execution environments to free up scaling capacity for other functions.</p> <p>Use the Lambda CloudWatch metric named <code>ConcurrentExecutions</code> to view concurrent invocations for all or individual functions. To estimate concurrent requests use:  Request per second * Avg duration in seconds = concurrent requests</p> <p>Lambda scales to very high limits, but not all account's concurrency quota is available immediately, so requests could be throttled for a few minutes in case of burst. </p> <p>There are two scaling quotas to consider with concurrency. Account concurrency quota (1000 per region) and burst concurrency quota (from 500 to 3000 per min per region). Further requests are throttled, and lambda returns HTTP 429 (too many requests).</p> <p>It is possible to use Reserved concurrency, which splits the pool of available concurrency into subsets. A function with reserved concurrency only uses concurrency from its dedicated pool. This is helpful to avoid one lambda function to take all the concurrency quota and impact other functions in the same region. No extra charges.</p> <p>For functions that take a long time to initialize, or that require extremely low latency for all invocations, provisioned concurrency enables to pre-initialize instances of the function and keep them running at all times.</p> <p>Use <code>concurrency limit</code> to guarantee concurrency availability for a function, or to avoid overwhelming a downstream resource that the function is interacting with.</p> <p>If the test results uncover situations where functions from different applications or different environments are competing with each other for concurrency, developers probably need to rethink the account segregation strategy and consider moving to a multi-account strategy.</p> <p>Memory is the only setting that can impact performance. Both CPU and I/O scale linearly with memory configuration. We can allocate up to 10 GB of memory to a Lambda function. In case of low performance, start by adding memory to the lambda.</p> <p></p> <ul> <li>AWS Lambda Power Tuning tool to find the right memory configuration.</li> <li>Understanding AWS Lambda scaling and throughput - an AWS blog.</li> </ul>"},{"location":"serverless/lambda/#networking","title":"Networking","text":"<ul> <li>Lambda functions always operate from an AWS-owned VPC. By default, the function has the full ability to make network requests to any public internet addresses \u2014 this includes access to any of the public AWS APIs.</li> <li>Only enable the functions to run inside the context of a private subnet in a VPC, when we need to interact with a private resource located in a private subnet. In this case we need to enable internet outbound connection, like NAT, network policies, IGW.</li> <li>When connecting a Lambda function to a VPC, Lambda creates an elastic network interface, ENI, for each combination of subnet and security group attached to the function.</li> </ul>"},{"location":"serverless/lambda/#private-resource-within-vpc","title":"Private resource within VPC","text":"<p>![]</p>"},{"location":"serverless/lambda/#api-gateway-integration","title":"API Gateway integration","text":"<p>Resources defined as API in Amazon API Gateway may define one or more methods, such as GET or POST which integration routes requests to a Lambda function. We can add a Trigger to a Lambda to get a HTTP API integration from the API Gateway. We configure API Gateway to pass the body of the HTTP request as-is.</p> <p>The event format is defined here.</p>"},{"location":"serverless/lambda/#edge-function","title":"Edge Function","text":"<p>When we need to customize the CDN content, we can use Edge Function to run closer to the end users.</p> <p>CloudFront provides two types: CloudFront functions or Lambda@Edge.</p> <p>Edge can be used for:</p> <ul> <li>Website security and privacy.</li> <li>Dynamic web application at the Edge.</li> <li>Search engine optimization (SEO).</li> <li>Intelligently route across origins and data centers.</li> <li>Bot mitigation at the Edge.</li> <li>Real-time image transformation.</li> <li>User authentication and authorization.</li> </ul>"},{"location":"serverless/lambda/#criteria-to-use-lambda","title":"Criteria to use lambda","text":""},{"location":"serverless/lambda/#technical-constraints","title":"Technical constraints","text":"<ul> <li>Per region deployment.</li> <li>Must run under 15 min.</li> <li>Memory from 128MB to 10GB.</li> <li>Maximum 1000 concurrent calls.</li> <li>Code in compressed zip should be under 50MB and 250MB uncompressed.</li> <li>Disk capacity for /tmp is limited to 10GB.</li> </ul>"},{"location":"serverless/lambda/#design-constraints","title":"Design constraints","text":"<ul> <li>When migrating existing application review the different design patterns to consider.</li> <li>Think about co-existence with existing application and how API Gateway can be integrated to direct traffic to new components (Lambda functions) without disrupting existing systems. With API Gateway developer can export the SDK for the business APIs to make integration easier for other clients, and can use throttling and usage plans to control how different clients can use the API.</li> <li>Do cost comparison analysis. For example API Gateway is pay by the requests, while ALB is priced by hours based on the load balance capacity units used per hour. Lambda is also per request based.</li> <li>Not everything fits into the function design. Assess if it makes sense to map REST operations in the same handler. AWS Lambda Powertools has a APIGatewayRestResolver that makes the code neat with api being defined with annotation: see Lambda DynamoDB example</li> <li>Assess when to use Fargate when the application is in container, and may run for a long time period. Larger packaging may not be possible to run on Lambda. Applications that use non HTTP end point, integrate to messaging middleware with Java based APIs are better fit for Fargate deployment.</li> </ul>"},{"location":"serverless/lambda/#security","title":"Security","text":"<p>As a managed service, AWS manages the underlying infrastructure and foundation services, the operating system, and the application platform. Developers need to ensure code, libraries, configuration, IAM are well set. Some security considerations:</p> <ul> <li>Data privacy with encryption at rest with customer managed Key, encryption in transit, access control.</li> <li>Function runtime environment variables are secured by encryption using a Lambda-managed KMS key (named <code>aws/lambda</code>). The <code>CreateFunction</code> API or <code>UpdateFunctionConfiguration</code> may use KMS Keys.</li> <li>AWS X-Ray also encrypts data by default.</li> <li>TLS1.2 for all public APIs.</li> <li>Run on EC2 with Nitro System for better security isolation.</li> <li> <p>Code releases go through security review and penetration testing. It provides a code signing feature to ensure only trusted code is run in the Lambda function.</p> </li> <li> <p>Lambda also supports function URLs, a built-in HTTPS endpoint for invoking functions. No need for API Gateway and ALB.</p> </li> <li>Each Lambda exec environment includes a writeable /tmp folder: files written within it, remain for the lifetime of the execution environment.</li> <li> <p>Also to enable the Lambda function to access resources inside the private VPC, we must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs and the <code>AWSLambdaVPCAccessExecutionRole</code> policy. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable the function to connect securely to other resources in the VPC. </p> <p></p> <p>As Lambda function always runs inside a VPC owned by the Lambda service, the function accesses resources in our VPC using a Hyperplane ENI. Hyperplane ENIs provide NAT capabilities from the Lambda VPC to our account VPC using VPC-to-VPC NAT (V2N).</p> <p>If it runs out of IP@ then we will have EC2 error types like <code>EC2ThrottledException</code> and the function will not scale. So be sure to have multiple AZ/ subnets and enough IP@.</p> </li> <li> <p>For the function to access the internet, route outbound traffic to a NAT gateway in one public subnet in the VPC.</p> </li> <li> <p>To establish a private connection between the VPC and Lambda, create an interface VPC endpoint (using AWS PrivateLink). Traffic between the VPC and Lambda does not leave the AWS network.</p> <pre><code>aws ec2 create-vpc-endpoint --vpc-id vpc-ec43eb89 --vpc-endpoint-type Interface --service-name \\\ncom.amazonaws.us-west-2.lambda --subnet-id subnet-abababab --security-group-id sg-1a2b3c4d      \n</code></pre> </li> </ul> <p>Endpoint policy can be attached to the interface endpoint to add more control on which resource inside the VPC can access the lambda function (Principal = user, Allow lambda:InvokeFunction on resource with the function arn).</p>"},{"location":"serverless/lambda/#policies-and-roles","title":"Policies and roles","text":"<p>With Lambda functions, there are two sides that define the necessary scope of permissions \u2013 permission to invoke the function (using resource policies), and permission of the Lambda function itself to act upon other services (IAM execution role).</p> <p>The execution role must include a trust policy that allows Lambda to \u201cAssumeRole\u201d so that it can take that action for another service.</p> <p>Lambda resource policies are :</p> <ul> <li>Associated with a \"push\" event source such as Amazon API Gateway.</li> <li>Created when we add a trigger to a Lambda function.</li> <li>Allows the event source to take the <code>lambda:InvokeFunction</code> action.</li> </ul> <p>We can use Parameter Store, from System Manager, to reference Secrets Manager secrets,  creating a consistent and secure process for calling and using secrets and reference data in the code and configuration script. </p>"},{"location":"serverless/lambda/#monitoring","title":"Monitoring","text":"<p>AWS Lambda automatically monitors Lambda functions and reports metrics through Amazon CloudWatch. To help monitoring the code as it executes, Lambda automatically tracks the number of requests, the latency per request, and the number of requests resulting in an error and publishes the associated metrics.  Developer can leverage these metrics to set custom CloudWatch alarms.</p> <p>Distributed tracing helps pinpoint where failures occur and what causes poor performance. Tracing is about understanding the path of data as it propagates through the components of the application.</p> <p>Amazon CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on Lambda. Lambda Insights collects, aggregates, and summarizes system-level metrics. It also summarizes diagnostic information such as cold starts and Lambda worker shutdowns to help isolate issues with the Lambda functions and resolve them quickly.</p> <p>X-Ray provides an end-to-end view of requests as they travel through the application and the underlying components. Developer can use AWS X-Ray to visualize the components of the application, identify performance bottlenecks, and troubleshoot requests that resulted in an error.</p> <p>See this article: Operating Lambda with logging and custom metrics.</p>"},{"location":"serverless/lambda/#custom-metrics","title":"Custom metrics","text":"<p>Custom metrics can be used for tracking statistics in the application domain, instead of measuring performance related to the Lambda function. Use AWS SDK with the CloudWatch library and the <code>putMetricData</code> API.</p> Javascript Code Example <pre><code>const AWSXRay = require('aws-xray-sdk-core')\nconst AWS = AWSXRay.captureAWS(require('aws-sdk'))\nconst cloudwatch = new AWS.CloudWatch()\n\nexports.putMetric = async (name, unit = MetricUnit.Count, value = 0, options) =&gt; {\n    try {\n        log.debug(`Creating custom metric ${name}`)\n        const metric = buildMetricData(name, unit, value, options)\n        await cloudwatch.putMetricData(metric).promise()\n    } catch (err) {\n        log.error({ operation: options.operation !== undefined ? options.operation : 'undefined_operation', method: 'putMetric', details: err })\n        throw err\n    }\n}\n\nconst buildMetricData = (name, unit, value, options) =&gt; {\n    let namespace = 'MonitoringApp',\n        service = process.env.SERVICE_NAME !== undefined ? process.env.SERVICE_NAME : 'service_undefined'\n\n    if (options) {\n        if (options.namespace !== undefined) namespace = options.namespace\n        if (options.service !== undefined) service = options.service\n        delete options.namespace\n        delete options.service\n    }\n\n    const metric = {\n        MetricData: [\n            {\n                MetricName: name,\n                Dimensions: buildDimensions(service, options),\n                Timestamp: new Date(),\n                Unit: unit,\n                Value: value\n            },\n        ],\n        Namespace: namespace\n    };\n    return metric\n}\n</code></pre> <p>Metrics are in namespace in CloudWatch. </p> <p></p> <p>But those synchronous calls consumes resources and adds latency to the lambda's response. To overcome this overhead, we can adopt an asynchronous strategy to create these metrics. This strategy consists of printing the metrics in a structured or semi-structured format as logs to Amazon CloudWatch Logs and have a mechanism in background processing these entries based on a filter pattern that matches the same entry that was printed.</p>"},{"location":"serverless/lambda/#using-the-embedded-metric-format","title":"Using the Embedded Metric Format","text":"EMF logging code example <pre><code>const { createMetricsLogger, Unit } = require(\"aws-embedded-metrics\")\nexports.logMetricEMF = async (name, unit = Unit.Count, value = 0, dimensions) =&gt; {\n    try {\n        const metrics = createMetricsLogger()\n        metrics.putDimensions(buildEMFDimensions(dimensions))\n        metrics.putMetric(name, value, unit)\n        metrics.setNamespace(process.env.AWS_EMF_NAMESPACE !== undefined ? process.env.AWS_EMF_NAMESPACE : 'aws-embedded-metrics')\n        log.debug(`Logging custom metric ${name} via Embbeded Metric Format (EMF)`)\n        log.debug(metrics)\n        await metrics.flush()\n    } catch (err) {\n        log.error({ operation: dimensions.operation !== undefined ? options.dimensions : 'undefined_operation', method: 'logMetricEMF', details: err })\n        throw err\n    }\n}\n\nconst buildEMFDimensions = (dimensions) =&gt; {\nlet service = process.env.SERVICE_NAME !== undefined ? process.env.SERVICE_NAME : 'service_undefined'\n\n    if (dimensions) {\n        if (dimensions.service !== undefined) service = dimensions.service\n        delete dimensions.namespace\n        delete dimensions.service\n    }\n\n    return dimensions\n}\n</code></pre>"},{"location":"serverless/lambda/#find-cold-starts","title":"Find cold starts","text":"<p>Example of cloudwatch insight query:</p> <pre><code>filter @type = \"REPORT\"\n    | parse @message /Init Duration: (?&lt;init&gt;\\S+)/ \n    | stats count() as total, count(init) as coldStarts, \n        median(init) as avgInitDuration,\n        max(init) as maxInitDuration,\n        avg(@maxMemoryUsed)/1000/1000 as memoryused\nby bin(5min)\n</code></pre>"},{"location":"serverless/lambda/#hands-on","title":"Hands-on","text":""},{"location":"serverless/lambda/#getting-started","title":"Getting Started","text":"Basic getting started <p>In a serverless deployment, developer needs to provide all the necessary components: </p> <ul> <li>Code, bundled with any necessary dependencies</li> <li> <p>CloudFormation template: (AWS SAM helps building this CF template).</p> </li> <li> <p>Can start from a blueprint</p> <p></p> </li> <li> <p>To get the function to upload logs to cloudWatch, select an existing IAM role, or create a new one with the right policy:</p> <p></p> <p>Here the example of role created</p> <p></p> </li> <li> <p>Add the code as an implementation of an handler function:</p> <p></p> </li> <li> <p>Create a test event (a request) and run the test and get the resources and logs output.</p> <p></p> </li> <li> <p>Verify configuration and monitoring in CloudWatch.</p> </li> </ul>"},{"location":"serverless/lambda/#python-function-with-dependencies","title":"Python function with dependencies","text":"<p>It is common to have a function that needs libraries not in the standard Python 3.x environment. The approach is to use a zip file as source of the dependencies. The process to build such zip can be summarized as:</p> <ul> <li><code>lambda</code> is the folder with code and future dependencies. It has a <code>requirements.txt</code> file to define dependencies.</li> <li>do a <code>pip install --target ./python/lib/ -r requirements.txt</code></li> <li> <p>zip the content of the package directory in a zip in the lambda folder</p> <pre><code>cd python\nzip -r ../lambda-layer.zip .\n</code></pre> </li> <li> <p>The zip can be used as a layer, so reusable between different lambda functions. Upload the zip to a <code>S3</code> bucket and create a layer in the Lambda console, referencing the zip in S3 bucket.</p> <p>As an alternate here is a CDK declaration for the powertool dependencies:</p> <pre><code>powertools_layer = aws_lambda.LayerVersion.from_layer_version_arn(\n        self,\n        id=\"lambda-powertools\",\n        layer_version_arn=f\"arn:aws:lambda:{env.region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:61\"\n    )\n\n...\nacm_lambda = aws_lambda.Function(self, 'CarMgrService',\n        runtime=aws_lambda.Runtime.PYTHON_3_11,\n        layers=[powertools_layer],\n        ...\n</code></pre> </li> <li> <p>A layer can be added to any lambda function, then the libraries included in the layer can be imported in the code. Example is the XRay tracing capability in Python.</p> </li> </ul> project with modules <p>When the Lambda code is organized with different modules, we may get <code>Unable to import module 'app':...</code> when running the function in the Lambda runtime. Here is an example:</p> <pre><code>from .acm_model import AutonomousCar, AutonomousCarEvent\n</code></pre> <p>The folder tree:</p> <pre><code>src\n\u251c\u2500\u2500 carmgr\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 acm_model.py\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Add the cdk:</p> <pre><code> acm_lambda = aws_lambda.Function(self, 'CarMgrService',\n                code= aws_lambda.Code.from_asset(path=\"../src/\",\n                handler='carmgr.app.handler',\n</code></pre> <ul> <li> <p>We can also add the lambda-function code in the zip and modify the existing function, something like:</p> <pre><code>zip lambda-layer.zip lambda-handler.py\naws lambda update-function-code --function-name  ApigwLambdaCdkStack-SageMakerMapperLambda2E...C --zip-file file://lambda-layer.zip\n</code></pre> </li> </ul> <p>See the the from-git-to-slack-serverless repository for a Lambda example in Python using SAM for deployment and autonomous-car-mgr project for cdk based deployment and more module integration.</p>"},{"location":"serverless/lambda/#java-based-function","title":"Java based function","text":"<p>We can run java code in Lambda and implement different handlers.</p> <pre><code>import com.amazonaws.services.lambda.runtime.Context;\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\n\npublic class HandlerWeatherData implements RequestHandler&lt;WeatherData,WeatherData&gt;{\n\n    @Override\n    public WeatherData handleRequest(WeatherData event, Context context) {\n</code></pre> <p>Code can be uploaded as jar or zip to a S3 bucket, and CloudFormation template or CDK can define the lambda function.</p> <p>In the Java code template in folder the Lambda processes a JavaBean and we can build a docker image, push to ECR and use CDK to deploy the function. </p> <pre><code>FROM public.ecr.aws/lambda/java:11\nCOPY target/classes ${LAMBDA_TASK_ROOT}\nCOPY target/dependency/* ${LAMBDA_TASK_ROOT}/lib/\nCMD [ \"jbcodeforce.HandlerWeatherData::handleRequest\" ]\n</code></pre> <p>See also the docker image lambda java.</p> <p>To be sure the dependencies are used, use the build plugin in maven:</p> <pre><code>    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n    &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\n    &lt;version&gt;3.1.2&lt;/version&gt;\n       ....\n</code></pre> <p>See the readme for instructions.</p> <p>Product documentation: Deploy Java Lambda functions with container images..</p> <p>Here is a Lambda Function CDK declaration using ECR image</p> <pre><code>repository= aws_ecr.Repository.from_repository_name(self,\"java-lambda-ecr-repo\",\"jbcodeforce/java-lambda\")\n\naws_lambda.DockerImageFunction(self,\"JavaLambda\",\n            code=aws_lambda.DockerImageCode.from_ecr(repository),\n            role=sm_role,\n            tracing=aws_lambda.Tracing.ACTIVE,\n             architecture=aws_lambda.Architecture.ARM_64)\n</code></pre> <p>Java on AWS Lambda workshop  to discover best practices for Java and Lambda using GraalVM or SnapStart</p>"},{"location":"serverless/lambda/#cicd","title":"CI/CD","text":"<p>Workshop: Building CI/CD pipelines for Lambda canary deployments using AWS CDK</p>"},{"location":"serverless/lambda/#other-personal-implementations","title":"Other personal implementations","text":"<ul> <li>Lamdba prepared by SAM + tutorial form AWS Lambda powertool with API gateway.</li> <li>S3 to Lambda to S3 for data transformation</li> <li>Big data SaaS: lambda to call SageMaker</li> <li>cdk python for lambda, dynamoDB, api gtw, AWS powertools (autonomous car manager): a Python lambda + API gateway, dynamoDB, iam policy, Alias and version.</li> </ul>"},{"location":"serverless/lambda/#faqs","title":"FAQs","text":"<ul> <li>The Amazon official FAQs.</li> </ul>"},{"location":"serverless/lambda/#best-practices","title":"Best practices","text":"<p>Extracted from the Top best practices and tips for building Serverless applications video</p> Be sure to measure concurrency correctly <p>Concurrency = TPS * Duration. Ex 100 Tx/s for and average duration of 500ms = so concurrency id 50, but is it takes 2s per tx, then c=200. So Optimize for duration &lt;1 and monitor ConcurrentExecutions metric.</p> Lambda bursting and scaling <p>Burst limit may differ per region. Each new lambda invocation reuses available execution contexts, and it creates new contexts until the account concurrency limit is reached. So initial burst is subject to the account concurrency limit which may be 1000, but it then scales by +500 contexts per minute until burst limit (3000), once the burst limit is reached, all other requests are throttled. </p> Reuse the execution environment properly <p>Initialize outside of the handler, lazy load it when needed, and cache static assets in /tmp. In Java the class constructor is only called at cold-start.</p> Control and limit dependencies <p>Include only what is needed. In java use the shade maven plugin to create a uber jar which removes duplicates. Use exclusion of jars from aws-sdk. Attach SDK in a layer.</p> Tell the SDK what to do <p>Most of the time SDK will know in which region it is, but it costs time, so prefer to use env variables and get in the code. Try to reuse connection. It can reduce CPU utilization up to 50%.</p> VPC - Lambda <p>Lambda must target private subnets and never public one. Target two subnets for HA. Do not remove created ENIs in EC2 console. Controls the VPCs, subnets and security groups the Lambda functions can target.</p> Lambda chaining <p>Avoid chaining directly lambda functions synchronously. Use Lambda destination or Step Functions.</p> Inject secrets safely <p>Inject them in environment variables, where values come in the function yaml definition.</p>"},{"location":"serverless/lambda/#more-reading","title":"More reading","text":"<ul> <li>Serverless application lens: focus on how to design, deploy, and architect serverless application workloads in the AWS Cloud.</li> <li>Security overview of AWS Lambda - whitepaper</li> <li>Using an Amazon S3 trigger to invoke a Lambda function.</li> <li> <p>Tutorial: Resize Images on the Fly with Amazon S3, AWS Lambda, and Amazon API Gateway.</p> </li> <li> <p>Best Practices for Developing on AWS Lambda.</p> </li> <li>Power tuning tool for Lambda.</li> <li>Serverless Patterns Collection (Serverlessland).</li> <li>Youtube videos - Build on serverless 2019.</li> <li>Lambda- configuring reserved concurrency</li> <li>Increasing real-time stream processing performance with Amazon Kinesis Data Streams enhanced fan-out and AWS Lambda</li> <li>Powertool for AWS Lambda - Python a developer toolkit to implement Serverless best practices and increase developer velocity.</li> <li>Powertool for AWS Lambda - Java.</li> </ul>"},{"location":"serverless/msk/","title":"AWS Managed Streaming for Apache Kafka","text":""},{"location":"serverless/msk/#value-propositions","title":"Value Propositions","text":"<ul> <li>Fully managed Kafka cluster and Kafka Connect cluster.</li> <li>Highly available with deployment over multi AZs by default. Deployed in Private subnets.</li> <li>Pay-as-you go pricing.</li> <li>AWS MSK SDK API to create and manage cluster resources.</li> <li>Use the same Kafka tools as open source Apache Kafka, like Cruise control, Mirror maker 2, Kafka connect...</li> <li>User interface to manage topics, consumer groups is not provided, and any open source can be used. </li> <li>Integrated with IAM, to use IAM role to control who can access and create topics on the cluster, and to send data... Client applications can assume the defined role.</li> <li>Integrated with VPC security group to control external machines access. It is still not recommended to expose cluster to public internet.</li> <li>Provisioning with IaC in CloudFormation or CDK. </li> <li>Integrate with Amazon Glue Schema Registry for schema management</li> <li>Direct integration with Apache Flink for real-time processing using Zeppelin Notebook or custom code via s3 bucket.</li> <li>The data storage is backed by Amazon Elastic Block Storage (Amazon EBS) volumes and remote storage.</li> <li>Support tiered storage to move data out of retention period window to longer storage  class.</li> <li>Longer persistence can be added by using S3 Sink Kafka Connector to S3 bucket and then AWS Glacier.</li> </ul>"},{"location":"serverless/msk/#typical-use-cases","title":"Typical use cases:","text":"<ul> <li>Data ingestion for data lakes, data warehouses</li> <li>Event streaming with time windowing analytics: click streams processing.</li> <li>Event sourcing in an EDA to support event backbone and event store capabilities.</li> <li>Messaging support for asynchronous communication between microservices, to increase decoupling.</li> <li>Metrics and log aggregations.</li> <li>Deliver messages to Spark Streaming jobs.</li> </ul> <p>Strangely MSK is part of the data and analytics organization and not part of the EDA approach at AWS</p>"},{"location":"serverless/msk/#moving-to-a-managed-service-for-kafka","title":"Moving to a managed service for Kafka","text":"<p>We may want to assess the following items:</p> <ul> <li>Are you managing your Kafka infrastructure today? on which platform (VM, Kubernetes, bare metal)?</li> <li>Number of cluster and how may nodes per cluster?</li> <li>What are the Sources of the event? </li> <li>What are the target Sinks?</li> <li>How are you consuming events from Kafka today (application type, volumes, throughput, lag monitoring)?</li> <li>How do you persist the records? and what is the current second level of storage?</li> <li>How do you currently upgrade your Kafka clusters? </li> <li>How elastic are your current Kafka clusters? Do you scale up or down according to message traffic?</li> <li>Do you use Kafka Connect? How do you operate those connectors today?</li> <li>Does your Kafka cluster rely on ZooKeeper?</li> <li>How do you think about resiliency for your Kafka clusters? Are they able to tolerate a zonal failure today?</li> </ul>"},{"location":"serverless/msk/#getting-started","title":"Getting Started","text":"<ul> <li>MSK cluster may be created from the AWS console, from the CLI, or using Cloud Formation template or CDK code.</li> <li>We can connect clients from up to five different VPCs with MSK Serverless clusters.</li> <li>Client connection can use IAM user to authenticate to MSK, using a special Java class for that (in the <code>aws-msk-iam-auth-1.1.7-all.jar</code>), or use the SASL, or Plain connection types.</li> <li>Specify EC2 machine instance type from <code>kafka.t3.small</code> to <code>kafka.m5.24xlarge</code>.</li> <li>Routing rules in the security group needs to support all tcp traffic on Zookeeper port 2181, and the different kafka bootstrap ports depending on the authentication procotol used: (9098 for IAM, 9092 for PLAIN, 9094 SASL)</li> </ul>"},{"location":"serverless/msk/#manual-creation-using-aws-console","title":"Manual creation using AWS Console","text":"<p>The AWS Console MSK creation wizard is quite simple to follow. Some considerations to address:</p> <ul> <li>Can create serverless cluster or provisioned cluster.</li> <li>Recommended version is kafka 3.6.0 for getting tiered storage. For 2.8.x version there is only one version: 2.8.2.tiered. See [restrictions documentation)(https://docs.aws.amazon.com/msk/latest/developerguide/msk-tiered-storage.html#msk-tiered-storage-constraints).</li> <li>VPC needs DNS hostnames and DNS resolution enabled and at least 2 subnets for 2 AZs.</li> <li>To use provisioned storage throughput up to 1000 MiB/s, choose kafka.m5.4xlarge or a larger broker type.</li> <li> <p>Think about what are the default configuration for the Kafka brokers, retention, topic creation... and define a MSK configuration which can be used by different clusters. </p> <p></p> </li> <li> <p>A security group is assigned to the cluster ENI's. Public access can be set up after cluster creation. For public access, the security groups that we select must have inbound TCP rules configured to allow public access. The ports that we can use to configure the rules depend on the access-control methods that we set for the cluster.</p> </li> <li> <p>The access control methods will define how client authenticate to Kafka. </p> <p></p> </li> <li> <p>Serverless clusters require IAM authentication. And Consumer, Topic, and Consumer Group metrics are always enabled.</p> </li> <li>For IAM authentication, always create an IAM role, with the policy to access the cluster and create, read, write to topic.</li> <li>To get visibility of the connection to kafka, add a LogGroup on CloudWath and link the group during the cluster creation.</li> <li>Retrieve the bootstrap server URL</li> </ul>"},{"location":"serverless/msk/#ec2-client","title":"EC2 Client","text":"<p>Be sure to create a EC2 in the same VPC as MSK, add ssh port 22 in security group and outbound rule to reach internet. Then install java on the EC2:</p> <pre><code>sudo yum -y install java-11\n# Kafka binary for the tools\nwget https://archive.apache.org/dist/kafka/3.4.1/kafka_2.12-3.4.1.tgz\ntar -xzf kafka_2.12-3.4.1.tgz \n# download the Amazon MSK IAM JAR file under kafka_2.12-&lt;&gt;/libs\nwget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.7/aws-msk-iam-auth-1.1.7-all.jar\n</code></pre> <ul> <li>Add a <code>client.properties</code> under kafka_2.12-&lt;&gt;/bin folder with the following declaration when using IAM:</li> </ul> <pre><code>security.protocol=SASL_SSL\nsasl.mechanism=AWS_MSK_IAM\nsasl.jaas.config=software.amazon.msk.auth.iam.IAMLoginModule required;\nsasl.client.callback.handler.class=software.amazon.msk.auth.iam.IAMClientCallbackHandler\n</code></pre> <p>For plaintext:</p> <pre><code>security.protocol=PLAINTEXT\n</code></pre> <ul> <li>Get Cluster Bootstrap address. (Be sure to have the AWS key and token in EC2 instance. See the client library note)</li> </ul> <pre><code>aws kafka get-bootstrap-brokers --region us-west-2 --cluster-arn arn:aws:kafka....\n</code></pre> <ul> <li> <p>Be sure there is a inbound rules in Security group used by MSK to authorize traffic on the port number of the bootstrap broker URL, from source within the CIDR expected range.</p> </li> <li> <p>Create a topic.</p> </li> </ul> <pre><code>export BS=boot-.....b.c3.kafka-serverless.us-west-2.amazonaws.com:9098\n# replication-factor set to 2 when there are only 2 AZs and 1 broker per AZ.\n./kafka-topics.sh --bootstrap-server $BS --command-config client.properties --create --topic carrides --partitions 1  --replication-factor 2\n</code></pre> Not responding <p>Most likely the port number of the bootstart server is not open in security group. So add an inbould rule on the port 9092 (or other) from the CIDR of the VPC(10.0.0.0/16). </p> <ul> <li>Validate topic created</li> </ul> <pre><code>./kafka-topics.sh --bootstrap-server $BS --command-config client.properties --describe --topic carrides\n# Results:\nTopic: carrides TopicId:04xSQmeTNpZR2FG4RA  PartitionCount: 1   ReplicationFactor: 3    Configs: min.insync.replicas=2,segment.bytes=134217728,retention.ms=604800000,message.format.version=2.8-IV2,unclean.leader.election.enable=false,retention.bytes=268435456000\n    Topic: carrides Partition: 0    Leader: 285 Replicas: 285,138,89    Isr: 285,138,89\n</code></pre> <ul> <li>Produce message</li> </ul> <pre><code>./kafka-console-producer.sh --broker-list $BS --producer.config client.properties --topic carrides\n</code></pre> <ul> <li>Consume message</li> </ul> <pre><code>./kafka-console-consumer.sh --bootstrap-server $BS --consumer.config client.properties --from-beginning --topic carrides --from-beginning\n</code></pre>"},{"location":"serverless/msk/#cdk-msk","title":"CDK MSK","text":"<p>The higher CDK abstraction level  is in alpha version, but a direct CloudFormation mapping is available. MSK Connect is not supported. See an example of serverless cluster in labs/cdk/msk folder deployed in a public subnet with iam-role and policy.</p> <p>Typical MSK cluster creation in cdk API. The cluster deployed in a private subnet of a custom VPC, needs outbound rule and inbound rule on the exposed port number (9094?) (See this code):</p> <pre><code> cluster = msk.Cluster(self, \"AcrCluster\",\n            cluster_name=\"AcrCluster\",\n            kafka_version=msk.KafkaVersion.V3_4_0,\n            vpc=self.vpc,\n            instance_type=ec2.InstanceType(\"kafka.t3.small\"),\n            security_groups=[mskSecurityGroup],\n            vpc_subnets=ec2.SubnetSelection( subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),\n            ebs_storage_info= msk.EbsStorageInfo(volume_size=10),                         \n            client_authentication=msk.ClientAuthentication.sasl(\n                iam=True\n            )\n\n        )\n</code></pre>"},{"location":"serverless/msk/#msk-and-iam","title":"MSK and IAM","text":"<p>To grant someone permission to describe an MSK cluster with the Amazon MSK DescribeCluster API operation, we include the <code>kafka:DescribeCluster</code> action in the policy. </p> <p>See service-specific resources, actions, and condition context keys for use in IAM permission policies.</p> <p>For resource-based policy, we can define which IAM principals have cross-account permissions to use the MSK cluster. </p>"},{"location":"serverless/msk/#limits","title":"Limits","text":"<ul> <li>When the throughput exceeds the write (200MiB/s) and read (400MiB/s) thresholds, client will be throttled. </li> <li>30 brokers per clusters with zookeeper, 60 with Kraft, and 90 brokers per account.</li> <li>20 TCP connections per broker per second.</li> <li>2400 partitions</li> </ul>"},{"location":"serverless/msk/#faqs","title":"FAQs","text":"How to migrate existing topic content to MSK new topic? <p>Use mirror maker 2, and a workshop explains how to do so.</p> How to move from Confluent schema registry to AWS Glue <p>See this note, also serializer are Java based, no other language supported yet.</p>"},{"location":"serverless/msk/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Amazon MSK Library for AWS Identity and Access Management</li> <li>MSK Workshop with cluster creation, storage, monitoring, cruise control, MM2, lambda integration and migration.</li> <li>AWS kafka CLI</li> <li>Creating a serverless Apache Kafka publisher using AWS Lambda, uses older CDK but the approach is interesting.</li> </ul>"},{"location":"serverless/msk/#msk-connect","title":"MSK Connect","text":"<p>An AWS deployment for Kafka Connect connectors to a MSK cluster or a self managed Kafka cluster. </p>"},{"location":"serverless/msk/#iam-roles-for-connector","title":"IAM roles for connector","text":"<p>We need an IAM role with each connector. This service execution role </p> <p>Depending on the kind of connector, we might also need to attach to the service execution role a permissions policy that allows it to access AWS resources. This has to be done via Custom Role with the following trusted entity</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"kafkaconnect.amazonaws.com\"\n                ]\n            }\n        }\n    ]\n}\n</code></pre> <p>and select the policy to access the target resource. For reading from Kinesis Data Stream select <code>AmazonKinesisReadOnlyAccess</code> managed policy.</p>"},{"location":"serverless/mwaa/","title":"Amazon Workflow Apache Airflow","text":""},{"location":"serverless/mwaa/#resources","title":"Resources","text":"<ul> <li>AWS MWAA local runner - github repo</li> <li>Verify environment tool</li> </ul>"},{"location":"serverless/stepfct/","title":"AWS Step Function","text":"<p>AWS Step Functions is a fully managed service that we can use to coordinate the components of distributed applications and microservices using visual workflows. </p>"},{"location":"serverless/stepfct/#value-propositions","title":"Value propositions","text":"<ul> <li>Resilient workflow automation</li> <li>Built-in error handling</li> <li>AWS service integration</li> <li>Auditable execution history and visual monitoring</li> <li>Serverless, can scale by itself</li> <li>Support short execution flow, or long lived transactions.</li> <li>Standard Workflows follow an exactly-once model, while Express Workflows employ an at-least-once model when called asynchronously, and at-most-once with synchronous call (message may be lost). Important differentiations.</li> <li>Reuse business logic in different flows</li> <li>Standard is billed by the # of state transitions while express by the number of executions, the duration of execution, and the memory consumed.</li> <li>Support a Map operation/state to run a set of workflow steps for each item in a dataset, in parallel. With inline mode maps runs in the context of the workflow. With distributed each map state runs in a child workflow execution, in parallel (scale to more than 40 parallel iterations).</li> </ul>"},{"location":"serverless/stepfct/#concepts","title":"Concepts","text":"<p>The workflow is defined using the State Language Notation. </p> <ul> <li>All work in Step Functions is done by tasks. </li> <li>A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services. </li> <li>An activity is an application that we write and host on AWS, on premises or on mobile devices. </li> <li>Activity workers execute the application code and report success or failure.</li> <li>The patterns supported are sequential sequencing of tasks, conditional branching, looping task, try-catch-finally for error and compensation, parallel execution.</li> <li>A <code>Task</code> state (\"Type\": \"Task\") represents a single unit of work performed by a state machine.</li> <li>Can integrate Human task. </li> <li>They are long running flow. But there is also the Express Workflow that is a short duration execution to support 100k state transitions per sec. ( Duration has to be less than 5 mins). It saves states in memory.</li> <li>When calling external service, one of the parameter is the <code>TaskToken</code> to send back to the Step service so the corresponding task can get the asynch response.</li> <li>For Standard workflow the max duration is 365 days.</li> <li>Input data can be pass in StartExecution call. </li> <li>For Standard Workflows, you can retrieve execution results from the execution history using external callers, such as the <code>DescribeExecution</code> action (<code>awslocal stepfunctions describe-execution --execution-arn ...</code> ).</li> <li>A <code>path</code> is a string beginning with <code>$</code> that you can use to identify components within JSON text.</li> <li><code>InputPath</code> can limit the input that is passed by filtering the JSON notation by using a path</li> <li><code>OutputPath</code> enables you to select a portion of the state output to pass to the next state. </li> </ul>"},{"location":"serverless/stepfct/#integration","title":"Integration","text":"<ul> <li>Step can be started from HTTP requests from APIGTW, IoT Rules, EventBridge, Lambda...</li> <li>For asynchronous express workflows (SDK StartExecution), to get the results we must poll cloudwatch logs.</li> <li>Step function uses a context object to keep, in JSON, the state of the state machine execution. </li> </ul> Call Lambda <p>Need a reference to the function ARN, and then stipulates the input, output parameters and retries logic. The Parameter for \"Payload.$\": \"$\" takes the input and send that to the lambda. <pre><code>\"Get credit limit\": {\n  \"Type\": \"Task\",\n  \"Resource\": \"arn:aws:states:::lambda:invoke\",\n  \"OutputPath\": \"$.Payload\",\n  \"Parameters\": {\n    \"Payload.$\": \"$\",\n    \"FunctionName\": \"arn:aws:lambda:us-west-2:000000000000:function:GetCreditLimit\"\n  },\n  \"Retry\": [\n    {\n      \"ErrorEquals\": [\n        \"Lambda.ServiceException\",\n        \"Lambda.AWSLambdaException\",\n        \"Lambda.SdkClientException\",\n        \"Lambda.TooManyRequestsException\"\n      ],\n      \"IntervalSeconds\": 2,\n      \"MaxAttempts\": 6,\n      \"BackoffRate\": 2\n    }\n  ],\n  \"Next\": \"Credit applied &gt;= 5000?\"\n},\n</code></pre></p>"},{"location":"serverless/stepfct/#error-handling","title":"Error Handling","text":"<p>Step Functions supports errors and retries via a looping pattern and provides try/catch/finally logic for known or unknown errors.</p> <p>At a high level, task and parallel states can use fields named Retry and Catch for error handling. When a state reports an error and there is no Retry or the retries don\u2019t resolve the problem, Step Functions looks through the catchers for a matching error and transitions to the state named in the next field. </p> <p>Each catcher can specify multiple errors to handle. The reserved name <code>States.ALL</code> is a wildcard that matches any error name.</p>"},{"location":"serverless/stepfct/#hands-on-demo","title":"Hands-on demo","text":"<p>We can use LocalStack to start developing Step function in vscode and run locally. See the labs/step/first-solution</p>"},{"location":"serverless/stepfct/#basic-step-flow-to-lambda-call","title":"Basic Step flow to Lambda call","text":"<p>See the Lab in labs/step/step2lambda folder.</p>"},{"location":"serverless/stepfct/#simple-solution","title":"Simple solution","text":"<p>This demo includes all the basic control flow constructs for a workflow: call lambda, read from DynamoDB, and perform Map computation. See the labs/step/first-solution.</p>"},{"location":"serverless/stepfct/#image-processing-from-s3","title":"Image processing from S3","text":"<p>Image processing for Autonomous car: upload traveler's selfie photos, to build a collection against it a camera can send a new image to recognize the traveler's face so he/she can enter in the car. This example is based on the image processing with step function workshop. See this repo for SAM / CloudFormation templates and function code.</p>"},{"location":"serverless/stepfct/#step-function-faqs","title":"Step Function FAQs","text":"<ul> <li>Where process instance information is persisted for the running workflows?</li> <li>How to support re-entrance?</li> <li>How to support DR</li> </ul>"},{"location":"serverless/stepfct/#expected-skill-set","title":"Expected Skill Set","text":"What are the different state types? <p>Pass, Task, Choice, Wait, Succeed, Fail, Parallel, Map.</p>"},{"location":"serverless/stepfct/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Main product marketing page</li> <li>10 mns getting started</li> <li>Information resources like reference architecture</li> <li>How step functions work.</li> <li>Design pattern for step functions.</li> <li>Git repo with Step Function examples defined as CloudFormation templates.</li> <li>Developer guide.</li> <li>VScode extension with visualization</li> <li>Step function workshop.</li> <li>Saga with State machine.</li> <li>Create serverless workflow with Step - hands-on tutorial 10 minutes</li> </ul>"},{"location":"templ/","title":"Templates to drive some architecture and design discussions","text":""},{"location":"templ/#whiteboard-best-practices","title":"Whiteboard best practices","text":"<ul> <li>Complement the discussion with diagrams, flows, are more engaging with participants.</li> <li>Discovery and collaborate on a one of a kind solution that the participants own.</li> <li>Take care of time management (segmentation of the agenda and what needs to be addressed) and space management (name section of the whiteboard, use 25% (agenda), 50% (main), 25% (takeaways) sections). BE PREPARED.</li> <li>Example of sections: topics, takeaways, next steps (action for me and then action for customer)</li> <li>Brings your own supplies: markers, eraser..</li> <li>Use Uppercase, limit color</li> <li>Apply the technique of <code>Touch, Turn, Talk</code></li> <li>Be sure to ask a participant if she/he is confortable to draw and go to the whiteboard.</li> <li>Do not assume you can keep the content of the whiteboard with a photo. Ask permission.</li> <li> <p>For diagramming:</p> <ul> <li>Focus on core elements</li> <li>Label elements by function, not service name</li> <li>Ask before erasing (or using alternative methods like strikethrough or an X)</li> <li>Plan iconography in advance</li> </ul> </li> </ul>"},{"location":"templ/#discover-architecture-requirements","title":"Discover architecture requirements","text":"<p>This is a simple agenda, parking lots zone, use cases and system context map in one drawio template:</p> <p></p> <p>Be sure to address:</p> <ul> <li>Do you understand the business?</li> <li>Do you know how they make money?</li> <li>Do you understand the problem?</li> <li>Who will be using this/who is the customer?</li> <li>What are the expectations from the customer?</li> <li>What does success look like?</li> <li>Does the customer have an established timeline or deadline?</li> </ul> <p>Use Well-Architected Pillars to approach a problem:</p> <ul> <li> <p>Cost:</p> <ul> <li>What\u2019s the budget?</li> <li>How do the company make money?</li> </ul> </li> <li> <p>Reliability: </p> <ul> <li>What happens if this app fails?</li> <li>Is there an SLA, internal or external?</li> </ul> </li> <li> <p>Operations:</p> <ul> <li>What does the team look like that is building this?</li> <li>Who will support it once it\u2019s built?</li> <li>Do they need help?</li> <li>Is there a partner they work with?</li> </ul> </li> <li> <p>Performance:</p> <ul> <li>Are there performance requirements?</li> <li>How fast does it need to be?</li> <li>Do they already have a similar app?</li> <li>Have they had any problems with it?</li> </ul> </li> <li> <p>Security:</p> <ul> <li>Are there security or compliance requirements?</li> <li>Any PII data?</li> <li>Do you have any upcoming audits? </li> <li>How do those tend to do? </li> <li>Do they need help? </li> </ul> </li> </ul>"}]}